{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing is not the most exciting part of NLP, but it is still one of the most important ones. Your task is to preprocess raw text (you can use your own, or [this one](http://mattmahoney.net/dc/text8.zip). For this task text preprocessing mostly consists of:\n",
    "\n",
    "1. cleaning (mostly, if your dataset is from social media or parsed from the internet)\n",
    "1. tokenization\n",
    "1. building the vocabulary and choosing its size. Use only high-frequency words, change all other words to UNK or handle it in your own manner. You can use `collections.Counter` for that.\n",
    "1. assigning each token a number (numericalization). In other words, make word2index Ð¸ index2word objects.\n",
    "1. data structuring and batching - make X and y matrices generator for word2vec (explained in more details below)\n",
    "\n",
    "**ATTN!:** If you use your own data, please, attach a download link. \n",
    "\n",
    "Your goal is to make SkipGramBatcher class which returns two numpy tensors with word indices. It should be possible to use one for word2vec training. You can implement batcher for Skip-Gram or CBOW architecture, the picture below can be helpful to remember the difference.\n",
    "\n",
    "![text](https://raw.githubusercontent.com/deepmipt/deep-nlp-seminars/651804899d05b96fc72b9474404fab330365ca09/seminar_02/pics/architecture.png)\n",
    "\n",
    "There are several ways to do it right. Shapes could be `x_batch.shape = (batch_size, 2*window_size)`, `y_batch.shape = (batch_size,)` for CBOW or `(batch_size,)`, `(batch_size,)` for Skip-Gram. You should **not** do negative sampling here.\n",
    "\n",
    "They should be adequately parametrized: CBOW(window_size, ...), SkipGram(window_size, ...). You should implement only one batcher in this task; and it's up to you which one to chose.\n",
    "\n",
    "Useful links:\n",
    "1. [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "1. [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "1. [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "\n",
    "You can write the code in this notebook, or in a separate file. It can be reused for the next task. The result of your work should represent that your batch has a proper structure (right shapes) and content (words should be from one context, not some random indices). To show that, translate indices back to words and print them to show something like this:\n",
    "\n",
    "```\n",
    "text = ['first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including']\n",
    "\n",
    "window_size = 2\n",
    "\n",
    "# CBOW:\n",
    "indices_to_words(x_batch) = \\\n",
    "        [['first', 'used', 'early', 'working'],\n",
    "        ['used', 'against', 'working', 'class'],\n",
    "        ['against', 'early', 'class', 'radicals'],\n",
    "        ['early', 'working', 'radicals', 'including']]\n",
    "\n",
    "indices_to_words(labels_batch) = ['against', 'early', 'working', 'class']\n",
    "\n",
    "# Skip-Gram\n",
    "\n",
    "indices_to_words(x_batch) = ['against', 'early', 'working', 'class']\n",
    "\n",
    "indices_to_words(labels_batch) = ['used', 'working', 'early', 'radicals']]\n",
    "\n",
    "```\n",
    "\n",
    "If you struggle with something, ask your neighbor. If it is not obvious for you, probably someone else is looking for the answer too. And in contrast, if you see that you can help someone - do it! Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from itertools import islice, product, chain\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an implementation of SkipGramBatch (for text8 dataset) with the following steps:\n",
    "- Load text from file  \n",
    "- Tokenize text by whitespace  \n",
    "- Count tokens' frequencies  \n",
    "- Build vocabulary  \n",
    "    - Filter tokens by frequency threshold (cutoff)\n",
    "    - Build 2 mappings: token to indices and indices to tokens\n",
    "    - Build vocabulary as filtered set of tokens\n",
    "- Filter tokens in text by frequency (with the same parameters as for vocabulary)\n",
    "    - Tokens with frequency less than cutoff are removed\n",
    "- Vectorize filtered tokens\n",
    "- Create sliding window with specific window size (window_size)\n",
    "    - For tokens at the beginning and end of the text there are windows with smaller sizes, no padding is used\n",
    "- Generate batches based on specific batch size (batch_size)\n",
    "    - Whether to use last batch, if its length is smaller than batch size, is controlled by parameter drop_last (default value: True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- There is only one line in original dataset, so the full size of original text is read in RAM, and there are no additional tokens are used (e.g. for marking sentences' boundaries)\n",
    "- This implementation is based on generating batches by Python generators, so shuffling is not used. Perhaps it is not critical here because we will have tens of millions of input/output pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramBatcher():\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, file_path):\n",
    "        with open(file_path) as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        return cls(text)\n",
    "    \n",
    "    def _tokenize(self):\n",
    "        self.tokens = self.text.split()\n",
    "    \n",
    "    def _count_tokens(self):\n",
    "        self.token_counts = Counter(self.tokens)\n",
    "    \n",
    "    def _build_vocab(self, cutoff):\n",
    "        filtered_token_counts = dict(filter(lambda x: x[1] >= cutoff, self.token_counts.items()))\n",
    "        self.token_to_idx = {token:idx for (idx, (token, _)) \n",
    "                             in enumerate(filtered_token_counts.items())}\n",
    "        self.idx_to_token = {idx:token for (token, idx) in self.token_to_idx.items()}\n",
    "        self.vocab = set(self.token_to_idx)\n",
    "\n",
    "    def _filter_tokens(self):\n",
    "        self.tokens = [token for token in self.tokens if token in self.vocab]\n",
    "    \n",
    "    def _vectorize_tokens(self):\n",
    "        self.vectorized_tokens = [self.token_to_idx[token] for token in self.tokens]\n",
    "    \n",
    "    def _create_sliding_window(self, window_size):\n",
    "        tokens_size = len(self.tokens)\n",
    "\n",
    "        for i in range(0, tokens_size):\n",
    "            center_word = self.vectorized_tokens[i:i+1]\n",
    "            left_context = self.vectorized_tokens[max(0, i - window_size): i]\n",
    "            right_context = self.vectorized_tokens[i + 1: min(tokens_size, i + window_size + 1)]\n",
    "            context = left_context + right_context\n",
    "            window = [list(product(center_word, context))]\n",
    "            yield window \n",
    "    \n",
    "    def devectorize_tokens(self, indices):\n",
    "        return [self.idx_to_token[idx] for idx in indices]\n",
    "        \n",
    "    def prepare_data(self, cutoff=1):\n",
    "        self._tokenize()\n",
    "        self._count_tokens()\n",
    "        self._build_vocab(cutoff)\n",
    "        self._filter_tokens()\n",
    "        self._vectorize_tokens()\n",
    "        \n",
    "    def generate_batches(self, window_size=1, batch_size=1, drop_last=True):\n",
    "        window = self._create_sliding_window(window_size)\n",
    "        batch = list(zip(*islice(window, batch_size)))\n",
    "        \n",
    "        if drop_last:\n",
    "            while batch and len(batch[0]) == batch_size:\n",
    "                batch = list(zip(*[pair for pairs in batch[0] for pair in pairs]))\n",
    "                yield batch\n",
    "                batch = list(zip(*islice(window, batch_size)))\n",
    "        else:\n",
    "            while batch:\n",
    "                batch = list(zip(*[pair for pairs in batch[0] for pair in pairs]))\n",
    "                yield batch\n",
    "                batch = list(zip(*islice(window, batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_batcher = SkipGramBatcher.from_file(DATA_PATH/'text8')\n",
    "sg_batcher.prepare_data(cutoff=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5656412"
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sg_batcher.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1, 2], [3, 4]]]"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(chain([[[1, 2], [3, 4]]]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "w = sg_batcher._create_sliding_window(window_size=2)\n",
    "\n",
    "for win in w:\n",
    "    print(list(islice(chain.from_iterable(win), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sg_batcher.generate_batches(window_size=3, batch_size=1024)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for _ in tqdm_notebook(g):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 1, 1, 1, 2, 2, 2, 2), (1, 2, 0, 2, 3, 0, 1, 3, 4)]"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([list([(0, 1), (0, 2)]), list([(1, 0), (1, 2), (1, 3)]),\n",
    "       list([(2, 0), (2, 1), (2, 3), (2, 4)])])\n",
    "list(zip(*[pair for pairs in arr for pair in pairs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(arr[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([(0, 1), (0, 2)],\n",
       "  [(1, 0), (1, 2), (1, 3)],\n",
       "  [(2, 0), (2, 1), (2, 3), (2, 4)])]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [([(0, 1), (0, 2)], [(1, 0), (1, 2), (1, 3)], [(2, 0), (2, 1), (2, 3), (2, 4)])]\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0), (1, 2)], [(1, 1, 1), (0, 2, 3)], [(2, 2, 2, 2), (0, 1, 3, 4)]]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: list(zip(*x)), arr[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 1, 1, 1, 2, 2, 2, 2), (1, 2, 0, 2, 3, 0, 1, 3, 4)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(*[pair for pairs in arr[0] for pair in pairs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing, let's try to create batcher, generate batch and check correctness of batch's content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "\n",
      "First original words in text: \n",
      "anarchism originated as a term of abuse first used against\n",
      "\n",
      "First pre-processed tokens:\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "\n",
      "First vectorized tokens:\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "\n",
      "Cutoff:\n",
      "10\n",
      "\n",
      "Window size:\n",
      "3\n",
      "\n",
      "Batch size:\n",
      "10\n",
      "\n",
      "First token-index mappings:\n",
      "[('anarchism', 0), ('originated', 1), ('as', 2), ('a', 3), ('term', 4), ('of', 5), ('abuse', 6), ('first', 7), ('used', 8), ('against', 9)]\n",
      "\n",
      "Vocabulary size:\n",
      "47134\n",
      "\n",
      "Batch x indices:\n",
      "(0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9)\n",
      "\n",
      "Batch x tokens:\n",
      "['anarchism', 'anarchism', 'anarchism', 'originated', 'originated', 'originated', 'originated', 'as', 'as', 'as', 'as', 'as', 'a', 'a', 'a', 'a', 'a', 'a', 'term', 'term', 'term', 'term', 'term', 'term', 'of', 'of', 'of', 'of', 'of', 'of', 'abuse', 'abuse', 'abuse', 'abuse', 'abuse', 'abuse', 'first', 'first', 'first', 'first', 'first', 'first', 'used', 'used', 'used', 'used', 'used', 'used', 'against', 'against', 'against', 'against', 'against', 'against']\n",
      "\n",
      "Batch labels indices:\n",
      "(1, 2, 3, 0, 2, 3, 4, 0, 1, 3, 4, 5, 0, 1, 2, 4, 5, 6, 1, 2, 3, 5, 6, 7, 2, 3, 4, 6, 7, 8, 3, 4, 5, 7, 8, 9, 4, 5, 6, 8, 9, 10, 5, 6, 7, 9, 10, 11, 6, 7, 8, 10, 11, 12)\n",
      "\n",
      "Batch labels tokens:\n",
      "['originated', 'as', 'a', 'anarchism', 'as', 'a', 'term', 'anarchism', 'originated', 'a', 'term', 'of', 'anarchism', 'originated', 'as', 'term', 'of', 'abuse', 'originated', 'as', 'a', 'of', 'abuse', 'first', 'as', 'a', 'term', 'abuse', 'first', 'used', 'a', 'term', 'of', 'first', 'used', 'against', 'term', 'of', 'abuse', 'used', 'against', 'early', 'of', 'abuse', 'first', 'against', 'early', 'working', 'abuse', 'first', 'used', 'early', 'working', 'class']\n",
      "\n",
      "Batch pairs indices:\n",
      "[(0, 1), (0, 2), (0, 3), (1, 0), (1, 2), (1, 3), (1, 4), (2, 0), (2, 1), (2, 3), (2, 4), (2, 5), (3, 0), (3, 1), (3, 2), (3, 4), (3, 5), (3, 6), (4, 1), (4, 2), (4, 3), (4, 5), (4, 6), (4, 7), (5, 2), (5, 3), (5, 4), (5, 6), (5, 7), (5, 8), (6, 3), (6, 4), (6, 5), (6, 7), (6, 8), (6, 9), (7, 4), (7, 5), (7, 6), (7, 8), (7, 9), (7, 10), (8, 5), (8, 6), (8, 7), (8, 9), (8, 10), (8, 11), (9, 6), (9, 7), (9, 8), (9, 10), (9, 11), (9, 12)]\n",
      "\n",
      "Batch pairs tokens:\n",
      "[('anarchism', 'originated'), ('anarchism', 'as'), ('anarchism', 'a'), ('originated', 'anarchism'), ('originated', 'as'), ('originated', 'a'), ('originated', 'term'), ('as', 'anarchism'), ('as', 'originated'), ('as', 'a'), ('as', 'term'), ('as', 'of'), ('a', 'anarchism'), ('a', 'originated'), ('a', 'as'), ('a', 'term'), ('a', 'of'), ('a', 'abuse'), ('term', 'originated'), ('term', 'as'), ('term', 'a'), ('term', 'of'), ('term', 'abuse'), ('term', 'first'), ('of', 'as'), ('of', 'a'), ('of', 'term'), ('of', 'abuse'), ('of', 'first'), ('of', 'used'), ('abuse', 'a'), ('abuse', 'term'), ('abuse', 'of'), ('abuse', 'first'), ('abuse', 'used'), ('abuse', 'against'), ('first', 'term'), ('first', 'of'), ('first', 'abuse'), ('first', 'used'), ('first', 'against'), ('first', 'early'), ('used', 'of'), ('used', 'abuse'), ('used', 'first'), ('used', 'against'), ('used', 'early'), ('used', 'working'), ('against', 'abuse'), ('against', 'first'), ('against', 'used'), ('against', 'early'), ('against', 'working'), ('against', 'class')]\n"
     ]
    }
   ],
   "source": [
    "cutoff = 10\n",
    "window_size = 3\n",
    "batch_size = 10\n",
    "\n",
    "sg_batcher = SkipGramBatcher.from_file(DATA_PATH/'text8')\n",
    "sg_batcher.prepare_data(cutoff=cutoff)\n",
    "\n",
    "g = sg_batcher.generate_batches(window_size=window_size, batch_size=batch_size)\n",
    "x_batch, labels_batch = next(g)\n",
    "\n",
    "x_tokens_batch = sg_batcher.devectorize_tokens(x_batch)\n",
    "labels_tokens_batch = sg_batcher.devectorize_tokens(labels_batch)\n",
    "\n",
    "print('\\nFirst original words in text: ')\n",
    "print(' '.join(sg_batcher.text.split()[:batch_size]))\n",
    "\n",
    "print('\\nFirst pre-processed tokens:')\n",
    "print(sg_batcher.tokens[:batch_size])\n",
    "\n",
    "print('\\nFirst vectorized tokens:')\n",
    "print(sg_batcher.vectorized_tokens[:batch_size])\n",
    "\n",
    "print('\\nCutoff:')\n",
    "print(cutoff)\n",
    "\n",
    "print('\\nWindow size:')\n",
    "print(window_size)\n",
    "\n",
    "print('\\nBatch size:')\n",
    "print(batch_size)\n",
    "\n",
    "print('\\nFirst token-index mappings:')\n",
    "print(list(sg_batcher.token_to_idx.items())[:batch_size])\n",
    "\n",
    "print('\\nVocabulary size:')\n",
    "print(len(sg_batcher.vocab))\n",
    "\n",
    "print('\\nBatch x indices:')\n",
    "print(repr(x_batch))\n",
    "\n",
    "print('\\nBatch x tokens:')\n",
    "print(x_tokens_batch)\n",
    "\n",
    "print('\\nBatch labels indices:')\n",
    "print(repr(labels_batch))\n",
    "\n",
    "print('\\nBatch labels tokens:')\n",
    "print(labels_tokens_batch)\n",
    "\n",
    "print('\\nBatch pairs indices:')\n",
    "print(list(zip(x_batch, labels_batch)))\n",
    "\n",
    "print('\\nBatch pairs tokens:')\n",
    "print(list(zip(x_tokens_batch, labels_tokens_batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
