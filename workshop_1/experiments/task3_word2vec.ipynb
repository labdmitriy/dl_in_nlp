{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive word2vec\n",
    "\n",
    "This task can be formulated very simply. Follow this [paper](https://arxiv.org/pdf/1411.2738.pdf) and implement word2vec like a two-layer neural network with matrices $W$ and $W'$. One matrix projects words to low-dimensional 'hidden' space and the other - back to high-dimensional vocabulary space.\n",
    "\n",
    "![word2vec](https://i.stack.imgur.com/6eVXZ.jpg)\n",
    "\n",
    "You can use TensorFlow/PyTorch (numpy too, if you love to calculate gradients on your own and want some extra points, but don't forget to numerically check your gradients) and code from your previous task. Again: you don't have to implement negative sampling (you may reduce your vocabulary size for faster computation).\n",
    "\n",
    "**Results of this task**:\n",
    " * trained word vectors (mention somewhere, how long it took to train)\n",
    " * plotted loss (so we can see that it has converged)\n",
    " * function to map token to corresponding word vector\n",
    " * beautiful visualizations (PCE, T-SNE), you can use TensorBoard and play with your vectors in 3D (don't forget to add screenshots to the task)\n",
    " * qualitative evaluations of word vectors: nearest neighbors, word analogies\n",
    "\n",
    "**Extra:**\n",
    " * quantitative evaluation:\n",
    "   * for intrinsic evaluation you can find datasets [here](https://aclweb.org/aclwiki/Analogy_(State_of_the_art))\n",
    "   * for extrincis evaluation you can use [these](https://medium.com/@dataturks/rare-text-classification-open-datasets-9d340c8c508e)\n",
    "\n",
    "Also, you can find any other datasets for quantitative evaluation. If you chose to do this, please use the same datasets across tasks 3, 4, 5 and 6.\n",
    "\n",
    "Again. It is **highly recommended** to read this [paper](https://arxiv.org/pdf/1411.2738.pdf)\n",
    "\n",
    "Example of visualization in tensorboard:\n",
    "https://projector.tensorflow.org\n",
    "\n",
    "Example of 2D visualisation:\n",
    "\n",
    "![2dword2vec](https://www.tensorflow.org/images/tsne.png)\n",
    "\n",
    "If you struggle with something, ask your neighbor. If it is not obvious for you, probably someone else is looking for the answer too. And in contrast, if you see that you can help someone - do it! Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from itertools import islice, product, chain\n",
    "from argparse import Namespace\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramBatcher():\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, file_path):\n",
    "        with open(file_path) as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        return cls(text)\n",
    "    \n",
    "    def _tokenize(self):\n",
    "        self.tokens = self.text.split()\n",
    "    \n",
    "    def _count_tokens(self):\n",
    "        self.token_counts = Counter(self.tokens)\n",
    "    \n",
    "    def _build_vocab(self, cutoff):\n",
    "        filtered_token_counts = dict(filter(lambda x: x[1] >= cutoff, self.token_counts.items()))\n",
    "        self.token_to_idx = {token:idx for (idx, (token, _)) \n",
    "                             in enumerate(filtered_token_counts.items())}\n",
    "        self.idx_to_token = {idx:token for (token, idx) in self.token_to_idx.items()}\n",
    "        self.vocab = set(self.token_to_idx)\n",
    "\n",
    "    def _filter_tokens(self):\n",
    "        self.tokens = [token for token in self.tokens if token in self.vocab]\n",
    "    \n",
    "    def _vectorize_tokens(self):\n",
    "        self.vectorized_tokens = [self.token_to_idx[token] for token in self.tokens]\n",
    "    \n",
    "    def _create_sliding_window(self, window_size):\n",
    "        tokens_size = len(self.tokens)\n",
    "\n",
    "        for i in range(0, tokens_size):\n",
    "            center_word = islice(self.vectorized_tokens, i, i + 1)\n",
    "            left_context = islice(self.vectorized_tokens, i + 1, \n",
    "                                  min(tokens_size, i + window_size + 1))\n",
    "            right_context = islice(self.vectorized_tokens, \n",
    "                                   max(0, i - window_size), i)\n",
    "            yield from product(center_word, chain(left_context, right_context))\n",
    "    \n",
    "    def devectorize_tokens(self, indices):\n",
    "        return [self.idx_to_token[idx] for idx in indices]\n",
    "        \n",
    "    def prepare_data(self, cutoff=1):\n",
    "        self._tokenize()\n",
    "        self._count_tokens()\n",
    "        self._build_vocab(cutoff)\n",
    "        self._filter_tokens()\n",
    "        self._vectorize_tokens()\n",
    "        \n",
    "    def generate_batches(self, window_size=1, batch_size=1, drop_last=True):\n",
    "        window = self._create_sliding_window(window_size)\n",
    "        batch = list(zip(*islice(window, batch_size)))\n",
    "        x_batch, labels_batch = torch.tensor(batch[0]), torch.tensor(batch[1])\n",
    "\n",
    "        if drop_last:\n",
    "            while batch and len(batch[0]) == batch_size:\n",
    "                yield x_batch, labels_batch\n",
    "                batch = list(zip(*islice(window, batch_size)))\n",
    "                x_batch, labels_batch = torch.tensor(batch[0]), torch.tensor(batch[1])\n",
    "        else:\n",
    "            while batch:\n",
    "                yield x_batch, labels_batch\n",
    "                batch = list(zip(*islice(window, batch_size)))\n",
    "                x_batch, labels_batch = torch.tensor(batch[0]), torch.tensor(batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveWord2VecClassifier(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocabulary_size,\n",
    "                                      embedding_dim=embedding_size)\n",
    "        self.fc1 = nn.Linear(in_features=embedding_size,\n",
    "                             out_features=vocabulary_size,\n",
    "                             bias=False)\n",
    "    \n",
    "    def forward(self, x_in):\n",
    "        x_embedded = self.embedding(x_in)\n",
    "        y_out = self.fc1(x_embedded)\n",
    "        \n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    file_path = DATA_PATH/'text8',\n",
    "    \n",
    "    embedding_size = 50,\n",
    "    \n",
    "    seed = 42,\n",
    "    cutoff = 10,\n",
    "    window_size = 1,\n",
    "    batch_size = 128,\n",
    "    learning_rate = 0.03,\n",
    "    iterations = 100,\n",
    "    early_stopping_criteria = 1e8,\n",
    "    \n",
    "    cuda=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'batch_idx': 0,\n",
    "            'loss': []}\n",
    "\n",
    "def update_train_state(args, train_state):\n",
    "    if train_state['batch_idx'] == 0:\n",
    "        train_state['stop_early'] = False\n",
    "    else:\n",
    "        loss = train_state['loss'][-1]\n",
    "\n",
    "        if loss < train_state['early_stopping_best_val']:\n",
    "            train_state['early_stopping_best_val'] = loss\n",
    "        else:\n",
    "            train_state['early_stopping_step'] += 1 \n",
    "    \n",
    "        train_state['stop_early'] = train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    args.cuda=False\n",
    "    \n",
    "print(f'Using CUDA: {args.cuda}')\n",
    "args.device = torch.device('cuda' if args.cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47134"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_batcher = SkipGramBatcher.from_file(args.file_path)\n",
    "sg_batcher.prepare_data(cutoff=args.cutoff)\n",
    "\n",
    "vocabulary_size = len(sg_batcher.vocab)\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f502bb545e414bc58ec73bc0b80ba0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', style=ProgressStyle(description_width='initial')),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_seeds(args.seed)\n",
    "\n",
    "classifier = NaiveWord2VecClassifier(vocabulary_size=vocabulary_size,\n",
    "                                     embedding_size=args.embedding_size)\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = optim.Adam(params=classifier.parameters(),\n",
    "                      lr=args.learning_rate)\n",
    "\n",
    "train_bar = tqdm_notebook(desc='Training',\n",
    "                          position=1,\n",
    "                          total=args.iterations)\n",
    "\n",
    "batch_generator = sg_batcher.generate_batches(window_size=args.window_size, \n",
    "                                              batch_size=args.batch_size)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "writer = SummaryWriter(log_dir='logs/task_3', comment='embedding_training')\n",
    "\n",
    "running_loss = 0.\n",
    "classifier.train()\n",
    "\n",
    "for batch_idx, (x_batch, labels_batch) in enumerate(batch_generator, 1):\n",
    "    x_batch = x_batch.to(args.device)\n",
    "    labels_batch = labels_batch.to(args.device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    y_pred = classifier(x_in=x_batch)\n",
    "    \n",
    "    loss = loss_func(y_pred, labels_batch)\n",
    "    loss_value = loss.item()\n",
    "    running_loss += (loss_value - running_loss) / (batch_idx)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    train_state['batch_idx'] = batch_idx\n",
    "    train_state['loss'].append(running_loss)\n",
    "    writer.add_scalar('loss', scalar_value=loss, global_step=batch_idx)\n",
    "    \n",
    "    if train_state['stop_early']:\n",
    "        train_state = update_train_state(args=args,\n",
    "                                         train_state=train_state)\n",
    "    \n",
    "    train_params = dict(loss=running_loss,\n",
    "                        batch=batch_idx,\n",
    "                        lr=optimizer.param_groups[0]['lr'])\n",
    "    train_bar.set_postfix(train_params)\n",
    "    train_bar.update()\n",
    "    \n",
    "    if (batch_idx == args.iterations):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_state_df = pd.DataFrame(train_state)\n",
    "train_state_df['loss'].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    }
   ],
   "source": [
    "metadata = list(sg_batcher.token_to_idx)\n",
    "W = classifier.embedding.weight\n",
    "W_prime = classifier.fc1.weight\n",
    "\n",
    "W_avg = (W + W_prime) / 2\n",
    "\n",
    "writer.add_embedding(W, metadata=metadata, tag='W')\n",
    "writer.add_embedding(W_prime, metadata=metadata, tag='W_prime')\n",
    "writer.add_embedding(W_avg, metadata=metadata, tag='W_avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([47134, 50])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(classifier.parameters())[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([47134, 50])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(classifier.parameters())[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = classifier.embedding.weight\n",
    "W_prime = classifier.fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([47134, 50])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = (W + W_prime) / 2\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_batcher = SkipGramBatcher.from_file(args.file_path)\n",
    "sg_batcher.prepare_data(cutoff=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0]), tensor([1, 2, 3, 2, 3]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = sg_batcher.generate_batches(window_size=5, \n",
    "                                batch_size=5)\n",
    "x_batch, labels_batch = next(g)\n",
    "\n",
    "x_batch, labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 50)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = len(sg_batcher.vocab)\n",
    "embedding_size = 50\n",
    "\n",
    "vocabulary_size, embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = NaiveWord2VecClassifier(vocabulary_size=vocabulary_size,\n",
    "                              embedding_size=embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1679, -0.7678, -0.4254,  0.3084,  0.4510, -0.2278, -0.0301,  0.1163,\n",
       "          0.0782,  0.1542, -0.5275, -0.5969, -0.2852,  0.9326, -0.0609,  1.0134,\n",
       "          0.7858, -0.3970, -0.0972,  0.2352, -0.1621, -0.3524],\n",
       "        [ 0.1679, -0.7678, -0.4254,  0.3084,  0.4510, -0.2278, -0.0301,  0.1163,\n",
       "          0.0782,  0.1542, -0.5275, -0.5969, -0.2852,  0.9326, -0.0609,  1.0134,\n",
       "          0.7858, -0.3970, -0.0972,  0.2352, -0.1621, -0.3524],\n",
       "        [ 0.1679, -0.7678, -0.4254,  0.3084,  0.4510, -0.2278, -0.0301,  0.1163,\n",
       "          0.0782,  0.1542, -0.5275, -0.5969, -0.2852,  0.9326, -0.0609,  1.0134,\n",
       "          0.7858, -0.3970, -0.0972,  0.2352, -0.1621, -0.3524],\n",
       "        [ 0.1679, -0.7678, -0.4254,  0.3084,  0.4510, -0.2278, -0.0301,  0.1163,\n",
       "          0.0782,  0.1542, -0.5275, -0.5969, -0.2852,  0.9326, -0.0609,  1.0134,\n",
       "          0.7858, -0.3970, -0.0972,  0.2352, -0.1621, -0.3524],\n",
       "        [ 0.1679, -0.7678, -0.4254,  0.3084,  0.4510, -0.2278, -0.0301,  0.1163,\n",
       "          0.0782,  0.1542, -0.5275, -0.5969, -0.2852,  0.9326, -0.0609,  1.0134,\n",
       "          0.7858, -0.3970, -0.0972,  0.2352, -0.1621, -0.3524]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf(x_batch)\n",
    "# y_pred = F.softmax(y_pred, dim=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 2, 3])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7678, -0.4254,  0.3084, -0.4254,  0.3084], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[range(5), labels_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.9903, 3.6480, 2.9141, 3.6480, 2.9141], grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.log(y_pred[range(5), labels_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = torch.zeros((5, vocabulary_size), dtype=torch.long)\n",
    "y_true[range(5), labels_batch] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 22]), torch.Size([5, 22]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.shape, y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3.9903, 3.6480, 2.9141, 3.6480, 2.9141], grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss(reduce=False)\n",
    "loss(y_pred, labels_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = torch.zeros((5, vocabulary_size))\n",
    "arr[range(5), labels_batch] = 1\n",
    "arr.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 2, 3])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in log\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "loss = -np.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
