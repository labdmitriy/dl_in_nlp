{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive word2vec\n",
    "\n",
    "This task can be formulated very simply. Follow this [paper](https://arxiv.org/pdf/1411.2738.pdf) and implement word2vec like a two-layer neural network with matrices $W$ and $W'$. One matrix projects words to low-dimensional 'hidden' space and the other - back to high-dimensional vocabulary space.\n",
    "\n",
    "![word2vec](https://i.stack.imgur.com/6eVXZ.jpg)\n",
    "\n",
    "You can use TensorFlow/PyTorch (numpy too, if you love to calculate gradients on your own and want some extra points, but don't forget to numerically check your gradients) and code from your previous task. Again: you don't have to implement negative sampling (you may reduce your vocabulary size for faster computation).\n",
    "\n",
    "**Results of this task**:\n",
    " * trained word vectors (mention somewhere, how long it took to train)\n",
    " * plotted loss (so we can see that it has converged)\n",
    " * function to map token to corresponding word vector\n",
    " * beautiful visualizations (PCE, T-SNE), you can use TensorBoard and play with your vectors in 3D (don't forget to add screenshots to the task)\n",
    " * qualitative evaluations of word vectors: nearest neighbors, word analogies\n",
    "\n",
    "**Extra:**\n",
    " * quantitative evaluation:\n",
    "   * for intrinsic evaluation you can find datasets [here](https://aclweb.org/aclwiki/Analogy_(State_of_the_art))\n",
    "   * for extrincis evaluation you can use [these](https://medium.com/@dataturks/rare-text-classification-open-datasets-9d340c8c508e)\n",
    "\n",
    "Also, you can find any other datasets for quantitative evaluation. If you chose to do this, please use the same datasets across tasks 3, 4, 5 and 6.\n",
    "\n",
    "Again. It is **highly recommended** to read this [paper](https://arxiv.org/pdf/1411.2738.pdf)\n",
    "\n",
    "Example of visualization in tensorboard:\n",
    "https://projector.tensorflow.org\n",
    "\n",
    "Example of 2D visualisation:\n",
    "\n",
    "![2dword2vec](https://www.tensorflow.org/images/tsne.png)\n",
    "\n",
    "If you struggle with something, ask your neighbor. If it is not obvious for you, probably someone else is looking for the answer too. And in contrast, if you see that you can help someone - do it! Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from itertools import islice, product, chain\n",
    "from argparse import Namespace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramBatcher():\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, file_path):\n",
    "        with open(file_path) as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        return cls(text)\n",
    "    \n",
    "    def _tokenize(self):\n",
    "        self.tokens = self.text.split()\n",
    "    \n",
    "    def _count_tokens(self):\n",
    "        self.token_counts = Counter(self.tokens)\n",
    "    \n",
    "    def _build_vocab(self, cutoff):\n",
    "        filtered_token_counts = dict(filter(lambda x: x[1] >= cutoff, self.token_counts.items()))\n",
    "        self.token_to_idx = {token:idx for (idx, (token, _)) \n",
    "                             in enumerate(filtered_token_counts.items())}\n",
    "        self.idx_to_token = {idx:token for (token, idx) in self.token_to_idx.items()}\n",
    "        self.vocab = set(self.token_to_idx)\n",
    "\n",
    "    def _filter_tokens(self):\n",
    "        self.tokens = [token for token in self.tokens if token in self.vocab]\n",
    "    \n",
    "    def _vectorize_tokens(self):\n",
    "        self.vectorized_tokens = [self.token_to_idx[token] for token in self.tokens]\n",
    "    \n",
    "    def _create_sliding_window(self, window_size):\n",
    "        tokens_size = len(self.tokens)\n",
    "\n",
    "        for i in range(0, tokens_size):\n",
    "            center_word = islice(self.vectorized_tokens, i, i + 1)\n",
    "            left_context = islice(self.vectorized_tokens, i + 1, \n",
    "                                  min(tokens_size, i + window_size + 1))\n",
    "            right_context = islice(self.vectorized_tokens, \n",
    "                                   max(0, i - window_size), i)\n",
    "            yield from product(center_word, chain(left_context, right_context))\n",
    "    \n",
    "    def devectorize_tokens(self, indices):\n",
    "        return [self.idx_to_token[idx] for idx in indices]\n",
    "        \n",
    "    def prepare_data(self, cutoff=1):\n",
    "        self._tokenize()\n",
    "        self._count_tokens()\n",
    "        self._build_vocab(cutoff)\n",
    "        self._filter_tokens()\n",
    "        self._vectorize_tokens()\n",
    "        \n",
    "    def generate_batches(self, window_size=1, batch_size=1, drop_last=True):\n",
    "        window = self._create_sliding_window(window_size)\n",
    "        batch = list(zip(*islice(window, batch_size)))\n",
    "        x_batch, labels_batch = torch.tensor(batch[0]), torch.tensor(batch[1])\n",
    "\n",
    "        if drop_last:\n",
    "            while batch and len(batch[0]) == batch_size:\n",
    "                yield x_batch, labels_batch\n",
    "                batch = list(zip(*islice(window, batch_size)))\n",
    "                x_batch, labels_batch = torch.tensor(batch[0]), torch.tensor(batch[1])\n",
    "        else:\n",
    "            while batch:\n",
    "                yield x_batch, labels_batch\n",
    "                batch = list(zip(*islice(window, batch_size)))\n",
    "                x_batch, labels_batch = torch.tensor(batch[0]), torch.tensor(batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveWord2VecClassifier(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocabulary_size,\n",
    "                                      embedding_dim=embedding_size)\n",
    "        self.fc1 = nn.Linear(in_features=embedding_size,\n",
    "                             out_features=vocabulary_size,\n",
    "                             bias=False)\n",
    "    \n",
    "    def forward(self, x_in):\n",
    "        x_embedded = self.embedding(x_in)\n",
    "        y_out = self.fc1(x_embedded)\n",
    "        \n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    file_path = DATA_PATH/'text8',\n",
    "    \n",
    "    embedding_size = 50,\n",
    "    \n",
    "    seed = 42,\n",
    "    cutoff = 10,\n",
    "    window_size = 1,\n",
    "    batch_size = 128,\n",
    "    learning_rate = 0.01,\n",
    "    iterations = 10000,\n",
    "    early_stopping_criteria = 1e8,\n",
    "    \n",
    "    cuda=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'batch_idx': 0,\n",
    "            'loss': []}\n",
    "\n",
    "def update_train_state(args, train_state):\n",
    "    if train_state['batch_idx'] == 0:\n",
    "        train_state['stop_early'] = False\n",
    "    else:\n",
    "        loss = train_state['loss'][-1]\n",
    "\n",
    "        if loss < train_state['early_stopping_best_val']:\n",
    "            train_state['early_stopping_best_val'] = loss\n",
    "        else:\n",
    "            train_state['early_stopping_step'] += 1 \n",
    "    \n",
    "        train_state['stop_early'] = train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    args.cuda=False\n",
    "    \n",
    "print(f'Using CUDA: {args.cuda}')\n",
    "args.device = torch.device('cuda' if args.cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47134"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_batcher = SkipGramBatcher.from_file(args.file_path)\n",
    "sg_batcher.prepare_data(cutoff=args.cutoff)\n",
    "\n",
    "vocabulary_size = len(sg_batcher.vocab)\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1785c034b7c84ac2b4533b82b713c7e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=10000, style=ProgressStyle(description_width='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-1637a80b5adf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mlabels_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-3ff4a73b2a27>\u001b[0m in \u001b[0;36mgenerate_batches\u001b[0;34m(self, window_size, batch_size, drop_last)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-3ff4a73b2a27>\u001b[0m in \u001b[0;36m_create_sliding_window\u001b[0;34m(self, window_size)\u001b[0m\n\u001b[1;32m     38\u001b[0m             right_context = islice(self.vectorized_tokens, \n\u001b[1;32m     39\u001b[0m                                    max(0, i - window_size), i)\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenter_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdevectorize_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "set_seeds(args.seed)\n",
    "\n",
    "classifier = NaiveWord2VecClassifier(vocabulary_size=vocabulary_size,\n",
    "                                     embedding_size=args.embedding_size)\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = optim.Adam(params=classifier.parameters(),\n",
    "                      lr=args.learning_rate)\n",
    "\n",
    "train_bar = tqdm_notebook(desc='Training',\n",
    "                          position=1,\n",
    "                          total=args.iterations)\n",
    "\n",
    "batch_generator = sg_batcher.generate_batches(window_size=args.window_size, \n",
    "                                              batch_size=args.batch_size)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "running_loss = 0.\n",
    "classifier.train()\n",
    "\n",
    "for batch_idx, (x_batch, labels_batch) in enumerate(batch_generator, 1):\n",
    "    x_batch = x_batch.to(args.device)\n",
    "    labels_batch = labels_batch.to(args.device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    y_pred = classifier(x_in=x_batch)\n",
    "    \n",
    "    loss = loss_func(y_pred, labels_batch)\n",
    "    loss_value = loss.item()\n",
    "    running_loss += (loss_value - running_loss) / (batch_idx)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    train_state['batch_idx'] = batch_idx\n",
    "    train_state['loss'].append(running_loss)\n",
    "    \n",
    "    if train_state['stop_early']:\n",
    "        train_state = update_train_state(args=args,\n",
    "                                         train_state=train_state)\n",
    "    \n",
    "    train_params = dict(loss=running_loss,\n",
    "                        batch=batch_idx,\n",
    "                        lr=optimizer.param_groups[0]['lr'])\n",
    "    train_bar.set_postfix(train_params)\n",
    "    train_bar.update()\n",
    "    \n",
    "    if (batch_idx == args.iterations):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_state_df = pd.DataFrame(train_state)\n",
    "train_state_df['loss'].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([47134, 50])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(classifier.parameters())[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([47134, 50])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(classifier.parameters())[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = classifier.embedding.weight\n",
    "W_prime = classifier.fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([47134, 50])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = (W + W_prime) / 2\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_batcher = SkipGramBatcher.from_file(args.file_path)\n",
    "sg_batcher.prepare_data(cutoff=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0]), tensor([1, 2, 3, 2, 3]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = sg_batcher.generate_batches(window_size=5, \n",
    "                                batch_size=5)\n",
    "x_batch, labels_batch = next(g)\n",
    "\n",
    "x_batch, labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 50)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = len(sg_batcher.vocab)\n",
    "embedding_size = 50\n",
    "\n",
    "vocabulary_size, embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = NaiveWord2VecClassifier(vocabulary_size=vocabulary_size,\n",
    "                              embedding_size=embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1679, -0.7678, -0.4254,  0.3084,  0.4510, -0.2278, -0.0301,  0.1163,\n",
       "          0.0782,  0.1542, -0.5275, -0.5969, -0.2852,  0.9326, -0.0609,  1.0134,\n",
       "          0.7858, -0.3970, -0.0972,  0.2352, -0.1621, -0.3524],\n",
       "        [ 0.1679, -0.7678, -0.4254,  0.3084,  0.4510, -0.2278, -0.0301,  0.1163,\n",
       "          0.0782,  0.1542, -0.5275, -0.5969, -0.2852,  0.9326, -0.0609,  1.0134,\n",
       "          0.7858, -0.3970, -0.0972,  0.2352, -0.1621, -0.3524],\n",
       "        [ 0.1679, -0.7678, -0.4254,  0.3084,  0.4510, -0.2278, -0.0301,  0.1163,\n",
       "          0.0782,  0.1542, -0.5275, -0.5969, -0.2852,  0.9326, -0.0609,  1.0134,\n",
       "          0.7858, -0.3970, -0.0972,  0.2352, -0.1621, -0.3524],\n",
       "        [ 0.1679, -0.7678, -0.4254,  0.3084,  0.4510, -0.2278, -0.0301,  0.1163,\n",
       "          0.0782,  0.1542, -0.5275, -0.5969, -0.2852,  0.9326, -0.0609,  1.0134,\n",
       "          0.7858, -0.3970, -0.0972,  0.2352, -0.1621, -0.3524],\n",
       "        [ 0.1679, -0.7678, -0.4254,  0.3084,  0.4510, -0.2278, -0.0301,  0.1163,\n",
       "          0.0782,  0.1542, -0.5275, -0.5969, -0.2852,  0.9326, -0.0609,  1.0134,\n",
       "          0.7858, -0.3970, -0.0972,  0.2352, -0.1621, -0.3524]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf(x_batch)\n",
    "# y_pred = F.softmax(y_pred, dim=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 2, 3])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7678, -0.4254,  0.3084, -0.4254,  0.3084], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[range(5), labels_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.9903, 3.6480, 2.9141, 3.6480, 2.9141], grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.log(y_pred[range(5), labels_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = torch.zeros((5, vocabulary_size), dtype=torch.long)\n",
    "y_true[range(5), labels_batch] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 22]), torch.Size([5, 22]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.shape, y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3.9903, 3.6480, 2.9141, 3.6480, 2.9141], grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss(reduce=False)\n",
    "loss(y_pred, labels_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = torch.zeros((5, vocabulary_size))\n",
    "arr[range(5), labels_batch] = 1\n",
    "arr.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 2, 3])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in log\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "loss = -np.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
