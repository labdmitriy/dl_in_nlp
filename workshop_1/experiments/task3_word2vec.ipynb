{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive word2vec\n",
    "\n",
    "This task can be formulated very simply. Follow this [paper](https://arxiv.org/pdf/1411.2738.pdf) and implement word2vec like a two-layer neural network with matrices $W$ and $W'$. One matrix projects words to low-dimensional 'hidden' space and the other - back to high-dimensional vocabulary space.\n",
    "\n",
    "![word2vec](https://i.stack.imgur.com/6eVXZ.jpg)\n",
    "\n",
    "You can use TensorFlow/PyTorch (numpy too, if you love to calculate gradients on your own and want some extra points, but don't forget to numerically check your gradients) and code from your previous task. Again: you don't have to implement negative sampling (you may reduce your vocabulary size for faster computation).\n",
    "\n",
    "**Results of this task**:\n",
    " * trained word vectors (mention somewhere, how long it took to train)\n",
    " * plotted loss (so we can see that it has converged)\n",
    " * function to map token to corresponding word vector\n",
    " * beautiful visualizations (PCE, T-SNE), you can use TensorBoard and play with your vectors in 3D (don't forget to add screenshots to the task)\n",
    " * qualitative evaluations of word vectors: nearest neighbors, word analogies\n",
    "\n",
    "**Extra:**\n",
    " * quantitative evaluation:\n",
    "   * for intrinsic evaluation you can find datasets [here](https://aclweb.org/aclwiki/Analogy_(State_of_the_art))\n",
    "   * for extrincis evaluation you can use [these](https://medium.com/@dataturks/rare-text-classification-open-datasets-9d340c8c508e)\n",
    "\n",
    "Also, you can find any other datasets for quantitative evaluation. If you chose to do this, please use the same datasets across tasks 3, 4, 5 and 6.\n",
    "\n",
    "Again. It is **highly recommended** to read this [paper](https://arxiv.org/pdf/1411.2738.pdf)\n",
    "\n",
    "Example of visualization in tensorboard:\n",
    "https://projector.tensorflow.org\n",
    "\n",
    "Example of 2D visualisation:\n",
    "\n",
    "![2dword2vec](https://www.tensorflow.org/images/tsne.png)\n",
    "\n",
    "If you struggle with something, ask your neighbor. If it is not obvious for you, probably someone else is looking for the answer too. And in contrast, if you see that you can help someone - do it! Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from itertools import islice, product, chain\n",
    "from argparse import Namespace\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('../data')\n",
    "MODELS_PATH = Path('../models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramBatcher():\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, file_path):\n",
    "        with open(file_path) as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        return cls(text)\n",
    "    \n",
    "    def _tokenize(self):\n",
    "        self.tokens = self.text.split()\n",
    "    \n",
    "    def _count_tokens(self):\n",
    "        self.token_counts = Counter(self.tokens)\n",
    "    \n",
    "    def _build_vocab(self, cutoff):\n",
    "        filtered_token_counts = dict(filter(lambda x: x[1] >= cutoff, self.token_counts.items()))\n",
    "        self.token_to_idx = {token:idx for (idx, (token, _)) \n",
    "                             in enumerate(filtered_token_counts.items())}\n",
    "        self.idx_to_token = {idx:token for (token, idx) in self.token_to_idx.items()}\n",
    "        self.vocab = set(self.token_to_idx)\n",
    "\n",
    "    def _filter_tokens(self):\n",
    "        self.tokens = [token for token in self.tokens if token in self.vocab]\n",
    "    \n",
    "    def _vectorize_tokens(self):\n",
    "        self.vectorized_tokens = [self.token_to_idx[token] for token in self.tokens]\n",
    "    \n",
    "    def _create_sliding_window(self, window_size, stride):\n",
    "        tokens_size = len(self.tokens)\n",
    "\n",
    "        for i in range(0, tokens_size, stride):\n",
    "#             center_word = islice(self.vectorized_tokens, i, i + 1)\n",
    "#             left_context = islice(self.vectorized_tokens, i + 1, \n",
    "#                                   min(tokens_size, i + window_size + 1))\n",
    "#             right_context = islice(self.vectorized_tokens, \n",
    "#                                    max(0, i - window_size), i)\n",
    "#             yield from product(center_word, chain(left_context, right_context))\n",
    "\n",
    "#             window_size = np.random.randint(1, max_window_size+1)\n",
    "            center_word = self.vectorized_tokens[i:i+1]\n",
    "            left_context = self.vectorized_tokens[max(0, i - window_size): i]\n",
    "            right_context = self.vectorized_tokens[i + 1: min(tokens_size, i + window_size + 1)]\n",
    "            yield from product(center_word, chain(left_context, right_context))\n",
    "        \n",
    "    def devectorize_tokens(self, indices):\n",
    "        return [self.idx_to_token[idx] for idx in indices]\n",
    "        \n",
    "    def prepare_data(self, cutoff=1):\n",
    "        self._tokenize()\n",
    "        self._count_tokens()\n",
    "        self._build_vocab(cutoff)\n",
    "        self._filter_tokens()\n",
    "        self._vectorize_tokens()\n",
    "        \n",
    "    def generate_batches(self, window_size=1, stride=1, batch_size=1, drop_last=True):\n",
    "        window = self._create_sliding_window(window_size, stride)\n",
    "        batch = list(zip(*islice(window, batch_size)))\n",
    "        x_batch, labels_batch = torch.tensor(batch[0]), torch.tensor(batch[1])\n",
    "\n",
    "        if drop_last:\n",
    "            while batch and len(batch[0]) == batch_size:\n",
    "                yield x_batch, labels_batch\n",
    "                batch = list(zip(*islice(window, batch_size)))\n",
    "                x_batch, labels_batch = torch.tensor(batch[0]), torch.tensor(batch[1])\n",
    "        else:\n",
    "            while batch:\n",
    "                yield x_batch, labels_batch\n",
    "                batch = list(zip(*islice(window, batch_size)))\n",
    "                x_batch, labels_batch = torch.tensor(batch[0]), torch.tensor(batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveWord2VecClassifier(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_size):\n",
    "        super().__init__()\n",
    "        \n",
    "#         self.embedding = nn.utils.weight_norm(nn.Embedding(num_embeddings=vocabulary_size,\n",
    "#                                       embedding_dim=embedding_size), dim=1)\n",
    "# #                                       max_norm=1.0)\n",
    "# #                                       scale_grad_by_freq=True)\n",
    "                                      \n",
    "#         self.fc1 = nn.utils.weight_norm(nn.Linear(in_features=embedding_size,\n",
    "#                              out_features=vocabulary_size,\n",
    "#                              bias=False), dim=1)\n",
    "\n",
    "        self.embedding = nn.utils.weight_norm(nn.Embedding(num_embeddings=vocabulary_size,\n",
    "                                              embedding_dim=embedding_size), dim=1)\n",
    "\n",
    "        self.fc1 = nn.utils.weight_norm(nn.Linear(in_features=embedding_size,\n",
    "                             out_features=vocabulary_size,\n",
    "                             bias=False), dim=1)\n",
    "        \n",
    "    def forward(self, x_in):\n",
    "#         self.embedding.weight.data = F.normalize(self.embedding.weight.data, p=2, dim=1)\n",
    "#         self.fc1.weight.data = F.normalize(self.fc1.weight.data, p=2, dim=1)\n",
    "        x_embedded = self.embedding(x_in)\n",
    "        y_out = self.fc1(x_embedded)\n",
    "        \n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    file_path = DATA_PATH/'text8',\n",
    "    model_state_path = MODELS_PATH/'naive_word2vec_embeddings.pth',\n",
    "    weights_path = MODELS_PATH/'weights.npz',\n",
    "    \n",
    "    embedding_size = 100,\n",
    "    \n",
    "    seed = 42,\n",
    "    cutoff = 10, # 10\n",
    "    window_size = 3, # 1\n",
    "    stride = 1, # 1\n",
    "    batch_size = 1024, #  1024\n",
    "    learning_rate = 0.03, # 0.03\n",
    "    iterations = 1000,\n",
    "    save_iterations = 100,\n",
    "    early_stopping_criteria = 1e8,\n",
    "    factor=0.5, # 0.7\n",
    "    patience=5000, # 1000\n",
    "    \n",
    "    cuda=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': [], # args.learning_rate\n",
    "            'batch_idx': 0,\n",
    "            'loss': [],\n",
    "            'model_file_name': args.model_state_path}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    if train_state['batch_idx'] == 0:\n",
    "        train_state['stop_early'] = False\n",
    "        torch.save(model.state_dict(), train_state['model_file_name'])\n",
    "    else:\n",
    "        loss = train_state['loss'][-1]\n",
    "\n",
    "        if loss < train_state['early_stopping_best_val']:\n",
    "            train_state['early_stopping_best_val'] = loss\n",
    "            train_state['early_stopping_step'] = 0\n",
    "            \n",
    "            if train_state['batch_idx'] % args.save_iterations == 0:\n",
    "                torch.save(model.state_dict(), train_state['model_file_name'])\n",
    "        else:\n",
    "            train_state['early_stopping_step'] += 1 \n",
    "    \n",
    "        train_state['stop_early'] = train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    args.cuda=False\n",
    "    \n",
    "print(f'Using CUDA: {args.cuda}')\n",
    "args.device = torch.device('cuda' if args.cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47134"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_batcher = SkipGramBatcher.from_file(args.file_path)\n",
    "sg_batcher.prepare_data(cutoff=args.cutoff)\n",
    "\n",
    "vocabulary_size = len(sg_batcher.vocab)\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43ce9219c7e4b9d86ecdeb69c42c52a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=97037, style=ProgressStyle(description_width='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_seeds(args.seed)\n",
    "\n",
    "classifier = NaiveWord2VecClassifier(vocabulary_size=vocabulary_size,\n",
    "                                     embedding_size=args.embedding_size)\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = optim.Adam(params=classifier.parameters(),\n",
    "                      lr=args.learning_rate)\n",
    "\n",
    "epoch_size = 2 * (args.window_size * len(sg_batcher.tokens) \n",
    "                  - np.math.factorial(args.window_size)) // (args.batch_size * args.stride)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min',\n",
    "                                                 factor=args.factor,\n",
    "                                                 patience=args.patience)\n",
    "\n",
    "train_bar = tqdm_notebook(desc='Training',\n",
    "                          position=1,\n",
    "#                           total=args.iterations,\n",
    "                          total=epoch_size)\n",
    "\n",
    "batch_generator = sg_batcher.generate_batches(window_size=args.window_size, \n",
    "                                              batch_size=args.batch_size)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "writer = SummaryWriter(log_dir='logs/task_3', comment='embedding_training')\n",
    "\n",
    "running_loss = 0.\n",
    "classifier.train()\n",
    "\n",
    "try:\n",
    "    for batch_idx, (x_batch, labels_batch) in enumerate(batch_generator, 1):\n",
    "        x_batch = x_batch.to(args.device)\n",
    "        labels_batch = labels_batch.to(args.device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = classifier(x_in=x_batch)\n",
    "\n",
    "        loss = loss_func(y_pred, labels_batch)\n",
    "        loss_value = loss.item()\n",
    "        running_loss += (loss_value - running_loss) / (batch_idx)\n",
    "        loss.backward()\n",
    "        \n",
    "        learning_rate = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        train_state['batch_idx'] = batch_idx\n",
    "        train_state['loss'].append(running_loss)\n",
    "        train_state['learning_rate'].append(learning_rate)\n",
    "#         writer.add_scalar('loss', scalar_value=loss, global_step=batch_idx)\n",
    "\n",
    "\n",
    "        train_state = update_train_state(args=args,\n",
    "                                         model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        train_params = dict(loss=running_loss,\n",
    "                            lr=learning_rate,\n",
    "                            early_step=train_state['early_stopping_step'],\n",
    "                            early_best=train_state['early_stopping_best_val'])\n",
    "        train_bar.set_postfix(train_params)\n",
    "        train_bar.update()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step(train_state['loss'][-1])\n",
    "\n",
    "        if train_state['stop_early'] or (batch_idx == epoch_size):\n",
    "            torch.save(classifier.state_dict(), str(train_state['model_file_name']) + '_last')\n",
    "            break\n",
    "except KeyboardInterrupt:\n",
    "    print('Exit training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_state_df = pd.DataFrame(train_state)\n",
    "train_state_df['loss'].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAEyZJREFUeJzt3X+MXedd5/H3p57aaUvzo7bLZu2kThSrwk0RNKM0LCzabWhwoNRZSISrqAklyFAUiaVCK1co0W4UCbJCdLc0ajc0LUmWkhQD2xE48hYSpAUV4wkJTZzUdJoGMnGWOkoa0oKTpnz54z4TbsYznusf9865nvdLuppznvOc4+ccH8/H5zznuSdVhSRJr1nuBkiSusFAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkZmK5G3As1q1bV5s2bVruZkjS2Fi3bh179uzZU1Vbl6o7VoGwadMmpqenl7sZkjRWkqwbpJ63jCRJgIEgSWoMBEkSYCBIkhoDQZIEDBgISbYmOZBkJsnOBZavSXJPW743yaZWfnGSh9rnr5P8p0G3KUkarSUDIckq4FbgcmAL8L4kW+ZVuw54rqouAD4C3NLKHwEmq+p7gK3A/0oyMeA2JUkjNMg4hIuBmap6HCDJ3cA24NG+OtuA/9qmdwEfS5Kq+se+OqcBc+/rHGSbR/j7fzjMr//fAwM0WV30xtNeywe+fxMTq7xTKXXRIIGwAXiyb34WeOdidarq5STPA2uBZ5K8E/gU8Bbg/W35INsEIMkOYAfA6n9zAb9x/8wATVbXzL26+5Lz1/L2jWcsb2MkLWiQQMgCZTVonaraC7wtyXcBdyS5d8Bt0ta/DbgNYHJysqZ/5UcHaLK65v4vfY0P/NY+vl0L/jVL6oBBrt1ngXP65jcCBxerk2QCOAN4tr9CVT0GfBO4cMBtSpJGaJBA2AdsTnJektXAdmBqXp0p4No2fSVwX1VVW2cCIMlbgLcCTwy4TUnSCC15y6jd878e2AOsAj5VVfuT3ARMV9UUcDtwV5IZelcG29vqPwDsTPIt4J+Bn6+qZwAW2uZJ3jd1UHnLSOqsgb7ttKp2A7vnld3YN30YuGqB9e4C7hp0m5Kk5ePzfxqNhR4jkNQpBoIkCTAQJEmNgaCRsktZ6i4DQZIEGAgaEfuUpe4zECRJgIGgEXNcmtRdBoIkCTAQNCKJvQhS1xkIkiTAQJAkNQaCRsxeZamrDARJEmAgaETsUpa6z0CQJAEGgkbMgWlSdxkIkiTAQNCIOC5N6j4DQZIEGAiSpMZA0EjZpyx1l4EgSQIMBI1IHJomdZ6BIEkCDASNmAPTpO4yECRJgIGgEXFgmtR9BoIkCTAQJEmNgaCRKnuVpc4yECRJgIGgEbFPWeo+A0GSBBgIGjF7EKTuGigQkmxNciDJTJKdCyxfk+Setnxvkk2t/N1JHkjycPv5rr51/rRt86H2efPJ2ilJ0rGbWKpCklXArcC7gVlgX5Kpqnq0r9p1wHNVdUGS7cAtwE8CzwA/VlUHk1wI7AE29K13dVVNn6R9UZfZiSB13iBXCBcDM1X1eFW9BNwNbJtXZxtwR5veBVyaJFX1YFUdbOX7gdOSrDkZDZcknVyDBMIG4Mm++Vle/b/8V9WpqpeB54G18+r8BPBgVb3YV/bpdrvohmThLzdIsiPJdJLpQ4cODdBcSdLxGCQQFvpFPb9v8Kh1kryN3m2kn+1bfnVVvR349+3z/oX+8Kq6raomq2py/fr1AzRXXea4NKm7BgmEWeCcvvmNwMHF6iSZAM4Anm3zG4E/AK6pqq/MrVBVT7WfLwCfoXdrSpK0TAYJhH3A5iTnJVkNbAem5tWZAq5t01cC91VVJTkT+CPgw1X153OVk0wkWdemXwu8B3jkxHZFXeYb06TuWzIQWp/A9fSeEHoM+GxV7U9yU5L3tmq3A2uTzAAfAuYeTb0euAC4Yd7jpWuAPUm+CDwEPAX85sncMUnSsVnysVOAqtoN7J5XdmPf9GHgqgXWuxm4eZHNXjR4M3WqKIemSZ3lSGVJEmAgaER8Y5rUfQaCJAkwECRJjYGg0bJPWeosA0GSBBgIGhH7lKXuMxAkSYCBoBGzC0HqLgNBkgQYCBqRRV53IalDDARJEmAgSJIaA0Ej5RvTpO4yECRJgIGgEbFPWeo+A0GSBBgIkqTGQNBI+QpNqbsMBI2EXQhS9xkIkiTAQJAkNQaCRsqBaVJ3GQiSJMBA0Ig4ME3qPgNBkgQYCJKkxkDQSNmnLHWXgaARsRNB6joDQZIEGAiSpMZA0EiVI9OkzjIQJEmAgaARcWCa1H0DBUKSrUkOJJlJsnOB5WuS3NOW702yqZW/O8kDSR5uP9/Vt85FrXwmyUcTf2VI0nJaMhCSrAJuBS4HtgDvS7JlXrXrgOeq6gLgI8AtrfwZ4Meq6u3AtcBdfet8HNgBbG6frSewH5KkEzTIFcLFwExVPV5VLwF3A9vm1dkG3NGmdwGXJklVPVhVB1v5fuC0djVxNnB6VX2her2MdwJXnPDeqPPsUpa6a5BA2AA82Tc/28oWrFNVLwPPA2vn1fkJ4MGqerHVn11imzqFeD9Q6r6JAeos9G95/n/0jlonydvo3Ua67Bi2ObfuDnq3ljj33HOXaqsk6TgNcoUwC5zTN78ROLhYnSQTwBnAs21+I/AHwDVV9ZW++huX2CYAVXVbVU1W1eT69esHaK4k6XgMEgj7gM1JzkuyGtgOTM2rM0Wv0xjgSuC+qqokZwJ/BHy4qv58rnJVPQ28kOSS9nTRNcDnTnBfNA7sRJA6a8lAaH0C1wN7gMeAz1bV/iQ3JXlvq3Y7sDbJDPAhYO7R1OuBC4AbkjzUPm9uyz4IfBKYAb4C3HuydkqSdOwG6UOgqnYDu+eV3dg3fRi4aoH1bgZuXmSb08CFx9JYjS+HmUjd50hlSRJgIEiSGgNBI1X2KkudZSBoJOxBkLrPQJAkAQaCJKkxEDRSvjBN6i4DQZIEGAgaEcelSd1nIEiSAANBktQYCBopO5Wl7jIQNBJxaJrUeQaCJAkwECRJjYGgkbILQeouA0GSBBgIGhEHpkndZyBIkgADQZLUGAgaqXJkmtRZBoIkCTAQJEmNgSBJAgwEjZg9CFJ3GQiSJMBA0Ig4ME3qPgNBkgQYCJKkxkDQSDkuTeouA0Ej4RvTpO4zECRJgIEgSWoMBI2YnQhSVxkIkiRgwEBIsjXJgSQzSXYusHxNknva8r1JNrXytUnuT/KNJB+bt86ftm0+1D5vPhk7pG5yYJrUfRNLVUiyCrgVeDcwC+xLMlVVj/ZVuw54rqouSLIduAX4SeAwcANwYfvMd3VVTZ/gPkiSToJBrhAuBmaq6vGqegm4G9g2r8424I42vQu4NEmq6ptV9Wf0gkGS1GFLXiEAG4An++ZngXcuVqeqXk7yPLAWeGaJbX86ybeB3wNuLl+ndcr7n38yw2/v/bvlboY0MmefcRq/+uPfzWte0/37poMEwkJ7Mf8X9yB15ru6qp5K8kZ6gfB+4M4j/vBkB7AD4Nxzz126teqkt6x9Pf/xrev5+j99i2+8+PJyN0caiUMvvMj/+/Iz7Lz8u3jTG1Yvd3OWNEggzALn9M1vBA4uUmc2yQRwBvDs0TZaVU+1ny8k+Qy9W1NHBEJV3QbcBjA5OekVxJh6/eoJPv2Bi5e7GdJI3fmFJ7jxc/uXuxkDG6QPYR+wOcl5SVYD24GpeXWmgGvb9JXAfUe7/ZNkIsm6Nv1a4D3AI8faeEkaB+NyN3zJK4TWJ3A9sAdYBXyqqvYnuQmYrqop4HbgriQz9K4Mts+tn+QJ4HRgdZIrgMuAvwX2tDBYBfwx8Jsndc8kScdkkFtGVNVuYPe8shv7pg8DVy2y7qZFNnvRYE2UpPE017k6HtcHjlSWJDUGgiQNSxuiPyZdCAaCJKnHQJCkIen+ULRXMxAkachqTLqVDQRJGpJx+5ZfA0GShm08LhAMBElSj4EgSUOS1q08JhcIBoIkqcdAkKQhmetUdmCaJGmsGAiSNCT/+uV243GJYCBIkgADQZKGxoFpkqRXsVNZkla4jNnX2xkIkjRkY3KBYCBIknoMBEkallcGpo3HNYKBIEkCDARJGppXBqaNxwWCgSBJ6jEQJGlIMmYj0wwESRJgIEjS0IzX9YGBIElDZ6eyJGmsGAiSNCSvvDFtTL68wkCQJAEGgiQNje9UliSNJQNBkobE9yFIkl5lTO4YGQiSNCxj9s0VgwVCkq1JDiSZSbJzgeVrktzTlu9NsqmVr01yf5JvJPnYvHUuSvJwW+ejGbcv/ZCkAZ0y70NIsgq4Fbgc2AK8L8mWedWuA56rqguAjwC3tPLDwA3ALy2w6Y8DO4DN7bP1eHZAknRyDHKFcDEwU1WPV9VLwN3Atnl1tgF3tOldwKVJUlXfrKo/oxcMr0hyNnB6VX2hetF5J3DFieyIJHXVeFwfDBYIG4An++ZnW9mCdarqZeB5YO0S25xdYpuSpBEaJBAWurc/P/AGqXNc9ZPsSDKdZPrQoUNH2aQkdctc1+iYdCEMFAizwDl98xuBg4vVSTIBnAE8u8Q2Ny6xTQCq6raqmqyqyfXr1w/QXEnS8RgkEPYBm5Ocl2Q1sB2YmldnCri2TV8J3FdH6VavqqeBF5Jc0p4uugb43DG3XpI6bNwenZxYqkJVvZzkemAPsAr4VFXtT3ITMF1VU8DtwF1JZuhdGWyfWz/JE8DpwOokVwCXVdWjwAeB3wJeB9zbPpJ0ChqPe0ZLBgJAVe0Gds8ru7Fv+jBw1SLrblqkfBq4cNCGStK4GbfRVY5UlqQhO5U6lSVJK4CBIElDMvdtp2NygWAgSJJ6DARJGhLfmCZJGksGgiQNyZg9dWogSNKw1Zh0KxsIkjQkDkyTJL2KncqSpLFiIEjS0Jx670OQJK0ABoIkDckrA9N8ykiSNE4MBEkakjF76nSwF+RIko7fz/3vBzhtYtVyN2NJBoIkDclFbzmLH3/HBg5/69vL2o4/HrBealyehwImJydrenp6uZshSWMlyQNVNblUPfsQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpGauBaUleAA4sdzs6Zh3wzHI3omM8JkfymBxppRyTZwCqautSFcftqysODDLabiVJMu0xeTWPyZE8JkfymBzJW0aSJMBAkCQ14xYIty13AzrIY3Ikj8mRPCZH8pjMM1adypKk4Rm3KwRJ0pCMRSAk2ZrkQJKZJDuXuz0nW5Jzktyf5LEk+5P8Qit/U5LPJ/ly+3lWK0+Sj7bj8cUk7+jb1rWt/peTXNtXflGSh9s6H00yFm/3S7IqyYNJ/rDNn5dkb9u/e5KsbuVr2vxMW76pbxsfbuUHkvxwX/nYnVdJzkyyK8mX2vnyfSv9PEnyi+3fzSNJfifJaSv9PDluVdXpD7AK+ApwPrAa+Gtgy3K36yTv49nAO9r0G4G/AbYA/x3Y2cp3Are06R8B7qX3ytZLgL2t/E3A4+3nWW36rLbsL4Hva+vcC1y+3Ps94LH5EPAZ4A/b/GeB7W36E8AH2/TPA59o09uBe9r0lnbOrAHOa+fSqnE9r4A7gJ9p06uBM1fyeQJsAL4KvK7v/PiplX6eHO9nHK4QLgZmqurxqnoJuBvYtsxtOqmq6umq+qs2/QLwGL0TfRu9XwC0n1e06W3AndXzF8CZSc4Gfhj4fFU9W1XPAZ8HtrZlp1fVF6p39t/Zt63OSrIR+FHgk20+wLuAXa3K/GMyd6x2AZe2+tuAu6vqxar6KjBD75wau/MqyenADwK3A1TVS1X1dVb4eUJvPNXrkkwArweeZgWfJydiHAJhA/Bk3/xsKzsltUvY7wX2At9ZVU9DLzSAN7dqix2To5XPLlDedf8D+C/AP7f5tcDXq+rlNt+/H6/se1v+fKt/rMeqy84HDgGfbrfRPpnkDazg86SqngJ+Dfg7ekHwPPAAK/s8OW7jEAgL3cM8JR+NSvIdwO8B/7mq/uFoVRcoq+Mo76wk7wG+VlUP9BcvULWWWHbKHBN6/xN+B/Dxqvpe4Jv0bhEt5pQ/Jq2/ZBu92zz/FngDcPkCVVfSeXLcxiEQZoFz+uY3AgeXqS1Dk+S19MLgt6vq91vx37fLeNrPr7XyxY7J0co3LlDeZd8PvDfJE/Qu099F74rhzHZrAF69H6/se1t+BvAsx36sumwWmK2qvW1+F72AWMnnyQ8BX62qQ1X1LeD3gX/Hyj5Pjts4BMI+YHN7amA1vY6gqWVu00nV7mHeDjxWVb/et2gKmHsC5Frgc33l17SnSC4Bnm+3CvYAlyU5q/3P6TJgT1v2QpJL2p91Td+2OqmqPlxVG6tqE72/8/uq6mrgfuDKVm3+MZk7Vle2+tXKt7enS84DNtPrOB2786qq/j/wZJK3tqJLgUdZwecJvVtFlyR5fWvz3DFZsefJCVnuXu1BPvSelvgber39v7zc7RnC/v0AvcvQLwIPtc+P0Lu3+SfAl9vPN7X6AW5tx+NhYLJvWz9Nr0NsBvhAX/kk8Ehb52O0QYnj8AH+A//6lNH59P6hzgC/C6xp5ae1+Zm2/Py+9X+57fcB+p6aGcfzCvgeYLqdK/+H3lNCK/o8Af4b8KXW7rvoPSm0os+T4/04UlmSBIzHLSNJ0ggYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIA+BeMKp8LInaDAgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_state_df['learning_rate'].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = list(sg_batcher.token_to_idx)\n",
    "W = classifier.embedding.weight.detach().cpu().numpy()\n",
    "W_prime = classifier.fc1.weight.detach().cpu().numpy()\n",
    "\n",
    "W_avg = (W + W_prime) / 2\n",
    "np.savez(args.weights_path, W=W, W_prime=W_prime, W_avg=W_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 100]), torch.Size([47134, 100]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.embedding.weight_g.shape, classifier.embedding.weight_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 100]), torch.Size([47134, 100]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fc1.weight_g.shape, classifier.fc1.weight_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer.add_embedding(W, metadata=metadata, tag='W')\n",
    "# writer.add_embedding(W_prime, metadata=metadata, tag='W_prime')\n",
    "# writer.add_embedding(W_avg, metadata=metadata, tag='W_avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.load(args.weights_path)\n",
    "W = weights['W']\n",
    "W_prime = weights['W_prime']\n",
    "W_avg = weights['W_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-15.041774, 13.829485, -0.60154366, 0.5448065)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.min(), W.max(), W_prime.min(), W_prime.max()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.heatmap(W)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.heatmap(W_prime)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.heatmap(W_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "state_dict = torch.load('../models/naive_word2vec_embeddings_final.pth', map_location='cpu')\n",
    "\n",
    "clf = NaiveWord2VecClassifier(vocabulary_size=vocabulary_size,\n",
    "                              embedding_size=100)\n",
    "clf.load_state_dict(state_dict)\n",
    "\n",
    "W = clf.embedding.weight.detach().numpy()\n",
    "W_prime = clf.fc1.weight.detach().numpy()\n",
    "W_avg = (W + W_prime) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 19.550741, 9.950522)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_norm = W / np.linalg.norm(W, axis=1, keepdims=True)\n",
    "W_prime_norm = W_avg / np.linalg.norm(W_prime, axis=1, keepdims=True)\n",
    "W_avg_norm = (W_norm + W_prime_norm) / 2\n",
    "np.abs(W_norm).max(), np.abs(W_prime_norm).max(), np.abs(W_avg_norm).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(embs_reduced, token_to_idx, tokens):\n",
    "    token_embs = embs_reduced[[token_to_idx[token] for token in tokens]]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    ax.scatter(token_embs[:, 0], token_embs[:, 1], alpha=0.2)\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        ax.annotate(token, (token_embs[i, 0], token_embs[i, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 336 ms, sys: 4 ms, total: 340 ms\n",
      "Wall time: 87.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pca = PCA(n_components=2)\n",
    "embs_pca = pca.fit_transform(W_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokens = ['man', 'woman', 'king', 'queen', 'mother', 'father']\n",
    "# tokens = list(dict(sg_batcher.token_counts.most_common(100)).keys())\n",
    "plot_embeddings(embs_pca, sg_batcher.token_to_idx, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tsne = TSNE(n_components=2, n_iter=1000, n_jobs=-1)\n",
    "embs_tsne = tsne.fit_transform(W_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['man', 'woman', 'king', 'queen', 'mother', 'father']\n",
    "# tokens = list(dict(sg_batcher.token_counts.most_common(100)).keys())\n",
    "plot_embeddings(embs_tsne, sg_batcher.token_to_idx, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "umap = UMAP(metric='cosine')\n",
    "embs_umap = umap.fit_transform(W_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['man', 'woman', 'king', 'queen', 'mother', 'father']\n",
    "# tokens = list(dict(sg_batcher.token_counts.most_common(100)).keys())\n",
    "plot_embeddings(embs_umap, sg_batcher.token_to_idx, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsExplorer():\n",
    "    def __init__(self, token_to_idx, vectors, metric='euclidean'):\n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.idx_to_token = {idx: token for token, idx \n",
    "                             in self.token_to_idx.items()}\n",
    "        self.vectors = vectors\n",
    "        self.metric = metric\n",
    "        \n",
    "        self.index = AnnoyIndex(vectors.shape[1], metric=metric)\n",
    "        \n",
    "        print('Building index is started')\n",
    "        for i in self.token_to_idx.values():\n",
    "            self.index.add_item(i, self.vectors[i])\n",
    "        \n",
    "        self.index.build(50)\n",
    "        print('Building index is finished')\n",
    "        \n",
    "    def get_embedding(self, token):\n",
    "        return self.vectors[self.token_to_idx[token]]\n",
    "    \n",
    "    def get_closest_to_vector(self, vector, n=1):\n",
    "        nn_indices = self.index.get_nns_by_vector(vector, n)\n",
    "        return [self.idx_to_token[neighbor] for neighbor in nn_indices]\n",
    "    \n",
    "    def compute_analogy(self, token1, token2, token3, n=20):\n",
    "        vec1 = self.get_embedding(token1)\n",
    "        vec2 = self.get_embedding(token2)\n",
    "        vec3 = self.get_embedding(token3)\n",
    "        vec4 = vec3 + vec2 - vec1\n",
    "        \n",
    "        tokens = set([token1, token2, token3])\n",
    "        closest_tokens = self.get_closest_to_vector(vec4, n=n)\n",
    "        closest_tokens = [token for token in closest_tokens\n",
    "                          if token not in tokens]\n",
    "        \n",
    "        if len(closest_tokens) == 0:\n",
    "            print('Could not find nearest neighbors for the computed vector')\n",
    "            return\n",
    "        \n",
    "        for token4 in closest_tokens:\n",
    "            print(f'{token1}:{token2} :: {token3}:{token4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building index is started\n",
      "Building index is finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingsExplorer at 0x7fa88460d6d8>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = EmbeddingsExplorer(sg_batcher.token_to_idx, W_avg, metric='euclidean')\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man:he :: woman:she\n",
      "man:he :: woman:briefly\n",
      "man:he :: woman:prison\n",
      "man:he :: woman:herself\n",
      "man:he :: woman:afterwards\n",
      "man:he :: woman:subsequently\n",
      "man:he :: woman:himself\n",
      "man:he :: woman:him\n",
      "man:he :: woman:parents\n",
      "man:he :: woman:publicly\n",
      "man:he :: woman:again\n",
      "man:he :: woman:marry\n",
      "man:he :: woman:apparently\n",
      "man:he :: woman:secretly\n",
      "man:he :: woman:marriage\n",
      "man:he :: woman:succeed\n",
      "man:he :: woman:wounded\n",
      "man:he :: woman:reportedly\n",
      "man:he :: woman:abroad\n"
     ]
    }
   ],
   "source": [
    "# man:he :: woman:(she)\n",
    "embeddings.compute_analogy('man', 'he', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fly:plane :: sail:circle\n",
      "fly:plane :: sail:arc\n",
      "fly:plane :: sail:column\n",
      "fly:plane :: sail:rotation\n",
      "fly:plane :: sail:outer\n",
      "fly:plane :: sail:ring\n",
      "fly:plane :: sail:arch\n",
      "fly:plane :: sail:intersection\n",
      "fly:plane :: sail:probe\n",
      "fly:plane :: sail:imaginary\n",
      "fly:plane :: sail:curvature\n",
      "fly:plane :: sail:burst\n",
      "fly:plane :: sail:mound\n",
      "fly:plane :: sail:gate\n",
      "fly:plane :: sail:crest\n",
      "fly:plane :: sail:junction\n",
      "fly:plane :: sail:stream\n",
      "fly:plane :: sail:orbital\n",
      "fly:plane :: sail:loop\n"
     ]
    }
   ],
   "source": [
    "# fly:plane :: sail:(ship)\n",
    "embeddings.compute_analogy('fly', 'plane', 'sail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitten:cat :: puppy:dog\n",
      "kitten:cat :: puppy:bird\n",
      "kitten:cat :: puppy:wolf\n",
      "kitten:cat :: puppy:magic\n",
      "kitten:cat :: puppy:soup\n",
      "kitten:cat :: puppy:cow\n",
      "kitten:cat :: puppy:pepper\n",
      "kitten:cat :: puppy:tiger\n",
      "kitten:cat :: puppy:horn\n",
      "kitten:cat :: puppy:monster\n",
      "kitten:cat :: puppy:flower\n",
      "kitten:cat :: puppy:killer\n",
      "kitten:cat :: puppy:beast\n",
      "kitten:cat :: puppy:pet\n",
      "kitten:cat :: puppy:chorus\n",
      "kitten:cat :: puppy:honey\n",
      "kitten:cat :: puppy:jacket\n",
      "kitten:cat :: puppy:rabbit\n",
      "kitten:cat :: puppy:grass\n"
     ]
    }
   ],
   "source": [
    "# kitten:cat :: puppy:dog\n",
    "embeddings.compute_analogy('kitten', 'cat', 'puppy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue:color :: dog:breed\n",
      "blue:color :: dog:fitting\n",
      "blue:color :: dog:joke\n",
      "blue:color :: dog:straightforward\n",
      "blue:color :: dog:confusing\n",
      "blue:color :: dog:perfect\n",
      "blue:color :: dog:trick\n",
      "blue:color :: dog:realistic\n",
      "blue:color :: dog:humorous\n",
      "blue:color :: dog:stupid\n",
      "blue:color :: dog:luck\n",
      "blue:color :: dog:clip\n",
      "blue:color :: dog:mirrors\n",
      "blue:color :: dog:puzzle\n",
      "blue:color :: dog:voice\n",
      "blue:color :: dog:meter\n",
      "blue:color :: dog:fancy\n",
      "blue:color :: dog:bunch\n",
      "blue:color :: dog:typing\n"
     ]
    }
   ],
   "source": [
    "# blue:color :: dog:(breed)\n",
    "embeddings.compute_analogy('blue', 'color', 'dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leg:legs :: hand:fingers\n",
      "leg:legs :: hand:side\n",
      "leg:legs :: hand:hands\n",
      "leg:legs :: hand:shoulder\n",
      "leg:legs :: hand:neck\n",
      "leg:legs :: hand:face\n",
      "leg:legs :: hand:clothes\n",
      "leg:legs :: hand:surfaces\n",
      "leg:legs :: hand:edges\n",
      "leg:legs :: hand:bodies\n",
      "leg:legs :: hand:finger\n",
      "leg:legs :: hand:horns\n",
      "leg:legs :: hand:teeth\n",
      "leg:legs :: hand:balls\n",
      "leg:legs :: hand:shoulders\n",
      "leg:legs :: hand:holes\n",
      "leg:legs :: hand:wooden\n",
      "leg:legs :: hand:sticks\n"
     ]
    }
   ],
   "source": [
    "# leg:legs :: hand:(hands)\n",
    "embeddings.compute_analogy('leg', 'legs', 'hand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toe:finger :: foot:leg\n",
      "toe:finger :: foot:neck\n",
      "toe:finger :: foot:legs\n",
      "toe:finger :: foot:arm\n",
      "toe:finger :: foot:fingers\n",
      "toe:finger :: foot:bottom\n",
      "toe:finger :: foot:floor\n",
      "toe:finger :: foot:shoulder\n",
      "toe:finger :: foot:tail\n",
      "toe:finger :: foot:bow\n",
      "toe:finger :: foot:deck\n",
      "toe:finger :: foot:edges\n",
      "toe:finger :: foot:stones\n",
      "toe:finger :: foot:wheel\n",
      "toe:finger :: foot:rear\n",
      "toe:finger :: foot:walking\n",
      "toe:finger :: foot:face\n",
      "toe:finger :: foot:wings\n"
     ]
    }
   ],
   "source": [
    "# toe:finger :: foot:(hand)\n",
    "embeddings.compute_analogy('toe', 'finger', 'foot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talk:communicate :: read:translate\n",
      "talk:communicate :: read:manipulate\n",
      "talk:communicate :: read:commands\n",
      "talk:communicate :: read:modify\n",
      "talk:communicate :: read:integrate\n",
      "talk:communicate :: read:reproduce\n",
      "talk:communicate :: read:identify\n",
      "talk:communicate :: read:detect\n",
      "talk:communicate :: read:confuse\n",
      "talk:communicate :: read:render\n",
      "talk:communicate :: read:overcome\n",
      "talk:communicate :: read:execute\n",
      "talk:communicate :: read:automatically\n",
      "talk:communicate :: read:checking\n",
      "talk:communicate :: read:classify\n",
      "talk:communicate :: read:dynamically\n",
      "talk:communicate :: read:modifications\n",
      "talk:communicate :: read:instructions\n"
     ]
    }
   ],
   "source": [
    "# talk:communicate :: read:(interpret)\n",
    "embeddings.compute_analogy('talk', 'communicate', 'read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man:king :: woman:viii\n",
      "man:king :: woman:prince\n",
      "man:king :: woman:son\n",
      "man:king :: woman:vii\n",
      "man:king :: woman:heir\n",
      "man:king :: woman:crowned\n",
      "man:king :: woman:henry\n",
      "man:king :: woman:queen\n",
      "man:king :: woman:elizabeth\n",
      "man:king :: woman:iii\n",
      "man:king :: woman:augustus\n",
      "man:king :: woman:khan\n",
      "man:king :: woman:cousin\n",
      "man:king :: woman:daughter\n",
      "man:king :: woman:constantine\n",
      "man:king :: woman:princess\n",
      "man:king :: woman:grandson\n",
      "man:king :: woman:bishop\n",
      "man:king :: woman:clement\n"
     ]
    }
   ],
   "source": [
    "# man:king :: woman:(queen)\n",
    "embeddings.compute_analogy('man', 'king', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man:doctor :: woman:nurse\n",
      "man:doctor :: woman:secretly\n",
      "man:doctor :: woman:colleague\n",
      "man:doctor :: woman:lawyer\n",
      "man:doctor :: woman:farmer\n",
      "man:doctor :: woman:mrs\n",
      "man:doctor :: woman:gertrude\n",
      "man:doctor :: woman:child\n",
      "man:doctor :: woman:nude\n",
      "man:doctor :: woman:twins\n",
      "man:doctor :: woman:stern\n",
      "man:doctor :: woman:fellow\n",
      "man:doctor :: woman:blonde\n",
      "man:doctor :: woman:oswald\n",
      "man:doctor :: woman:teenager\n",
      "man:doctor :: woman:mortal\n",
      "man:doctor :: woman:lover\n",
      "man:doctor :: woman:reverend\n"
     ]
    }
   ],
   "source": [
    "# man:doctor :: woman:(job title)\n",
    "embeddings.compute_analogy('man', 'doctor', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast:fastest :: rich:richest\n",
      "fast:fastest :: rich:landscape\n",
      "fast:fastest :: rich:wealthiest\n",
      "fast:fastest :: rich:poorest\n",
      "fast:fastest :: rich:predominant\n",
      "fast:fastest :: rich:sparsely\n",
      "fast:fastest :: rich:northeastern\n",
      "fast:fastest :: rich:strongest\n",
      "fast:fastest :: rich:principal\n",
      "fast:fastest :: rich:americas\n",
      "fast:fastest :: rich:northernmost\n",
      "fast:fastest :: rich:lowlands\n",
      "fast:fastest :: rich:uzbekistan\n",
      "fast:fastest :: rich:coastal\n",
      "fast:fastest :: rich:overwhelmingly\n",
      "fast:fastest :: rich:abundant\n",
      "fast:fastest :: rich:fiji\n",
      "fast:fastest :: rich:highland\n",
      "fast:fastest :: rich:nordic\n"
     ]
    }
   ],
   "source": [
    "# fast:fastest :: small:smallest\n",
    "embeddings.compute_analogy('fast', 'fastest', 'rich')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_batcher = SkipGramBatcher.from_file(args.file_path)\n",
    "sg_batcher.prepare_data(cutoff=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0]), tensor([1, 2, 3, 2, 3]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = sg_batcher.generate_batches(window_size=5, \n",
    "                                batch_size=5)\n",
    "x_batch, labels_batch = next(g)\n",
    "\n",
    "x_batch, labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 50)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = len(sg_batcher.vocab)\n",
    "embedding_size = 50\n",
    "\n",
    "vocabulary_size, embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = NaiveWord2VecClassifier(vocabulary_size=vocabulary_size,\n",
    "                              embedding_size=embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1679, -0.7678, -0.4254,  0.3084,  0.4510, -0.2278, -0.0301,  0.1163,\n",
       "          0.0782,  0.1542, -0.5275, -0.5969, -0.2852,  0.9326, -0.0609,  1.0134,\n",
       "          0.7858, -0.3970, -0.0972,  0.2352, -0.1621, -0.3524],\n",
       "        [ 0.1679, -0.7678, -0.4254,  0.3084,  0.4510, -0.2278, -0.0301,  0.1163,\n",
       "          0.0782,  0.1542, -0.5275, -0.5969, -0.2852,  0.9326, -0.0609,  1.0134,\n",
       "          0.7858, -0.3970, -0.0972,  0.2352, -0.1621, -0.3524],\n",
       "        [ 0.1679, -0.7678, -0.4254,  0.3084,  0.4510, -0.2278, -0.0301,  0.1163,\n",
       "          0.0782,  0.1542, -0.5275, -0.5969, -0.2852,  0.9326, -0.0609,  1.0134,\n",
       "          0.7858, -0.3970, -0.0972,  0.2352, -0.1621, -0.3524],\n",
       "        [ 0.1679, -0.7678, -0.4254,  0.3084,  0.4510, -0.2278, -0.0301,  0.1163,\n",
       "          0.0782,  0.1542, -0.5275, -0.5969, -0.2852,  0.9326, -0.0609,  1.0134,\n",
       "          0.7858, -0.3970, -0.0972,  0.2352, -0.1621, -0.3524],\n",
       "        [ 0.1679, -0.7678, -0.4254,  0.3084,  0.4510, -0.2278, -0.0301,  0.1163,\n",
       "          0.0782,  0.1542, -0.5275, -0.5969, -0.2852,  0.9326, -0.0609,  1.0134,\n",
       "          0.7858, -0.3970, -0.0972,  0.2352, -0.1621, -0.3524]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf(x_batch)\n",
    "# y_pred = F.softmax(y_pred, dim=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 2, 3])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7678, -0.4254,  0.3084, -0.4254,  0.3084], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[range(5), labels_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.9903, 3.6480, 2.9141, 3.6480, 2.9141], grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.log(y_pred[range(5), labels_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = torch.zeros((5, vocabulary_size), dtype=torch.long)\n",
    "y_true[range(5), labels_batch] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 22]), torch.Size([5, 22]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.shape, y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3.9903, 3.6480, 2.9141, 3.6480, 2.9141], grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss(reduce=False)\n",
    "loss(y_pred, labels_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = torch.zeros((5, vocabulary_size))\n",
    "arr[range(5), labels_batch] = 1\n",
    "arr.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 2, 3])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in log\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "loss = -np.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
