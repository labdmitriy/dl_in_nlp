{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive word2vec\n",
    "\n",
    "This task can be formulated very simply. Follow this [paper](https://arxiv.org/pdf/1411.2738.pdf) and implement word2vec like a two-layer neural network with matrices $W$ and $W'$. One matrix projects words to low-dimensional 'hidden' space and the other - back to high-dimensional vocabulary space.\n",
    "\n",
    "![word2vec](https://i.stack.imgur.com/6eVXZ.jpg)\n",
    "\n",
    "You can use TensorFlow/PyTorch (numpy too, if you love to calculate gradients on your own and want some extra points, but don't forget to numerically check your gradients) and code from your previous task. Again: you don't have to implement negative sampling (you may reduce your vocabulary size for faster computation).\n",
    "\n",
    "**Results of this task**:\n",
    " * trained word vectors (mention somewhere, how long it took to train)\n",
    " * plotted loss (so we can see that it has converged)\n",
    " * function to map token to corresponding word vector\n",
    " * beautiful visualizations (PCE, T-SNE), you can use TensorBoard and play with your vectors in 3D (don't forget to add screenshots to the task)\n",
    " * qualitative evaluations of word vectors: nearest neighbors, word analogies\n",
    "\n",
    "**Extra:**\n",
    " * quantitative evaluation:\n",
    "   * for intrinsic evaluation you can find datasets [here](https://aclweb.org/aclwiki/Analogy_(State_of_the_art))\n",
    "   * for extrincis evaluation you can use [these](https://medium.com/@dataturks/rare-text-classification-open-datasets-9d340c8c508e)\n",
    "\n",
    "Also, you can find any other datasets for quantitative evaluation. If you chose to do this, please use the same datasets across tasks 3, 4, 5 and 6.\n",
    "\n",
    "Again. It is **highly recommended** to read this [paper](https://arxiv.org/pdf/1411.2738.pdf)\n",
    "\n",
    "Example of visualization in tensorboard:\n",
    "https://projector.tensorflow.org\n",
    "\n",
    "Example of 2D visualisation:\n",
    "\n",
    "![2dword2vec](https://www.tensorflow.org/images/tsne.png)\n",
    "\n",
    "If you struggle with something, ask your neighbor. If it is not obvious for you, probably someone else is looking for the answer too. And in contrast, if you see that you can help someone - do it! Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from itertools import islice, product, chain\n",
    "from argparse import Namespace\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('../data')\n",
    "MODELS_PATH = Path('../models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramBatcher():\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, file_path):\n",
    "        with open(file_path) as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        return cls(text)\n",
    "    \n",
    "    def _tokenize(self):\n",
    "        self.tokens = self.text.split()\n",
    "    \n",
    "    def _count_tokens(self):\n",
    "        self.token_counts = Counter(self.tokens)\n",
    "    \n",
    "    def _build_vocab(self, cutoff):\n",
    "        filtered_token_counts = dict(filter(lambda x: x[1] >= cutoff, self.token_counts.items()))\n",
    "        self.token_to_idx = {token:idx for (idx, (token, _)) \n",
    "                             in enumerate(filtered_token_counts.items())}\n",
    "        self.idx_to_token = {idx:token for (token, idx) in self.token_to_idx.items()}\n",
    "        self.vocab = set(self.token_to_idx)\n",
    "\n",
    "    def _filter_tokens(self):\n",
    "        self.tokens = [token for token in self.tokens if token in self.vocab]\n",
    "    \n",
    "    def _vectorize_tokens(self):\n",
    "        self.vectorized_tokens = [self.token_to_idx[token] for token in self.tokens]\n",
    "    \n",
    "    def _create_sliding_window(self, window_size):\n",
    "        tokens_size = len(self.tokens)\n",
    "\n",
    "        for i in range(0, tokens_size):\n",
    "#             center_word = islice(self.vectorized_tokens, i, i + 1)\n",
    "#             left_context = islice(self.vectorized_tokens, i + 1, \n",
    "#                                   min(tokens_size, i + window_size + 1))\n",
    "#             right_context = islice(self.vectorized_tokens, \n",
    "#                                    max(0, i - window_size), i)\n",
    "#             yield from product(center_word, chain(left_context, right_context))\n",
    "            center_word = self.vectorized_tokens[i:i+1]\n",
    "#             context = []\n",
    "#             context.extend(self.vectorized_tokens[max(0, i - window_size): i])\n",
    "#             context.extend(self.vectorized_tokens[i + 1: min(tokens_size, i + window_size + 1)])\n",
    "            left_context = self.vectorized_tokens[max(0, i - window_size): i]\n",
    "            right_context = self.vectorized_tokens[i + 1: min(tokens_size, i + window_size + 1)]\n",
    "            yield from product(center_word, chain(left_context, right_context))\n",
    "        \n",
    "    def devectorize_tokens(self, indices):\n",
    "        return [self.idx_to_token[idx] for idx in indices]\n",
    "        \n",
    "    def prepare_data(self, cutoff=1):\n",
    "        self._tokenize()\n",
    "        self._count_tokens()\n",
    "        self._build_vocab(cutoff)\n",
    "        self._filter_tokens()\n",
    "        self._vectorize_tokens()\n",
    "        \n",
    "    def generate_batches(self, window_size=1, batch_size=1, drop_last=True):\n",
    "        window = self._create_sliding_window(window_size)\n",
    "        batch = list(zip(*islice(window, batch_size)))\n",
    "        x_batch, labels_batch = torch.tensor(batch[0]), torch.tensor(batch[1])\n",
    "\n",
    "        if drop_last:\n",
    "            while batch and len(batch[0]) == batch_size:\n",
    "                yield x_batch, labels_batch\n",
    "                batch = list(zip(*islice(window, batch_size)))\n",
    "                x_batch, labels_batch = torch.tensor(batch[0]), torch.tensor(batch[1])\n",
    "        else:\n",
    "            while batch:\n",
    "                yield x_batch, labels_batch\n",
    "                batch = list(zip(*islice(window, batch_size)))\n",
    "                x_batch, labels_batch = torch.tensor(batch[0]), torch.tensor(batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveWord2VecClassifier(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_size):\n",
    "        super().__init__()\n",
    "        \n",
    "#         self.embedding = nn.utils.weight_norm(nn.Embedding(num_embeddings=vocabulary_size,\n",
    "#                                       embedding_dim=embedding_size), dim=1)\n",
    "# #                                       max_norm=1.0)\n",
    "# #                                       scale_grad_by_freq=True)\n",
    "                                      \n",
    "#         self.fc1 = nn.utils.weight_norm(nn.Linear(in_features=embedding_size,\n",
    "#                              out_features=vocabulary_size,\n",
    "#                              bias=False), dim=1)\n",
    "\n",
    "        self.embedding = nn.utils.weight_norm(nn.Embedding(num_embeddings=vocabulary_size,\n",
    "                                              embedding_dim=embedding_size), dim=0)\n",
    "\n",
    "        self.fc1 = nn.utils.weight_norm(nn.Linear(in_features=embedding_size,\n",
    "                             out_features=vocabulary_size,\n",
    "                             bias=False), dim=0)\n",
    "        \n",
    "    def forward(self, x_in):\n",
    "#         self.embedding.weight.data = F.normalize(self.embedding.weight.data, p=2, dim=1)\n",
    "#         self.fc1.weight.data = F.normalize(self.fc1.weight.data, p=2, dim=1)\n",
    "        x_embedded = self.embedding(x_in)\n",
    "        y_out = self.fc1(x_embedded)\n",
    "        \n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    file_path = DATA_PATH/'text8',\n",
    "    model_state_path = MODELS_PATH/'naive_word2vec_embeddings.pth',\n",
    "    weights_path = MODELS_PATH/'weights.npz',\n",
    "    \n",
    "    embedding_size = 100,\n",
    "    \n",
    "    seed = 42,\n",
    "    cutoff = 10, # 10\n",
    "    window_size = 1, # 1\n",
    "    batch_size = 1024, #  1024\n",
    "    learning_rate = 0.03, # 0.03\n",
    "    iterations = 1000,\n",
    "    save_iterations = 100,\n",
    "    early_stopping_criteria = 1e8,\n",
    "    factor=0.7, # 0.7\n",
    "    patience=1000, # 1000\n",
    "    \n",
    "    cuda=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': [], # args.learning_rate\n",
    "            'batch_idx': 0,\n",
    "            'loss': [],\n",
    "            'model_file_name': args.model_state_path}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    if train_state['batch_idx'] == 0:\n",
    "        train_state['stop_early'] = False\n",
    "        torch.save(model.state_dict(), train_state['model_file_name'])\n",
    "    else:\n",
    "        loss = train_state['loss'][-1]\n",
    "\n",
    "        if loss < train_state['early_stopping_best_val']:\n",
    "            train_state['early_stopping_best_val'] = loss\n",
    "            train_state['early_stopping_step'] = 0\n",
    "            \n",
    "            if train_state['batch_idx'] % args.save_iterations == 0:\n",
    "                torch.save(model.state_dict(), train_state['model_file_name'])\n",
    "        else:\n",
    "            train_state['early_stopping_step'] += 1 \n",
    "    \n",
    "        train_state['stop_early'] = train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    args.cuda=False\n",
    "    \n",
    "print(f'Using CUDA: {args.cuda}')\n",
    "args.device = torch.device('cuda' if args.cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47134"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_batcher = SkipGramBatcher.from_file(args.file_path)\n",
    "sg_batcher.prepare_data(cutoff=args.cutoff)\n",
    "\n",
    "vocabulary_size = len(sg_batcher.vocab)\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b81dc2bf1a4791b1f540f5deba6255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=32345, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "weight_norm: expected v_in and g_in to be on the same device, but v_in is on cpu and g_in is on cuda:0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-267-74872a87a25a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-261-44d26482a88c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_in)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#         self.embedding.weight.data = F.normalize(self.embedding.weight.data, p=2, dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#         self.fc1.weight.data = F.normalize(self.fc1.weight.data, p=2, dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx_embedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0my_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_embedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/torch/nn/utils/weight_norm.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, module, inputs)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/torch/nn/utils/weight_norm.py\u001b[0m in \u001b[0;36mcompute_weight\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_g'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_weight_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: weight_norm: expected v_in and g_in to be on the same device, but v_in is on cpu and g_in is on cuda:0"
     ]
    }
   ],
   "source": [
    "set_seeds(args.seed)\n",
    "\n",
    "classifier = NaiveWord2VecClassifier(vocabulary_size=vocabulary_size,\n",
    "                                     embedding_size=args.embedding_size)\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = optim.Adam(params=classifier.parameters(),\n",
    "                      lr=args.learning_rate)\n",
    "\n",
    "epoch_size = 2 * (args.window_size * len(sg_batcher.tokens) \n",
    "                  - np.math.factorial(args.window_size)) // args.batch_size\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min',\n",
    "                                                 factor=args.factor,\n",
    "                                                 patience=args.patience)\n",
    "\n",
    "train_bar = tqdm_notebook(desc='Training',\n",
    "                          position=1,\n",
    "#                           total=args.iterations,\n",
    "                          total=epoch_size)\n",
    "\n",
    "batch_generator = sg_batcher.generate_batches(window_size=args.window_size, \n",
    "                                              batch_size=args.batch_size)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "writer = SummaryWriter(log_dir='logs/task_3', comment='embedding_training')\n",
    "\n",
    "running_loss = 0.\n",
    "classifier.train()\n",
    "\n",
    "try:\n",
    "    for batch_idx, (x_batch, labels_batch) in enumerate(batch_generator, 1):\n",
    "        x_batch = x_batch.to(args.device)\n",
    "        labels_batch = labels_batch.to(args.device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = classifier(x_in=x_batch)\n",
    "\n",
    "        loss = loss_func(y_pred, labels_batch)\n",
    "        loss_value = loss.item()\n",
    "        running_loss += (loss_value - running_loss) / (batch_idx)\n",
    "        loss.backward()\n",
    "        \n",
    "        learning_rate = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        train_state['batch_idx'] = batch_idx\n",
    "        train_state['loss'].append(running_loss)\n",
    "        train_state['learning_rate'].append(learning_rate)\n",
    "#         writer.add_scalar('loss', scalar_value=loss, global_step=batch_idx)\n",
    "\n",
    "\n",
    "        train_state = update_train_state(args=args,\n",
    "                                         model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        train_params = dict(loss=running_loss,\n",
    "                            lr=learning_rate,\n",
    "                            early_step=train_state['early_stopping_step'],\n",
    "                            early_best=train_state['early_stopping_best_val'])\n",
    "        train_bar.set_postfix(train_params)\n",
    "        train_bar.update()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step(train_state['loss'][-1])\n",
    "\n",
    "        if train_state['stop_early'] or (batch_idx == epoch_size):\n",
    "            torch.save(model.state_dict(), str(train_state['model_file_name']) + '_last')\n",
    "            break\n",
    "except KeyboardInterrupt:\n",
    "    print('Exit training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor([[1, 2], [3, 4]])#.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJzt3Xl4XHd97/H3dxZptO+WvEhe5SULTozirCQhISSkQAjQkkBLKKFpCrcs7X1uoX16aeld4Ckt3F64QFhKoPeGJQRIWBJMWBIgDpaTOHG8x5vkRZK1Wrs0871/zLGjKHJsz0iekebzep55Zuac38z56nj8mTO/8zvnmLsjIiK5IZTpAkRE5NxR6IuI5BCFvohIDlHoi4jkEIW+iEgOUeiLiOQQhb6ISA5R6IuI5BCFvohIDolkuoDJqqurfcmSJZkuQ0RkVtm8efMxd685XbusC/0lS5bQ3Nyc6TJERGYVMztwJu1O271jZl8zs3Yz2zph2h+a2fNmljCzpld47U1mttPM9pjZR8+sdBERmSln0qf/deCmSdO2Am8FHjvVi8wsDHweeANwHnC7mZ2XWpkiIjIdThv67v4Y0DVp2nZ333mal64H9rj7XncfBb4F3JJypSIikraZHL2zEGiZ8Lw1mCYiIhkyk6FvU0yb8uT9ZnaXmTWbWXNHR8cMliQikttmMvRbgfoJzxcBh6dq6O73uHuTuzfV1Jx2xJGIiKRoJkN/E9BoZkvNLA+4DXhwBpcnIiKncSZDNu8DngBWmVmrmd1pZreaWStwOfBjM3skaLvAzH4C4O7jwH8CHgG2A99x9+dPt7y2vmHajw+n/heJiMgpWbZdIzd/fqM/8eTvWddQkelSRERmDTPb7O6nPG7qhKw8907f0FimSxARmZOyMvR7FfoiIjMiK0NfW/oiIjMjK0NfW/oiIjMj60LfTKEvIjJTsi70w2b0DY1nugwRkTkp+0I/ZNrSFxGZIVkX+hGFvojIjMm60NeWvojIzMm60A8p9EVEZkzWhX7YjL5hhb6IyEzIvtAPGceHx4knsuucQCIic0FWhj7AcW3ti4hMu6wNffXri4hMP4W+iEgOyb7Qt2To66hcEZHpl32hH0qWpC19EZHpl4Whr+4dEZGZotAXEckhWRf6IYO8cEihLyIyA7Iu9AFKCyI6KldEZAacNvTN7Gtm1m5mWydMqzSzDWa2O7ivOMVr42b2THB78EyLKi2IaktfRGQGnMmW/teBmyZN+yjwqLs3Ao8Gz6cy5O4XBbc3n2lRZQVRXSdXRGQGnDb03f0xoGvS5FuAe4PH9wJvmc6iyrSlLyIyI1Lt06919yMAwf28U7SLmVmzmW00s1N+MZjZXUG75o6ODoW+iMgMmekduQ3u3gS8E/ismS2fqpG73+PuTe7eVFNTQ01xPu19I7jrTJsiItMp1dBvM7P5AMF9+1SN3P1wcL8X+BVw8Zm8eV1ZjKGxuE7FICIyzVIN/QeBO4LHdwA/nNzAzCrMLD94XA1cCWw7kzefX1YAwOHeoRTLExGRqZzJkM37gCeAVWbWamZ3Ap8EbjCz3cANwXPMrMnMvhK8dA3QbGZbgF8Cn3T3Mwr9urIYAEd7h8/yzxERkVcSOV0Dd7/9FLOun6JtM/C+4PHvgAtTKWpBeTL0taUvIjK9svKI3JrifEIGR3q0pS8iMp2yMvQj4RDzywpo6R7MdCkiInNKVoY+wJLqQg50KvRFRKZT1ob+4qoiDnQOZLoMEZE5JWtDf0lVId2DYzoyV0RkGmVt6C+uKgLQ1r6IyDTK2tCvrygEoLVbwzZFRKZL1ob+osrkUbmtGsEjIjJtsjb0S2NRygqitHRpS19EZLpkbegDLKoo0Ja+iMg0yurQr68opEV9+iIi0yarQ39BeQGHe4Z0Xn0RkWmS5aEfY3BU59UXEZkuWR36J06xrLNtiohMj6wO/WXVxQDsajue4UpEROaGrA79xtpi8iMhnm3tzXQpIiJzQlaHfjQcIi8c4qu/2cdjuzoyXY6IyKyX1aEPcPe1ywH44LeeZjyeyHA1IiKzW9aH/gdeu4LPvfNiegbHeKalJ9PliIjMalkf+gCvWVFDOGT8Wl08IiJpOW3om9nXzKzdzLZOmFZpZhvMbHdwX3GK194RtNltZnekWmRZYZSVtSXaoSsikqYz2dL/OnDTpGkfBR5190bg0eD5S5hZJfBx4FJgPfDxU305nIk1dSXsPKqhmyIi6Tht6Lv7Y0DXpMm3APcGj+8F3jLFS28ENrh7l7t3Axt4+ZfHGVtVV8LRvmF6BkdTfQsRkZyXap9+rbsfAQju503RZiHQMuF5azAtJavqSgC0tS8ikoaZ3JFrU0yb8sxpZnaXmTWbWXNHx9Q7a5cEl0/UWTdFRFKXaui3mdl8gOC+fYo2rUD9hOeLgMNTvZm73+PuTe7eVFNTM+UCF5QXYAYtXTq/vohIqlIN/QeBE6Nx7gB+OEWbR4DXm1lFsAP39cG0lORFQtSVxmjRRVVERFJ2JkM27wOeAFaZWauZ3Ql8ErjBzHYDNwTPMbMmM/sKgLt3Af8EbApunwimpSx5JS1174iIpCpyugbufvspZl0/Rdtm4H0Tnn8N+FrK1U1SX1HIxr2d0/V2IiI5Z1YckXtCfWUhR/qGGR6LZ7oUEZFZaVaF/tLqIty1M1dEJFWzKvTXzC8FYOO+tHYNiIjkrFkV+itri2mcV8xDz0w58lNERE5jVoW+mXHLRQv4/f4uDvVoFI+IyNmaVaEP8LrzagHYpC4eEZGzNutCf3lNMXnhENuP9GW6FBGRWWfWhX40HKKxtpjtOvGaiMhZm3WhD7C6rlRb+iIiKZiVob9mfgkdx0c41j+S6VJERGaVWRr6yfH6O46oi0dE5GzMytBfHVxQZcdRdfGIiJyNWRn6VcX5zCvJZ5v69UVEzsqsDH1IdvGoe0dE5OzM2tBfPb+EPe39jMUTmS5FRGTWmLWhv6aulNF4ghc6+jNdiojIrDFrQ7+xthiAF9oHMlyJiMjsMWtDf1FFIQCHenRufRGRMzVrQ7+sIEppLMKTe7v4+bY23D3TJYmIZL1ZG/oA6xZX8OiOdt73jWa+//ShTJcjIpL1ZnXof/h1K3n35YsB2LCtLcPViIhkv0g6LzazDwF/BhjwZXf/7KT51wI/BPYFkx5w90+ks8yJLqov56L6co71j7D1kA7UEhE5nZS39M3sApKBvx5YC7zRzBqnaPq4u18U3KYt8Ce6qL6cg12DtPUNz8Tbi4jMGel076wBNrr7oLuPA78Gbp2ess7OVStqAHh897FMLF5EZNZIJ/S3AlebWZWZFQI3A/VTtLvczLaY2U/N7Pw0lndKq+tKqC7O5/HdHTPx9iIic0bKffruvt3MPgVsAPqBLcD4pGZPAYvdvd/MbgZ+ALysC8jM7gLuAmhoaDjrWkIh4zWN1Ty2q4NEwgmF7KzfQ0QkF6Q1esfdv+ru69z9aqAL2D1pfp+79wePfwJEzax6ive5x92b3L2ppqYmpVpe01hN58CozrwpIvIK0gp9M5sX3DcAbwXumzS/zswseLw+WF5nOss8lStXJL9LNu6dkbcXEZkT0h2n/z0z2wY8BHzA3bvN7G4zuzuY/3Zgq5ltAf4NuM1n6NDZ2tIYS6oKeXJf10y8vYjInJDWOH13f80U07444fHngM+ls4yzsX5pJT8LTskQ/MAQEZEJZvURuZOtrS+nZ3CMQz1DmS5FRCQrzanQP3HB9G2HtTNXRGQqcyr0V9eVYIZG8IiInMKcCv3CvAhLq4t0Hh4RkVOYU6EPyfPwPH2wW+fXFxGZwpwL/XUNFXQOjHKwS1fUEhGZbE6GPsBTB7szXImISPaZc6G/qq6EorwwTx3oyXQpIiJZZ86FfjhkXNRQri19EZEpzLnQB2haXMn2I30c6dVBWiIiE83J0H/7qxcB8M0nDmS4EhGR7DInQ7++spBrVtbw7U0t9A6NZbocEZGsMSdDH+C9Vy2lc2CUB55qzXQpIiJZY86G/msaa1hYXsCm/TrVsojICXM29AEuWVLB7/fp6FwRkRPmdugvreRY/wj7O3V0rogIzPHQX7+kEkBdPCIigTkd+striikvjLJJl1AUEQHmeOiHQkbT4kqaD+joXBERmOOhD8mdufuODdB+fDjTpYiIZNzcD/2lyX79X2xvz3AlIiKZl1bom9mHzGyrmT1vZh+eYr6Z2b+Z2R4ze9bM1qWzvFRctKictfXlfPpnOxkcHT/XixcRySoph76ZXQD8GbAeWAu80cwaJzV7A9AY3O4CvpDq8lIVChl//wdrONY/ync2tZzrxYuIZJV0tvTXABvdfdDdx4FfA7dOanML8A1P2giUm9n8NJaZkqYllTQtruDLj+9jLJ4414sXEcka6YT+VuBqM6sys0LgZqB+UpuFwMTN69Zg2kuY2V1m1mxmzR0dHWmUdGp3X7OcQz1D/PjZIzPy/iIis0HKoe/u24FPARuAh4EtwOROc5vqpVO81z3u3uTuTTU1NamW9IquWz2PxnnFfPHXL+i0DCKSs9LakevuX3X3de5+NdAF7J7UpJWXbv0vAg6ns8xUhULGn1+znB1Hj/OrXTPza0JEJNulO3pnXnDfALwVuG9SkweBdwejeC4Det09Y/0rb167gPllMb74qxcyVYKISEalO07/e2a2DXgI+IC7d5vZ3WZ2dzD/J8BeYA/wZeD9aS4vLXmREHdetZQn93XxtK6hKyI5yLKtf7upqcmbm5tn7P37R8ZZ94kNXL9mHv/nXeswm2q3g4jI7GJmm9296XTt5vwRuZMV50d446vm89OtR/lOs8bti0huiWS6gEz45z9cS0v3IH/7/a3EE/DOSxsyXZKIyDmRc1v6AOGQ8fl3rWNFTTF/94Pn2HdsINMliYicEzkZ+gDzSmJ8833riYSMbzyxP9PliIicEzkb+pAM/psvnM93m1vpHhjNdDkiIjMup0Mf4P3XrmB4LM7/+Mn2TJciIjLjcj70V9WV8N6rlnL/U608tCUjBwuLiJwzOR/6AH91w0pe3VDBxx54jl1txzNdjojIjFHoA7FomM+84yLcnc//ck+myxERmTEK/UB9ZSFve/Uifrr1KPs1hFNE5iiF/gR3Xb2MvHCIf9mwK9OliIjMCIX+BIsqCnnXpQ38+NnDtHQNZrocEZFpp9Cf5E+vXEo4ZPy3H28jkciuk9GJiKRLoT9JXVmM9165lEeeb+ObGw9kuhwRkWml0J/CR9+wmmtW1vCph3dop66IzCkK/SmYGf/zrRcSCRnv/PJGunSKBhGZIxT6p7CgvIBv3nkpbcdH+OdHdma6HBGRaaHQfwVr68u54/IlfGvTQZ5r7c10OSIiaVPon8aHb2ikqiiP//rgVo3mEZFZT6F/GqWxKH9z02qePtjDA08fynQ5IiJpSSv0zewjZva8mW01s/vMLDZp/nvMrMPMnglu70uv3Mx427pFXNxQzid/uoP+kfFMlyMikrKUQ9/MFgIfBJrc/QIgDNw2RdNvu/tFwe0rqS4vk0Ih429vXsOx/hEefEanXxaR2Svd7p0IUGBmEaAQmLOJ2LS4gtV1JXz9d/sYiycyXY6ISEpSDn13PwR8GjgIHAF63f1nUzR9m5k9a2b3m1l9qsvLNDPjIzesZFdbP/c8tjfT5YiIpCSd7p0K4BZgKbAAKDKzP57U7CFgibu/Cvg5cO8p3usuM2s2s+aOjo5US5pxN55fx+vWzOOex/YyOq6tfRGZfdLp3nkdsM/dO9x9DHgAuGJiA3fvdPeR4OmXgVdP9Ubufo+7N7l7U01NTRolzbx3XbqY3qExHtuVvV9OIiKnkk7oHwQuM7NCMzPgeuAlVxc3s/kTnr558vzZ6KrGaioKo/xQ19MVkVkonT79J4H7gaeA54L3usfMPmFmbw6afTAY0rmF5Eif96RZb8ZFwyFuuWghD289wuGeoUyXIyJyVsw9u44ybWpq8ubm5kyX8Ypauwe59p9/xbsubeAfb7kg0+WIiGBmm9296XTtdERuChZVFPK2dYu4b1MLbX3DmS5HROSMKfRT9IHXriCRcP7zd7ewcW8ncZ2XR0RmAYV+ihqqCnn/a1fw+O5j3HbPRv76O8+QbV1lIiKTRTJdwGz2Vzes5C0XLeDbm1r40mN76RwY5Z3rG7h+TS15EX2fikj2UeinaVlNMR99w2oK8sJ89ue7eXz3MS5uKOfPr17OdavnKfxFJKto9M40eqalh+80t7BhWxsdx0cozAtz/Zpa/vutF1Aai2a6PBGZw8509I5CfwaMxxM8vucYP9/Wxrc3tbCspoivvecSFlUUZro0EZmjFPpZ4rd7jnH3f2wmPxLmL69bwa93dVCUH9HWv4hMK43TzxJXrqjmgb+4glg0xMcffJ5nW3v48bOHue1LG2ntHsx0eSKSY7Slf470Do6xpbWHS5dV8qudHXz4W88wNBbnbesW8cm3XUg0rO9fEUmdtvSzTFlhlKtX1pAfCXPj+XXc/xeXc+P5tXzvqVbuvLeZY/0jp38TEZE0KfQz5PwFZXzpT5r4+JvO47FdHdzyud/SrlM6iMgMU+hn2J9euZRvvHc93YOj3PL53/K13+xjd9vxTJclInOU+vSzxLOtPfz9D59nS0sPAI3zivnAa1fwlosXZrgyEZkNNGRzltrTfpyHtx7l4eePsvVQH2+4oI4/uqSeQ91DHOwapLV7kLKCPF69uIJbL15IOGSZLllEsoBCf5YbGo3zrxt2ct/vW+gfGT85fWF5AQOj4/QMjrG8poi/vK5RvwZERKE/VwyMjPPw1qMsqS7i/AWlxKJh3J2Hnj3CJ3+ynY7+EX72kWtYWl2U6VJFJIMU+jmgvW+Y6/7l14wnEtSU5HPe/FJetaica1bWcMHCskyXJyLnkEI/R/xiRxv/78mDhEPGrrZ+9h0bAGDtojLeddli3r5uESH1+4vMeQr9HNXZP8KDWw7z7U0t7Dh6nEuXVvIPbz6fVbUlCn+ROUyhn+Pcne82t/JPP97G8eFxSmMRrlk1j9suqeeK5VWY6QtAZC4509BP6yIqZvYR4H2AA88Bf+ruwxPm5wPfAF4NdALvcPf96SxTzoyZ8UeX1HPNqhoe2nKYPe39PLTlMA9tOcza+nL+6oaVXN1YrfAXyTEpb+mb2ULgN8B57j5kZt8BfuLuX5/Q5v3Aq9z9bjO7DbjV3d/xSu+rLf2Zc7R3mB89e5h//+1+DvUMsbK2mNV1pVy5ooqrV9YQNuOpgz0k3OkfHmdeaT7VxfmsqivRCeFEstw52dIPXl9gZmNAIXB40vxbgH8IHt8PfM7MzLOtTylH1JXFeN9rlvEnly/mgacOce/v9vPbPcd4cMvkf7aXKoiGWVZThBm4J48hiEXDvGntAu68aqkuCSkyi6Qc+u5+yMw+DRwEhoCfufvPJjVbCLQE7cfNrBeoAo6lulxJX34kzO3rG7h9fQPuzq62fh7b1cH2o33cevFCqovzKc6PcLRvmKO9w2w+0M2BzgHMjJAlX9/RP8KnHt7BD54+xHuuXMIVy6twh0UVBUT0q0Aka6Uc+mZWQXJLfinQA3zXzP7Y3f9jYrMpXvqyrXwzuwu4C6ChoSHVkiQFZsaquhJW1ZW8bF59ZfLyjm9au2DK1/5iRxufeGgbH3vguZPT8iIh1i4q486rlvH682o1Ykgky6TTp/+HwE3ufmfw/N3AZe7+/gltHgH+wd2fMLMIcBSoeaXuHfXpzy7uzs624zy5t4v8SIgXOvr52bY2DnQOsqq2hA9ct4I/uHC+zhEkMsPORZ/+QeAyMysk2b1zPTA5rR8E7gCeAN4O/EL9+XOLmbG6rpTVdaUnp/3NTav50bNH+Nwv9/DB+57mMxt20bS4gkjYWDGvhHgiwdBogv2dA2xp6eH8hWU0zivmTWsX6HQSIjMsrXH6ZvaPwDuAceBpksM3/w5odvcHzSwGfBO4GOgCbnP3va/0ntrSnzsSCeeR54/yhV+/wJHeYcbiCXoGx07Oz4+EWNdQwYHOAY70DeOePKHcpUsrOW9BKZcsqWRtfXkG/wKR2UMHZ0nWcXe6B8fIi4SIRUIv2eF7qGeIDc8f5ff7u/j9vi6O9Y8SMnj/tSuoLc1naCxOfUUha+vLmV8W0/EFIpMo9GXWcneO9Y/y9z/YysPPH33Z/IXlBVyxvIrrVs/jxvPrtLNYBIW+zBHtfcO0dA/RWFvM3o4Bnj7YzZN7u3hibye9Q2NcvqyKu65exrWrarT1LzlNoS9zWjzhfPU3e/ny4/voOD7ChQvLuGJ5FQvKC7iqsZpl1UUv+RJIJJwdR4+THw1RVZRHeWFeBqsXmX4KfckJo+MJ/mPjAe7f3Mru9uOMxZOf54JomPnlMS5dWsnymmJ+vr2NjXu7Tr5uQVmMxtoSrlhexaq6ElbWlrCgvCBTf4ZI2hT6knMSCWfvsQEe29VBS/cgL3QMsHl/FwOjcUIGt69vYP3SSg73DLO77TjPHepld3v/yde/alEZN55fx43n17Ji3ssPVhPJZgp9EWAsnmBwNE44ZBTnv/ywlOQ+g0E27e/m4a1HeaalB4AlVYVcs7KG6uJ86isLaawtZnlNMbFo+Fz/CSJnRKEvkoIjvUNs2NbGL3e087sXOhkZT5ycFzJYUl3E+iWVXLOyhsuXV1FWENUOZMkKCn2RNCUSzlgiwcHOQXa2HWfX0eNsO9LHk3u7OD4yDkB1cT6XLas8eexA7+AYRfkRygqiVBbnMb80Rkkswmg8QciMixvKKcxL9+S2Ii93rk6tLDJnhUJGfihMY20JjbUl8Krk9LF4gmdaeth8oJvtR/rYtK+L7sEx4u6UFUQZGo3TH3wpTBaLhrhyeTV1ZTGKYxFKY1Gqi/O4cGE5a+aX6FeDzDiFvshZioZDXLKkkkuWVJ6yzXg8QefAKEd6hxkYGScvEmJgZJxf7GjnN3uO8UxLD/0j4y/pPiqJRagtjXH+glKuX1PL68+r1T4EmXYKfZEZEAmHqC2NUVsae8n0a1fNe8nz4bE47X0jbNzbyfOHeznSO8zju4/xw2cOkx8JsaymmNV1JVy6tJLLllWxuKoQM8PdGYs70bDp14GcFYW+SAbFomEaqgppqCoE6oHkgWdP7u3kFzva2dPRz+O7O/j+04cAqCrKIx5cznI84RREw1SX5FFTnLy0ZVVxPmUFUcoLoyypKmJpdRF1pTHCYSNsRjiUvIUseYbU8XiCI73DDI3FSbiTSJC8dydkRiwaIi8cJi8SIj8SIi+4RUL6spmtFPoiWSYcMq5YUc0VK6qB5LmIXugYYOPeTra09BCLhimORSiIhukbGqOjf4Rj/SPs7xzgqYPd9A2NMxpPnGYpyeW4O4kUxnKEDArzIiytLqIgL5y8lkJ7P6GQUVmUR11pLHhfp6Ykn4rCPGLRMAXRMLG8MIXRMIV5YQrywpQWRFlYXkBNcb7Oo3QOaPSOyBzUNzzGgWODvNDRT+fAKPFEgniwFT8ed+LuxBPJEUULywsojkUImQW35BdCPOEMjycYDW4j4/GTj0fjCXqHxjjQOcjwWJyhsTiLKgqIRcN0HB+hvW8k+aVC8liI3qExxk/z7ZIfCdFYW8yaulIaa4tZUlXEkuoiGioLtW/jDGj0jkgOK41FuXBRGRcuKst0KSeNxxMMjycYGo0zNBpnYHScwdE4fUNjtHYPciAYGvvLne18d3PrydeZwfzSGIurkl8ANSX5lBZE6B+JEwl+WVQW5VFRmEd1cR4lsSg9Q6N09Y8yOBqnd2iM7sHR5K+KWJTSgijlBVFqS2NUF+fl3DWdFfoick5EwiGKw6Epj4yerHdwjP2dA8nbsUEOBI8f3dFO18AICU9+GaTbURGy5LEWdWUx5pXEqCvLp7YkRm1ZjIXlBVQV51GUF6GsMEppLJrewrKEQl9Esk5ZYZS1heVTXjktkXAGx+LkBVvoPYOjdA2O0jUwSmf/KL1DY5QXRqksyqM4P3ksREVRHsNjyV8VfcNjdA2M0dY3THvfMEf7hmnrG6G1e5DNB5LHXEylpiSfxnnFrKwtYUVw3zivmIqiPMbiCfqGxnAgGgoRCRuRsBENhbJuP4VCX0RmldCk8yjNK40xb9LQ2KmUBV06pzM8Fqfj+Agt3YP0DI4xMDJO18Aou9v72d3ez3ebWxgYjZ9snx8JveR4i8nCISMSMoryIxTnR0i4Ew2HiIYtuE+OiAqbYQahCfehk8+Tj6OREBWFUWpLYjRUFbK8JnlOqIK8M9/nodAXEZkgFg1TX1lIfWXhlPPdncO9yTO17m7rp/348Ml9BWYwFnfG4wnGE87oeILxRIKxuDM4Ok7/8DghM8YTzlg8wVg8wch48n48kcAdnBPDZglGV704lHZ0PEH34OjLfo0sKDv9l9kJCn0RkbNgwYinheUFLzvY7lwZHotzsGuQPe397GnvZ9+xAZ44w9cq9EVEZplYNMzK2uTFf0747G1n9tqUxyqZ2Soze2bCrc/MPjypzbVm1juhzX9NdXkiIpK+lLf03X0ncBGAmYWBQ8D3p2j6uLu/MdXliIjI9JmuoxKuB15w9wPT9H4iIjIDpiv0bwPuO8W8y81si5n91MzOn6qBmd1lZs1m1tzR0TFNJYmIyGRph76Z5QFvBr47xeyngMXuvhb438APpnoPd7/H3ZvcvammpibdkkRE5BSmY0v/DcBT7t42eYa797l7f/D4J0DUzKqnYZkiIpKC6Qj92zlF146Z1Vlw0m0zWx8sr3MalikiIilIa5y+mRUCNwB/PmHa3QDu/kXg7cBfmNk4MATc5tl2LmcRkRySdefTN7PjwM5M15FFqoFjmS4iS2hdvEjr4kVaF0mL3f20O0Wz8YjcnWdyIYBcYWbNWh9JWhcv0rp4kdbF2cmtqweIiOQ4hb6ISA7JxtC/J9MFZBmtjxdpXbxI6+JFWhdnIet25IqIyMzJxi19ERGZIVkV+mZ2k5ntNLM9ZvbRTNcz08ys3sx+aWbbzex5M/tQML3SzDaY2e7gviKYbmb2b8H6edbM1mX2L5h+ZhY2s6fN7EfB86Vm9mSwLr4dnPYDM8sPnu8J5i/JZN3TzczKzex+M9vRALkvAAADNElEQVQRfD4uz9XPhZl9JPj/sdXM7jOzWK5+LqZD1oR+cHrmz5M8rcN5wO1mdl5mq5px48Bfu/sa4DLgA8Hf/FHgUXdvBB4NnkNy3TQGt7uAL5z7kmfch4DtE55/CvhMsC66gTuD6XcC3e6+AvhM0G4u+V/Aw+6+GlhLcp3k3OfCzBYCHwSa3P0CIEzyBI+5+rlIn7tnxQ24HHhkwvOPAR/LdF3neB38kOQRzjuB+cG0+SSPXQD4EnD7hPYn282FG7CIZJhdB/wIMJIH3UQmf0aAR4DLg8eRoJ1l+m+YpvVQCuyb/Pfk4ucCWAi0AJXBv/OPgBtz8XMxXbes2dLnxX/cE1qDaTkh+Bl6MfAkUOvuRwCC+xMX4pzr6+izwH8BEsHzKqDH3ceD5xP/3pPrIpjfG7SfC5YBHcC/B11dXzGzInLwc+Huh4BPAweBIyT/nTeTm5+LaZFNoW9TTMuJoUVmVgx8D/iwu/e9UtMpps2JdWRmbwTa3X3zxMlTNPUzmDfbRYB1wBfc/WJggBe7cqYyZ9dFsN/iFmApsAAoItmdNVkufC6mRTaFfitQP+H5IuBwhmo5Z8wsSjLw/6+7PxBMbjOz+cH8+UB7MH0ur6MrgTeb2X7gWyS7eD4LlJvZidOFTPx7T66LYH4Z0HUuC55BrUCruz8ZPL+f5JdALn4uXgfsc/cOdx8DHgCuIDc/F9Mim0J/E9AY7JXPI7mz5sEM1zSjgtNOfxXY7u7/OmHWg8AdweM7SPb1n5j+7mC0xmVA74mf+7Odu3/M3Re5+xKS//a/cPd3Ab8kebZWePm6OLGO3h60nxNbdO5+FGgxs1XBpOuBbeTg54Jkt85lZlYY/H85sS5y7nMxbTK9U2HiDbgZ2AW8APxdpus5B3/vVSR/ej4LPBPcbibZB/kosDu4rwzaG8kRTi8Az5Ec0ZDxv2MG1su1wI+Cx8uA3wN7SF6dLT+YHgue7wnmL8t03dO8Di4CmoPPxg+Ailz9XAD/COwAtgLfBPJz9XMxHTcdkSsikkOyqXtHRERmmEJfRCSHKPRFRHKIQl9EJIco9EVEcohCX0Qkhyj0RURyiEJfRCSH/H8XYC0Cx9DQRQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_state_df = pd.DataFrame(train_state)\n",
    "train_state_df['loss'].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_state_df['learning_rate'].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = list(sg_batcher.token_to_idx)\n",
    "W = classifier.embedding.weight.detach().cpu().numpy()\n",
    "W_prime = classifier.fc1.weight.detach().cpu().numpy()\n",
    "\n",
    "W_avg = (W + W_prime) / 2\n",
    "np.savez(args.weights_path, W=W, W_prime=W_prime, W_avg=W_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 100]), torch.Size([47134, 100]))"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.embedding.weight_g.shape, classifier.embedding.weight_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 100]), torch.Size([47134, 100]))"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fc1.weight_g.shape, classifier.fc1.weight_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer.add_embedding(W, metadata=metadata, tag='W')\n",
    "# writer.add_embedding(W_prime, metadata=metadata, tag='W_prime')\n",
    "writer.add_embedding(W_avg, metadata=metadata, tag='W_avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.load(args.weights_path)\n",
    "W = weights['W']\n",
    "W_prime = weights['W_prime']\n",
    "W_avg = weights['W_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.8279662, 3.4566839, -7.8573947, 6.8002295)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.min(), W.max(), W_prime.min(), W_prime.max()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.heatmap(W)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.heatmap(W_prime)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.heatmap(W_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "state_dict = torch.load('../models/naive_word2vec_embeddings_final.pth', map_location='cpu')\n",
    "\n",
    "clf = NaiveWord2VecClassifier(vocabulary_size=vocabulary_size,\n",
    "                              embedding_size=100)\n",
    "clf.load_state_dict(state_dict)\n",
    "\n",
    "W = clf.embedding.weight.detach().numpy()\n",
    "W_prime = clf.fc1.weight.detach().numpy()\n",
    "W_avg = (W + W_prime) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6583071, 1.2893176, 0.97381234)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_norm = W / np.linalg.norm(W, axis=1, keepdims=True)\n",
    "W_prime_norm = W_avg / np.linalg.norm(W_prime, axis=1, keepdims=True)\n",
    "W_avg_norm = (W_norm + W_prime_norm) / 2\n",
    "np.abs(W_norm).max(), np.abs(W_prime_norm).max(), np.abs(W_avg_norm).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(embs_reduced, token_to_idx, tokens):\n",
    "    token_embs = embs_reduced[[token_to_idx[token] for token in tokens]]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    ax.scatter(token_embs[:, 0], token_embs[:, 1], alpha=0.2)\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        ax.annotate(token, (token_embs[i, 0], token_embs[i, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 356 ms, sys: 0 ns, total: 356 ms\n",
      "Wall time: 90.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pca = PCA(n_components=2)\n",
    "embs_pca = pca.fit_transform(W_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokens = ['man', 'woman', 'king', 'queen', 'mother', 'father']\n",
    "# tokens = list(dict(sg_batcher.token_counts.most_common(100)).keys())\n",
    "plot_embeddings(embs_pca, sg_batcher.token_to_idx, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tsne = TSNE(n_components=2, n_iter=1000, n_jobs=-1)\n",
    "embs_tsne = tsne.fit_transform(W_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['man', 'woman', 'king', 'queen', 'mother', 'father']\n",
    "# tokens = list(dict(sg_batcher.token_counts.most_common(100)).keys())\n",
    "plot_embeddings(embs_tsne, sg_batcher.token_to_idx, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "umap = UMAP(metric='cosine')\n",
    "embs_umap = umap.fit_transform(W_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['man', 'woman', 'king', 'queen', 'mother', 'father']\n",
    "# tokens = list(dict(sg_batcher.token_counts.most_common(100)).keys())\n",
    "plot_embeddings(embs_umap, sg_batcher.token_to_idx, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsExplorer():\n",
    "    def __init__(self, token_to_idx, vectors, metric='euclidean'):\n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.idx_to_token = {idx: token for token, idx \n",
    "                             in self.token_to_idx.items()}\n",
    "        self.vectors = vectors\n",
    "        self.metric = metric\n",
    "        \n",
    "        self.index = AnnoyIndex(vectors.shape[1], metric=metric)\n",
    "        \n",
    "        print('Building index is started')\n",
    "        for i in self.token_to_idx.values():\n",
    "            self.index.add_item(i, self.vectors[i])\n",
    "        \n",
    "        self.index.build(50)\n",
    "        print('Building index is finished')\n",
    "        \n",
    "    def get_embedding(self, token):\n",
    "        return self.vectors[self.token_to_idx[token]]\n",
    "    \n",
    "    def get_closest_to_vector(self, vector, n=1):\n",
    "        nn_indices = self.index.get_nns_by_vector(vector, n)\n",
    "        return [self.idx_to_token[neighbor] for neighbor in nn_indices]\n",
    "    \n",
    "    def compute_analogy(self, token1, token2, token3, n=20):\n",
    "        vec1 = self.get_embedding(token1)\n",
    "        vec2 = self.get_embedding(token2)\n",
    "        vec3 = self.get_embedding(token3)\n",
    "        vec4 = vec3 + vec2 - vec1\n",
    "        \n",
    "        tokens = set([token1, token2, token3])\n",
    "        closest_tokens = self.get_closest_to_vector(vec4, n=n)\n",
    "        closest_tokens = [token for token in closest_tokens\n",
    "                          if token not in tokens]\n",
    "        \n",
    "        if len(closest_tokens) == 0:\n",
    "            print('Could not find nearest neighbors for the computed vector')\n",
    "            return\n",
    "        \n",
    "        for token4 in closest_tokens:\n",
    "            print(f'{token1}:{token2} :: {token3}:{token4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building index is started\n",
      "Building index is finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingsExplorer at 0x7feaa9901320>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = EmbeddingsExplorer(sg_batcher.token_to_idx, W_avg, metric='euclidean')\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man:he :: woman:she\n",
      "man:he :: woman:when\n",
      "man:he :: woman:but\n",
      "man:he :: woman:was\n",
      "man:he :: woman:who\n",
      "man:he :: woman:saw\n",
      "man:he :: woman:they\n",
      "man:he :: woman:had\n",
      "man:he :: woman:however\n",
      "man:he :: woman:finally\n",
      "man:he :: woman:never\n",
      "man:he :: woman:apparently\n",
      "man:he :: woman:decided\n",
      "man:he :: woman:initially\n",
      "man:he :: woman:later\n",
      "man:he :: woman:after\n",
      "man:he :: woman:became\n",
      "man:he :: woman:nevertheless\n",
      "man:he :: woman:there\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_analogy('man', 'he', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fly:plane :: sail:vector\n",
      "fly:plane :: sail:sequence\n",
      "fly:plane :: sail:blocks\n",
      "fly:plane :: sail:gravity\n",
      "fly:plane :: sail:block\n",
      "fly:plane :: sail:pair\n",
      "fly:plane :: sail:phase\n",
      "fly:plane :: sail:generator\n",
      "fly:plane :: sail:trajectory\n",
      "fly:plane :: sail:lens\n",
      "fly:plane :: sail:infinite\n",
      "fly:plane :: sail:acceleration\n",
      "fly:plane :: sail:parameter\n",
      "fly:plane :: sail:surfaces\n",
      "fly:plane :: sail:measure\n",
      "fly:plane :: sail:concrete\n",
      "fly:plane :: sail:bow\n",
      "fly:plane :: sail:vectors\n",
      "fly:plane :: sail:linear\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_analogy('fly', 'plane', 'sail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitten:cat :: puppy:dog\n",
      "kitten:cat :: puppy:man\n",
      "kitten:cat :: puppy:girl\n",
      "kitten:cat :: puppy:word\n",
      "kitten:cat :: puppy:like\n",
      "kitten:cat :: puppy:baby\n",
      "kitten:cat :: puppy:words\n",
      "kitten:cat :: puppy:short\n",
      "kitten:cat :: puppy:person\n",
      "kitten:cat :: puppy:character\n",
      "kitten:cat :: puppy:boy\n",
      "kitten:cat :: puppy:meaning\n",
      "kitten:cat :: puppy:my\n",
      "kitten:cat :: puppy:black\n",
      "kitten:cat :: puppy:figure\n",
      "kitten:cat :: puppy:male\n",
      "kitten:cat :: puppy:women\n",
      "kitten:cat :: puppy:female\n",
      "kitten:cat :: puppy:novel\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_analogy('kitten', 'cat', 'puppy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue:color :: dog:etc\n",
      "blue:color :: dog:similarly\n",
      "blue:color :: dog:content\n",
      "blue:color :: dog:mix\n",
      "blue:color :: dog:includes\n",
      "blue:color :: dog:label\n",
      "blue:color :: dog:typically\n",
      "blue:color :: dog:dogs\n",
      "blue:color :: dog:pattern\n",
      "blue:color :: dog:typical\n",
      "blue:color :: dog:display\n",
      "blue:color :: dog:sample\n",
      "blue:color :: dog:picture\n",
      "blue:color :: dog:frame\n",
      "blue:color :: dog:drawing\n",
      "blue:color :: dog:whole\n",
      "blue:color :: dog:setting\n",
      "blue:color :: dog:copy\n",
      "blue:color :: dog:technique\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_analogy('blue', 'color', 'dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leg:legs :: hand:others\n",
      "leg:legs :: hand:typically\n",
      "leg:legs :: hand:usually\n",
      "leg:legs :: hand:etc\n",
      "leg:legs :: hand:hence\n",
      "leg:legs :: hand:thus\n",
      "leg:legs :: hand:while\n",
      "leg:legs :: hand:often\n",
      "leg:legs :: hand:animals\n",
      "leg:legs :: hand:humans\n",
      "leg:legs :: hand:words\n",
      "leg:legs :: hand:various\n",
      "leg:legs :: hand:unlike\n",
      "leg:legs :: hand:together\n",
      "leg:legs :: hand:individual\n",
      "leg:legs :: hand:though\n",
      "leg:legs :: hand:hands\n",
      "leg:legs :: hand:therefore\n",
      "leg:legs :: hand:hold\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_analogy('leg', 'legs', 'hand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toe:finger :: foot:section\n",
      "toe:finger :: foot:wall\n",
      "toe:finger :: foot:bottom\n",
      "toe:finger :: foot:top\n",
      "toe:finger :: foot:feet\n",
      "toe:finger :: foot:points\n",
      "toe:finger :: foot:track\n",
      "toe:finger :: foot:face\n",
      "toe:finger :: foot:leg\n",
      "toe:finger :: foot:arm\n",
      "toe:finger :: foot:plus\n",
      "toe:finger :: foot:notes\n",
      "toe:finger :: foot:eye\n",
      "toe:finger :: foot:lines\n",
      "toe:finger :: foot:stone\n",
      "toe:finger :: foot:speed\n",
      "toe:finger :: foot:cap\n",
      "toe:finger :: foot:walls\n",
      "toe:finger :: foot:shot\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_analogy('toe', 'finger', 'foot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talk:communicate :: read:easily\n",
      "talk:communicate :: read:freely\n",
      "talk:communicate :: read:proceed\n",
      "talk:communicate :: read:efficiently\n",
      "talk:communicate :: read:properly\n",
      "talk:communicate :: read:transmitted\n",
      "talk:communicate :: read:copied\n",
      "talk:communicate :: read:implemented\n",
      "talk:communicate :: read:execute\n",
      "talk:communicate :: read:automatically\n",
      "talk:communicate :: read:treated\n",
      "talk:communicate :: read:interpreted\n",
      "talk:communicate :: read:transported\n",
      "talk:communicate :: read:translate\n",
      "talk:communicate :: read:transferred\n",
      "talk:communicate :: read:operate\n",
      "talk:communicate :: read:recognise\n",
      "talk:communicate :: read:develop\n",
      "talk:communicate :: read:appealed\n",
      "talk:communicate :: read:convert\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_analogy('talk', 'communicate', 'read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man:king :: woman:prince\n",
      "man:king :: woman:son\n",
      "man:king :: woman:monarch\n",
      "man:king :: woman:father\n",
      "man:king :: woman:mayor\n",
      "man:king :: woman:henry\n",
      "man:king :: woman:ruler\n",
      "man:king :: woman:bishop\n",
      "man:king :: woman:kings\n",
      "man:king :: woman:lord\n",
      "man:king :: woman:successor\n",
      "man:king :: woman:captain\n",
      "man:king :: woman:alexander\n",
      "man:king :: woman:president\n",
      "man:king :: woman:james\n",
      "man:king :: woman:william\n",
      "man:king :: woman:arthur\n",
      "man:king :: woman:charles\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_analogy('man', 'king', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man:doctor :: woman:genius\n",
      "man:doctor :: woman:villain\n",
      "man:doctor :: woman:practitioner\n",
      "man:doctor :: woman:persona\n",
      "man:doctor :: woman:wise\n",
      "man:doctor :: woman:speaker\n",
      "man:doctor :: woman:bride\n",
      "man:doctor :: woman:narrator\n",
      "man:doctor :: woman:nurse\n",
      "man:doctor :: woman:doom\n",
      "man:doctor :: woman:bachelor\n",
      "man:doctor :: woman:protagonist\n",
      "man:doctor :: woman:sees\n",
      "man:doctor :: woman:foremost\n",
      "man:doctor :: woman:assassin\n",
      "man:doctor :: woman:cf\n",
      "man:doctor :: woman:speaks\n",
      "man:doctor :: woman:servant\n",
      "man:doctor :: woman:choosing\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_analogy('man', 'doctor', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast:fastest :: small:growing\n",
      "fast:fastest :: small:scale\n",
      "fast:fastest :: small:size\n",
      "fast:fastest :: small:huge\n",
      "fast:fastest :: small:largest\n",
      "fast:fastest :: small:larger\n",
      "fast:fastest :: small:overall\n",
      "fast:fastest :: small:large\n",
      "fast:fastest :: small:growth\n",
      "fast:fastest :: small:massive\n",
      "fast:fastest :: small:substantial\n",
      "fast:fastest :: small:increasing\n",
      "fast:fastest :: small:increases\n",
      "fast:fastest :: small:dominant\n",
      "fast:fastest :: small:commercial\n",
      "fast:fastest :: small:vast\n",
      "fast:fastest :: small:tiny\n",
      "fast:fastest :: small:amount\n",
      "fast:fastest :: small:presence\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_analogy('fast', 'fastest', 'small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_batcher = SkipGramBatcher.from_file(args.file_path)\n",
    "sg_batcher.prepare_data(cutoff=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0]), tensor([1, 2, 3, 2, 3]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = sg_batcher.generate_batches(window_size=5, \n",
    "                                batch_size=5)\n",
    "x_batch, labels_batch = next(g)\n",
    "\n",
    "x_batch, labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 50)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = len(sg_batcher.vocab)\n",
    "embedding_size = 50\n",
    "\n",
    "vocabulary_size, embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = NaiveWord2VecClassifier(vocabulary_size=vocabulary_size,\n",
    "                              embedding_size=embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1679, -0.7678, -0.4254,  0.3084,  0.4510, -0.2278, -0.0301,  0.1163,\n",
       "          0.0782,  0.1542, -0.5275, -0.5969, -0.2852,  0.9326, -0.0609,  1.0134,\n",
       "          0.7858, -0.3970, -0.0972,  0.2352, -0.1621, -0.3524],\n",
       "        [ 0.1679, -0.7678, -0.4254,  0.3084,  0.4510, -0.2278, -0.0301,  0.1163,\n",
       "          0.0782,  0.1542, -0.5275, -0.5969, -0.2852,  0.9326, -0.0609,  1.0134,\n",
       "          0.7858, -0.3970, -0.0972,  0.2352, -0.1621, -0.3524],\n",
       "        [ 0.1679, -0.7678, -0.4254,  0.3084,  0.4510, -0.2278, -0.0301,  0.1163,\n",
       "          0.0782,  0.1542, -0.5275, -0.5969, -0.2852,  0.9326, -0.0609,  1.0134,\n",
       "          0.7858, -0.3970, -0.0972,  0.2352, -0.1621, -0.3524],\n",
       "        [ 0.1679, -0.7678, -0.4254,  0.3084,  0.4510, -0.2278, -0.0301,  0.1163,\n",
       "          0.0782,  0.1542, -0.5275, -0.5969, -0.2852,  0.9326, -0.0609,  1.0134,\n",
       "          0.7858, -0.3970, -0.0972,  0.2352, -0.1621, -0.3524],\n",
       "        [ 0.1679, -0.7678, -0.4254,  0.3084,  0.4510, -0.2278, -0.0301,  0.1163,\n",
       "          0.0782,  0.1542, -0.5275, -0.5969, -0.2852,  0.9326, -0.0609,  1.0134,\n",
       "          0.7858, -0.3970, -0.0972,  0.2352, -0.1621, -0.3524]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf(x_batch)\n",
    "# y_pred = F.softmax(y_pred, dim=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 2, 3])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7678, -0.4254,  0.3084, -0.4254,  0.3084], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[range(5), labels_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.9903, 3.6480, 2.9141, 3.6480, 2.9141], grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.log(y_pred[range(5), labels_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = torch.zeros((5, vocabulary_size), dtype=torch.long)\n",
    "y_true[range(5), labels_batch] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 22]), torch.Size([5, 22]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.shape, y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3.9903, 3.6480, 2.9141, 3.6480, 2.9141], grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss(reduce=False)\n",
    "loss(y_pred, labels_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = torch.zeros((5, vocabulary_size))\n",
    "arr[range(5), labels_batch] = 1\n",
    "arr.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 2, 3])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in log\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "loss = -np.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
