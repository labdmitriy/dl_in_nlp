{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative sampling\n",
    "\n",
    "You may have noticed that word2vec is really slow to train. Especially with big (> 50 000) vocabularies. Negative sampling is the solution.\n",
    "\n",
    "The task is to implement word2vec with negative sampling. In more detail: you should implement two ways of negative sampling.\n",
    "\n",
    "## Vanilla negative sampling\n",
    "\n",
    "This is what was discussed in Stanford lecture. The main idea is in the formula:\n",
    "\n",
    "$$ L = \\log\\sigma(u^T_o u_c) + \\sum^k_{i=1} \\mathbb{E}_{j \\sim P(w)}[\\log\\sigma(-u^T_j, u_c)]$$\n",
    "\n",
    "Where $\\sigma$ - sigmoid function, $u_c$ - central word vector, $u_o$ - context (outside of the window) word vector, $u_j$ - vector or word with index $j$.\n",
    "\n",
    "The first term calculates the similarity between positive examples (word from one window)\n",
    "\n",
    "The second term is responsible for negative samples. $k$ is a hyperparameter - the number of negatives to sample.\n",
    "$\\mathbb{E}_{j \\sim P(w)}$\n",
    "means that $j$ is distributed accordingly to unigram distribution, but it is better to use $P^{3/4}(w)$ (empirical results) and you can experiment with some other approaches (for example, try to use uniform distribution).\n",
    "\n",
    "Thus, it is only required to calculate the similarity between positive samples and some other negatives. Not across all the vocabulary.\n",
    "\n",
    "Useful links:\n",
    "1. [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "1. [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "\n",
    "## Batch-transpose trick for negative sampling\n",
    "\n",
    "But we can do better. Maybe we don't need to compute vectors for negative samples at all, because we already have a batch of training data and (hopefully) examples in the batch are highly decorrelated.\n",
    "\n",
    "Let's assume we work with Skip-gram model.\n",
    "\n",
    "Let $S$ be a batch of _L2-normalized_ word vectors `(batch_size, 2*window_size + 1, word_vector_dim)`.\n",
    "\n",
    "```python\n",
    "x = 0.0\n",
    "for batch_idx in range(batch):\n",
    "    w = S[batch_idx, :, :]\n",
    "    x += np.sum(w.T @ w - 1.)\n",
    "\n",
    "y = 0.0\n",
    "for window_idx in range(window):\n",
    "    b = S[:, window_idx, :]\n",
    "    y += np.sum(b.T @ b)\n",
    "\n",
    "loss = -x + y```\n",
    "\n",
    "Think about this loss and compare it to vanilla negative sampling.\n",
    "\n",
    "Implement word2vec with batch-transpose trick. Modify the formula, if needed.\n",
    "\n",
    "If you are interested: [more info](https://www.tensorflow.org/extras/candidate_sampling.pdf) on other methods of candidate sampling.\n",
    "\n",
    "**Results of this task** are the very same as in task 3, **plus**:\n",
    " * implement two models (one with vanilla negative sampling and the other with batch-transpose trick)\n",
    " * compare all of the models from tasks 3-5. In terms of time and number of iterations until convergence and the quality of the resulting vectors.\n",
    " * answer the questions\n",
    "\n",
    "### Questions:\n",
    "1. Explain the batch-transpose trick formula in your own words. How would you name x, y, w and b?\n",
    "1. Should it be modified to serve as a word2vec loss? If yes, how?\n",
    "1. Is it possible to do the same trick with CBOW model? If yes, how?\n",
    "1. Does it matter how the batch is made in the case of batch-transpose trick? In the case of vanilla negative sampling?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers:\n",
    "  1. _\n",
    "  1. _\n",
    "  1. _\n",
    "  1. _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from itertools import islice, product, chain\n",
    "from argparse import Namespace\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "from umap import UMAP\n",
    "from adjustText import adjust_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('../data')\n",
    "MODELS_PATH = Path('../models')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class SkipGramBatcher():\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, file_path):\n",
    "        with open(file_path) as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        return cls(text)\n",
    "    \n",
    "    def _tokenize(self):\n",
    "        self.tokens = self.text.split()\n",
    "    \n",
    "    def _count_tokens(self):\n",
    "        self.token_counts = Counter(self.tokens)\n",
    "    \n",
    "    def _build_vocab(self, cutoff):\n",
    "        self.token_counts = dict(filter(lambda x: x[1] >= cutoff, self.token_counts.items()))\n",
    "        self.token_to_idx = {token:idx for (idx, (token, _)) \n",
    "                             in enumerate(self.token_counts.items())}\n",
    "        self.idx_to_token = {idx:token for (token, idx) in self.token_to_idx.items()}\n",
    "        self.vocab = set(self.token_to_idx)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "    def _filter_tokens(self):\n",
    "        self.tokens = [token for token in self.tokens if token in self.vocab]\n",
    "        self.tokens_size = len(self.tokens)\n",
    "    \n",
    "    def _vectorize_tokens(self):\n",
    "        self.vectorized_tokens = [self.token_to_idx[token] for token in self.tokens]\n",
    "    \n",
    "    def _calculate_neg_probs(self, neg_prob_exp):\n",
    "        token_neg_scores = [(value / self.tokens_size)**neg_prob_exp\n",
    "                            for key, value in self.token_counts.items()]\n",
    "        self.token_neg_probs = list(token_neg_scores / np.sum(token_neg_scores))\n",
    "        \n",
    "    def _generate_negative_outputs(self, k=1, size=1):\n",
    "#         for i in range(0, self.tokens_size):\n",
    "#             negative_outputs = torch.multinomial(torch.tensor(self.token_neg_probs), \n",
    "#                                                  size * k, replacement=False).view(size, -1)\n",
    "#         while True:\n",
    "            negative_outputs = np.random.choice(range(self.vocab_size),\n",
    "                                                size = size * k,\n",
    "                                                replace=True,\n",
    "                                                p = self.token_neg_probs).reshape(size, -1)\n",
    "            negative_outputs = torch.from_numpy(negative_outputs)\n",
    "            return negative_outputs\n",
    "        \n",
    "    def _create_sliding_window(self, window_size, k):\n",
    "        for i in range(0, self.tokens_size):\n",
    "            center_word = self.vectorized_tokens[i:i+1]\n",
    "            left_context = self.vectorized_tokens[max(0, i - window_size): i]\n",
    "            right_context = self.vectorized_tokens[i + 1: min(self.tokens_size, i + window_size + 1)]\n",
    "            context = left_context + right_context\n",
    "#             print(context)\n",
    "            context = list(zip(context, self._get_negative_outputs(k, len(context))))\n",
    "#             print(context)\n",
    "            window = [list(product(center_word, context))]\n",
    "            yield window \n",
    "    \n",
    "    def devectorize_tokens(self, indices):\n",
    "        return [self.idx_to_token[idx] for idx in indices]\n",
    "        \n",
    "    def prepare_data(self, cutoff=1, neg_prob_exp=0.75):\n",
    "        self._tokenize()\n",
    "        self._count_tokens()\n",
    "        self._build_vocab(cutoff)\n",
    "        self._filter_tokens()\n",
    "        self._vectorize_tokens()\n",
    "        self._calculate_neg_probs(neg_prob_exp)\n",
    "        \n",
    "    def generate_batches(self, window_size=1, batch_size=1, k=0, drop_last=True):\n",
    "        window = self._create_sliding_window(window_size, k)\n",
    "        batch = list(zip(*islice(window, batch_size)))\n",
    "#         print(batch, len(batch[0]), batch_size)\n",
    "        \n",
    "        if drop_last:\n",
    "            while batch and len(batch[0]) == batch_size:\n",
    "                batch = list(zip(*[pair for pairs in batch[0] for pair in pairs]))\n",
    "                #batch = list(zip(*batch[0][0]))\n",
    "                yield batch\n",
    "                batch = list(zip(*islice(window, batch_size)))\n",
    "        else:\n",
    "            while batch:\n",
    "                batch = list(zip(*batch))\n",
    "                yield x_batch, labels_batch\n",
    "                batch = list(zip(*islice(window, batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramBatcher():\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, file_path):\n",
    "        with open(file_path) as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        return cls(text)\n",
    "    \n",
    "    def _tokenize(self):\n",
    "        self.tokens = self.text.split()\n",
    "    \n",
    "    def _count_tokens(self):\n",
    "        self.token_counts = Counter(self.tokens)\n",
    "    \n",
    "    def _build_vocab(self, cutoff):\n",
    "        self.token_counts = dict(filter(lambda x: x[1] >= cutoff, self.token_counts.items()))\n",
    "        self.token_to_idx = {token:idx for (idx, (token, _)) \n",
    "                             in enumerate(self.token_counts.items())}\n",
    "        self.idx_to_token = {idx:token for (token, idx) in self.token_to_idx.items()}\n",
    "        self.vocab = set(self.token_to_idx)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "    def _filter_tokens(self):\n",
    "        self.tokens = [token for token in self.tokens if token in self.vocab]\n",
    "        self.tokens_size = len(self.tokens)\n",
    "    \n",
    "    def _vectorize_tokens(self):\n",
    "        self.vectorized_tokens = [self.token_to_idx[token] for token in self.tokens]\n",
    "        \n",
    "    def _calculate_neg_probs(self, neg_prob_exp):\n",
    "        token_neg_scores = [(value / self.tokens_size)**neg_prob_exp\n",
    "                            for key, value in self.token_counts.items()]\n",
    "        self.token_neg_probs = list(token_neg_scores / np.sum(token_neg_scores))\n",
    "    \n",
    "    def _create_sliding_window(self, window_size):\n",
    "        tokens_size = len(self.tokens)\n",
    "\n",
    "        for i in range(0, tokens_size):\n",
    "            center_word = self.vectorized_tokens[i:i+1]\n",
    "            left_context = self.vectorized_tokens[max(0, i - window_size): i]\n",
    "            right_context = self.vectorized_tokens[i + 1: min(self.tokens_size, i + window_size + 1)]\n",
    "            context = left_context + right_context\n",
    "            window = [list(product(center_word, context))]\n",
    "            yield window \n",
    "    \n",
    "    def devectorize_tokens(self, indices):\n",
    "        return [self.idx_to_token[idx] for idx in indices]\n",
    "        \n",
    "    def prepare_data(self, cutoff=1, neg_prob_exp=0.75):\n",
    "        self._tokenize()\n",
    "        self._count_tokens()\n",
    "        self._build_vocab(cutoff)\n",
    "        self._filter_tokens()\n",
    "        self._vectorize_tokens()\n",
    "        self._calculate_neg_probs(neg_prob_exp)\n",
    "        \n",
    "    def generate_batches(self, window_size=1, batch_size=1, drop_last=True):\n",
    "        window = self._create_sliding_window(window_size)\n",
    "        batch = list(zip(*islice(window, batch_size)))\n",
    "        \n",
    "        if drop_last:\n",
    "            while batch and len(batch[0]) == batch_size:\n",
    "                batch = list(zip(*[pair for pairs in batch[0] for pair in pairs]))\n",
    "                x_batch, labels_batch = torch.tensor(batch[0]), torch.tensor(batch[1])\n",
    "                yield x_batch, labels_batch\n",
    "                batch = list(zip(*islice(window, batch_size)))\n",
    "        else:\n",
    "            while batch:\n",
    "                batch = list(zip(*[pair for pairs in batch[0] for pair in pairs]))\n",
    "                x_batch, labels_batch = torch.tensor(batch[0]), torch.tensor(batch[1])\n",
    "                yield x_batch, labels_batch\n",
    "                batch = list(zip(*islice(window, batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_batcher = SkipGramBatcher.from_file(DATA_PATH/'text8')\n",
    "sg_batcher.prepare_data(cutoff=10, neg_prob_exp=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.3897566846470835e-05,\n",
       " 5.459257457856733e-05,\n",
       " 0.003228941523377695,\n",
       " 0.006366090944159513,\n",
       " 0.0003655479357826762]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_neg_probs = sg_batcher.token_neg_probs\n",
    "token_neg_probs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 1]), tensor([1, 0, 2]))"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = sg_batcher.generate_batches(window_size=1, batch_size=2)\n",
    "arr = next(g)\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveWord2VecClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, token_neg_probs, k=1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.token_neg_probs = token_neg_probs\n",
    "        \n",
    "#         self.weight_c = nn.Parameter(torch.randn(vocab_size, embedding_size))\n",
    "#         self.weight_o = nn.Parameter(torch.randn(embedding_size, vocab_size))\n",
    "        self.emb_c = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                  embedding_dim=embedding_size)\n",
    "        self.emb_o = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                  embedding_dim=embedding_size)\n",
    "        \n",
    "    def forward(self, v_c, v_o):\n",
    "        # (1 x d)\n",
    "#         x_embedded = self.weight_c[x_in]\n",
    "        v_c_embedded = self.emb_c(v_c)\n",
    "        # (k x d)\n",
    "        neg_samples = self.generate_neg_outputs(k=3, batch_size=v_c.shape[0])\n",
    "        neg_embedded = self.emb_o(neg_samples)\n",
    "        v_o_embedded = self.emb_o(v_o)\n",
    "#         o_embedded = torch.cat([v_o_embedded, neg_embedded], dim=0)\n",
    "        \n",
    "        return v_c_embedded, v_o_embedded, neg_samples, neg_embedded#, o_embedded\n",
    "    \n",
    "    def generate_neg_outputs(self, batch_size, k=1):\n",
    "        negative_outputs = np.random.choice(range(self.vocab_size),\n",
    "                                            size=batch_size * k,\n",
    "                                            replace=True,\n",
    "                                            p=self.token_neg_probs).reshape(batch_size, -1)\n",
    "        negative_outputs = torch.from_numpy(negative_outputs)\n",
    "        return negative_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 1]), tensor([1, 0, 2]))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.4100, -1.4974],\n",
       "         [ 1.2116, -1.8269],\n",
       "         [ 1.2116, -1.8269]], grad_fn=<EmbeddingBackward>),\n",
       " tensor([[-0.4380,  0.1938],\n",
       "         [ 0.2529, -0.9034],\n",
       "         [ 0.6692, -0.6682]], grad_fn=<EmbeddingBackward>),\n",
       " tensor([[13407, 34047,   683],\n",
       "         [43137,   779,  5082],\n",
       "         [ 3501,  4433, 19259]]),\n",
       " tensor([[[ 0.5742, -1.4690],\n",
       "          [-0.3603, -0.5749],\n",
       "          [ 1.4960,  0.9767]],\n",
       " \n",
       "         [[ 0.9171,  1.2679],\n",
       "          [-0.2828, -0.3097],\n",
       "          [ 0.8333,  0.2855]],\n",
       " \n",
       "         [[-0.8574, -0.4804],\n",
       "          [-0.4560,  0.5374],\n",
       "          [-0.2353,  1.5769]]], grad_fn=<EmbeddingBackward>))"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = NaiveWord2VecClassifier(sg_batcher.vocab_size, 2, token_neg_probs)\n",
    "res = clf(arr[0], arr[1])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2]) torch.Size([3, 2]) torch.Size([3, 3, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.4100, -1.4974],\n",
       "         [ 1.2116, -1.8269],\n",
       "         [ 1.2116, -1.8269]], grad_fn=<EmbeddingBackward>),\n",
       " tensor([[-0.4380,  0.1938],\n",
       "         [ 0.2529, -0.9034],\n",
       "         [ 0.6692, -0.6682]], grad_fn=<EmbeddingBackward>),\n",
       " tensor([[[ 0.5742, -1.4690],\n",
       "          [-0.3603, -0.5749],\n",
       "          [ 1.4960,  0.9767]],\n",
       " \n",
       "         [[ 0.9171,  1.2679],\n",
       "          [-0.2828, -0.3097],\n",
       "          [ 0.8333,  0.2855]],\n",
       " \n",
       "         [[-0.8574, -0.4804],\n",
       "          [-0.4560,  0.5374],\n",
       "          [-0.2353,  1.5769]]], grad_fn=<EmbeddingBackward>))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_c, v_o, _, neg = res\n",
    "print(v_c.shape, v_o.shape, neg.shape)\n",
    "v_c, v_o, neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 2]), torch.Size([3, 3, 2]), torch.Size([3, 2, 3]))"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_c.unsqueeze(0).shape, neg.shape, torch.transpose(neg, 2, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor to have size 1 at dimension 0, but got size 3 for argument #2 'batch2' (while checking arguments for bmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-182-7784f666ac33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor to have size 1 at dimension 0, but got size 3 for argument #2 'batch2' (while checking arguments for bmm)"
     ]
    }
   ],
   "source": [
    "torch.bmm(v_c.unsqueeze(0), torch.transpose(neg, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sg_batcher.generate_batches(window_size=3,\n",
    "                                batch_size=1024)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for _ in tqdm_notebook(g):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2021b057000446ab7e3bd9e1b182331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in tqdm_notebook(range(100)):\n",
    "    np.random.choice(range(sg_batcher.vocab_size),\n",
    "                        size = 1024 * 3,\n",
    "                        replace=True,\n",
    "                        p = sg_batcher.token_neg_probs).reshape(1024, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e34adc0fe414a84a731dd4d089f8cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in tqdm_notebook(range(100)):\n",
    "    sg_batcher._generate_negative_outputs(k=3, size=256)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sg_batcher._generate_negative_outputs(k=3, size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1639, 10077,  6125,  2075],\n",
       "       [ 3150,  4959, 29402,   305],\n",
       "       [ 1563, 12646, 38240, 25660]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(range(len(sg_batcher.vocab)), size=12, replace=True, p=sg_batcher.token_neg_probs).reshape(3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, (1, [340, 34997, 15])),\n",
       " (0, (2, [248, 11726, 1903])),\n",
       " (0, (3, [13380, 45263, 4490]))]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = [(0, (1, [340, 34997, 15])), (0, (2, [248, 11726, 1903])), (0, (3, [13380, 45263, 4490]))]\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, [340, 34997, 15]), (2, [248, 11726, 1903]), (3, [13380, 45263, 4490]))"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(*batch))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "center = [0, 0, 0]\n",
    "context = [[(0, (1, [23790, 4499, 165])),\n",
    "          (0, (2, [4135, 20707, 13853])),\n",
    "          (0, (3, [4306, 3837, 243]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, (1, [14550, 17844, 19042])),\n",
       " (0, (2, [5141, 33485, 21678])),\n",
       " (0, (3, [3263, 675, 33346]))]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(center, context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, (1, [4508, 267, 1695])),\n",
       " (0, (2, [4084, 344, 27567])),\n",
       " (0, (3, [6347, 327, 10095]))]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = [(0, (1, [4508, 267, 1695])), (0, (2, [4084, 344, 27567])), (0, (3, [6347, 327, 10095]))]\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list() takes at most 1 argument (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-077931a1f7c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list() takes at most 1 argument (3 given)"
     ]
    }
   ],
   "source": [
    "list(zip(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_batcher.token_neg_probs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_g = sg_batcher._get_negative_outputs(k=3, size=3)\n",
    "neg_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, [1801, 579, 25424], 2, [4366, 32120, 381], 3, [8384, 21399, 5617]]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(chain.from_iterable(zip([1, 2, 3], neg_g)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  506, 20011,  2034,   823],\n",
       "        [14741, 20081, 18746, 41748],\n",
       "        [24784,    15, 26064,  4076]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.multinomial(torch.tensor(token_neg_probs), \n",
    "                        12, replacement=False).view(3, -1)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[506, 20011, 2034, 823],\n",
       " [14741, 20081, 18746, 41748],\n",
       " [24784, 15, 26064, 4076]]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    file_path = DATA_PATH/'text8',\n",
    "    model_state_path = MODELS_PATH/'naive_word2vec_embeddings.pth',\n",
    "    weights_path = MODELS_PATH/'weights.npz',\n",
    "    \n",
    "    embedding_size = 100,\n",
    "    \n",
    "    seed = 42,\n",
    "    cutoff = 10,\n",
    "    window_size = 3,\n",
    "    stride = 1,\n",
    "    batch_size = 1024,\n",
    "    learning_rate = 0.03,\n",
    "    iterations = 1000,\n",
    "    save_iterations = 100,\n",
    "    early_stopping_criteria = 1e8,\n",
    "    factor=0.5,\n",
    "    patience=5000,\n",
    "    neg_prob_degree=0.75\n",
    "    \n",
    "    cuda=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (0, 2)]]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = sg_batcher._create_sliding_window(2)\n",
    "next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16561031"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_size = len(sg_batcher.tokens)\n",
    "tokens_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00027975, 0.00045054, 0.0266476 , 0.05253766, 0.00301677,\n",
       "       0.08238481, 0.00044521, 0.00851808, 0.00713238, 0.00338947])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_neg_scores = np.array([(value/sg_batcher.tokens_size)**0.75\n",
    "                            for key, value in sg_batcher.token_counts.items()])\n",
    "token_neg_scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.252735343222646"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_neg_scores_sum = np.sum(token_neg_scores)\n",
    "token_neg_scores_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.38975668e-05, 5.45925746e-05, 3.22894152e-03, ...,\n",
       "       2.62474160e-06, 3.90771827e-06, 2.62474160e-06])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_neg_probs = token_neg_scores / token_neg_scores_sum\n",
    "token_neg_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f32772ee6d8>]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAHB1JREFUeJzt3X90VeWd7/H3h4SEH/JDQqwKjIkLrI2OUzWX0bYz05ZOxR8tzly8jWt1hul468xUb9s702mhXbUzTO3V3s7V9qqdy1Ju0XYKlra3mZaW/sDe1jsWCP6oBkQjYomgREEUEEKS7/3jPNDDyUnOJgkmcD6vtbLY+9nfZ5/n2Yvkk733OdmKCMzMzEYN9wDMzGxkcCCYmRngQDAzs8SBYGZmgAPBzMwSB4KZmQEOBDMzSxwIZmYGZAwESXMlbZbUJmlhke3Vklak7Wsl1aX2GkkPSNor6Y6CPlWSlkh6StKTkv7jUEzIzMwGprJUgaQK4E7gj4F2YL2k5ojYmFd2HbA7ImZKagJuBT4AHAA+C5yfvvJ9BtgZEedIGgVMKTWWqVOnRl1dXelZmZnZERs2bHgpImpL1ZUMBGA20BYRWwAkLQfmAfmBMA/4h7S8ErhDkiJiH/CgpJlF9vuXwLkAEdEDvFRqIHV1dbS0tGQYspmZHSbpuSx1WS4ZTQO25a23p7aiNRHRBewBavoZ3OS0+E+SHpb0LUlvyjJgMzM7PrIEgoq0Ff5FvCw1+SqB6cD/i4iLgIeALxV9cel6SS2SWjo6OjIM18zMBiJLILQDM/LWpwPb+6qRVAlMAnb1s8+Xgf3Ad9P6t4CLihVGxJKIaIyIxtrakpfAzMxsgLIEwnpglqR6SVVAE9BcUNMMLEjL84E10c/f1U7b/g14Z2qaw9H3JMzM7A1W8qZyRHRJuhFYDVQASyOiVdJioCUimoF7gPsktZE7M2g63F/SVmAiUCXpauC96R1Kn0p9bgc6gA8N7dTMzOxY6ER6QE5jY2P4XUZmZsdG0oaIaCxV508qm5kZUCaB8MTze3hs2yvDPQwzsxEtywfTTnhX/c8HAdh6y5XDPBIzs5GrLM4QzMysNAeCmZkBDgQzM0scCGZmBjgQzMwscSCYmRngQDAzs8SBYGZmgAPBzMwSB4KZmQEOBDMzSxwIZmYGOBDMzCxxIJiZGZAxECTNlbRZUpukhUW2V0takbavlVSX2mskPSBpr6Q7+th3s6QnBjMJMzMbvJKBIKkCuBO4HGgArpXUUFB2HbA7ImYCtwG3pvYDwGeBT/Sx7z8F9g5s6GZmNpSynCHMBtoiYktEdALLgXkFNfOAZWl5JTBHkiJiX0Q8SC4YjiLpFOBvgc8PePRmZjZksgTCNGBb3np7aitaExFdwB6gpsR+/wn4Z2B/ppGamdlxlSUQVKQtBlDz22LprcDMiPhuyReXrpfUIqmlo6OjVLmZmQ1QlkBoB2bkrU8HtvdVI6kSmATs6meflwIXS9oKPAicI+nnxQojYklENEZEY21tbYbhmpnZQGQJhPXALEn1kqqAJqC5oKYZWJCW5wNrIqLPM4SI+GpEnBkRdcA7gKci4p3HOngzMxs6laUKIqJL0o3AaqACWBoRrZIWAy0R0QzcA9wnqY3cmUHT4f7pLGAiUCXpauC9EbFx6KdiZmaDUTIQACJiFbCqoO2mvOUDwDV99K0rse+twPlZxmFmZsePP6lsZmaAA8HMzBIHgpmZAQ4EMzNLHAhmZgY4EMzMLHEgmJkZ4EAwM7PEgWBmZoADwczMEgeCmZkBDgQzM0scCGZmBjgQzMwscSCYmRngQDAzs8SBYGZmQMZAkDRX0mZJbZIWFtleLWlF2r5WUl1qr5H0gKS9ku7Iqx8n6QeSnpTUKumWoZqQmZkNTMlAkFQB3AlcDjQA10pqKCi7DtgdETOB24BbU/sB4LPAJ4rs+ksRcS5wIfB2SZcPbApmZjYUspwhzAbaImJLRHQCy4F5BTXzgGVpeSUwR5IiYl9EPEguGI6IiP0R8UBa7gQeBqYPYh5mZjZIWQJhGrAtb709tRWtiYguYA9Qk2UAkiYD7wN+1sf26yW1SGrp6OjIskszMxuALIGgIm0xgJreO5YqgW8CX4mILcVqImJJRDRGRGNtbW3JwZqZ2cBkCYR2YEbe+nRge1816Yf8JGBXhn0vAZ6OiNsz1JqZ2XGUJRDWA7Mk1UuqApqA5oKaZmBBWp4PrImIfs8QJH2eXHB8/NiGbGZmx0NlqYKI6JJ0I7AaqACWRkSrpMVAS0Q0A/cA90lqI3dm0HS4v6StwESgStLVwHuBV4HPAE8CD0sCuCMi7h7KyZmZWXYlAwEgIlYBqwrabspbPgBc00ffuj52W+y+g5mZDRN/UtnMzAAHgpmZJQ4EMzMDHAhmZpY4EMzMDHAgmJlZ4kAwMzPAgWBmZokDwczMAAeCmZklDgQzMwMcCGZmljgQzMwMcCCYmVniQDAzM8CBYGZmSaZAkDRX0mZJbZIWFtleLWlF2r5WUl1qr5H0gKS9ku4o6HOxpMdTn68oPTbNzMyGR8lAkFQB3AlcDjQA10pqKCi7DtgdETOB24BbU/sB4LPAJ4rs+qvA9cCs9DV3IBMwM7OhkeUMYTbQFhFbIqITWA7MK6iZByxLyyuBOZIUEfsi4kFywXCEpDOAiRHxUEQEcC9w9WAmYmZmg5MlEKYB2/LW21Nb0ZqI6AL2ADUl9tleYp9mZvYGyhIIxa7txwBqBlQv6XpJLZJaOjo6+tmlmZkNRpZAaAdm5K1PB7b3VSOpEpgE7Cqxz+kl9glARCyJiMaIaKytrc0wXDMzG4gsgbAemCWpXlIV0AQ0F9Q0AwvS8nxgTbo3UFRE7ABek3RJenfRnwPfO+bRm5nZkKksVRARXZJuBFYDFcDSiGiVtBhoiYhm4B7gPklt5M4Mmg73l7QVmAhUSboaeG9EbAT+BvgaMBb4YfoyM7NhUjIQACJiFbCqoO2mvOUDwDV99K3ro70FOD/rQM3M7PjyJ5XNzAxwIJiZWeJAMDMzwIFgZmaJA8HMzAAHgpmZJQ4EMzMDHAhmZpY4EMzMDHAgmJlZ4kAwMzPAgWBmZokDwczMAAeCmZklDgQzMwMcCGZmljgQzMwMyBgIkuZK2iypTdLCIturJa1I29dKqsvbtii1b5Z0WV77f5XUKukJSd+UNGYoJmRmZgNTMhAkVQB3ApcDDcC1khoKyq4DdkfETOA24NbUt4Hc85XPA+YCd0mqkDQN+CjQGBHnk3tWcxNmZjZsspwhzAbaImJLRHQCy4F5BTXzgGVpeSUwR5JS+/KIOBgRzwJtaX+Qe57zWEmVwDhg++CmYmZmg5ElEKYB2/LW21Nb0ZqI6AL2ADV99Y2I54EvAb8BdgB7IuLHxV5c0vWSWiS1dHR0ZBiumZkNRJZAUJG2yFhTtF3SqeTOHuqBM4Hxkj5Y7MUjYklENEZEY21tbYbhmpnZQGQJhHZgRt76dHpf3jlSky4BTQJ29dP3PcCzEdEREYeA7wBvG8gEzMxsaGQJhPXALEn1kqrI3fxtLqhpBhak5fnAmoiI1N6U3oVUD8wC1pG7VHSJpHHpXsMcYNPgp2NmZgNVWaogIrok3QisJvduoKUR0SppMdASEc3APcB9ktrInRk0pb6tku4HNgJdwA0R0Q2slbQSeDi1PwIsGfrpmZlZVsr9In9iaGxsjJaWlmPuV7fwBwCcPnEMv/r0nKEelpnZiCZpQ0Q0lqorq08qv/DqgeEegpnZiFVWgWBmZn1zIJiZGeBAMDOzxIFgZmaAA8HMzBIHgpmZAQ4EMzNLHAhmZgY4EMzMLHEgmJkZ4EAwM7PEgWBmZoADwczMEgeCmZkBDgQzM0syBYKkuZI2S2qTtLDI9mpJK9L2tZLq8rYtSu2bJV2W1z5Z0kpJT0raJOnSoZiQmZkNTMlAkFQB3AlcDjQA10pqKCi7DtgdETOB24BbU98Gco/TPA+YC9yV9gfwZeBHEXEu8Hv4mcpmZsMqyxnCbKAtIrZERCewHJhXUDMPWJaWVwJzJCm1L4+IgxHxLNAGzJY0EfhDcs9iJiI6I+KVwU/HzMwGKksgTAO25a23p7aiNRHRBewBavrpezbQAfxvSY9IulvS+AHNwMzMhkSWQFCRtshY01d7JXAR8NWIuBDYB/S6NwEg6XpJLZJaOjo6MgzXzMwGIksgtAMz8tanA9v7qpFUCUwCdvXTtx1oj4i1qX0luYDoJSKWRERjRDTW1tZmGK6ZmQ1ElkBYD8ySVC+pitxN4uaCmmZgQVqeD6yJiEjtTeldSPXALGBdRLwAbJP05tRnDrBxkHMxM7NBqCxVEBFdkm4EVgMVwNKIaJW0GGiJiGZyN4fvk9RG7sygKfVtlXQ/uR/2XcANEdGddv1fgG+kkNkCfGiI52ZmZsegZCAARMQqYFVB2015yweAa/roezNwc5H2R4HGYxmsmZkdP/6kspmZAWUYCAe7uksXmZmVobILhKUPbh3uIZiZjUhlFwg+QzAzK67sAmHbrteHewhmZiNS2QXCtx9uH+4hmJmNSGUXCGZmVpwDwczMAAeCmZklDgQzMwMcCGZmljgQzMwMcCCYmVniQDAzM8CBYGZmiQPBzMyAMg2EJ57fM9xDMDMbcTIFgqS5kjZLapO0sMj2akkr0va1kuryti1K7ZslXVbQr0LSI5K+P9iJHIsP39vyRr6cmdkJoWQgSKoA7gQuBxqAayU1FJRdB+yOiJnAbcCtqW8DuecrnwfMBe5K+zvsY8CmwU7CzMwGL8sZwmygLSK2REQnsByYV1AzD1iWllcCcyQptS+PiIMR8SzQlvaHpOnAlcDdg5/GsYl4o1/RzGzkyxII04Bteevtqa1oTUR0AXuAmhJ9bwc+CfQc86jNzGzIZQkEFWkr/B27r5qi7ZKuAnZGxIaSLy5dL6lFUktHR0fp0WYQvYZvZmZZAqEdmJG3Ph3Y3leNpEpgErCrn75vB94vaSu5S1DvlvT1Yi8eEUsiojEiGmtrazMM18zMBiJLIKwHZkmql1RF7iZxc0FNM7AgLc8H1kREpPam9C6kemAWsC4iFkXE9IioS/tbExEfHIL5mJnZAFWWKoiILkk3AquBCmBpRLRKWgy0REQzcA9wn6Q2cmcGTalvq6T7gY1AF3BDRAz7U+59U9nMrLeSgQAQEauAVQVtN+UtHwCu6aPvzcDN/ez758DPs4zDzMyOn7L8pLJPEMzMeivLQDAzs97KMhB8D8HMrLeyDAQzM+vNgWBmZkDZBoKvGZmZFSrTQDAzs0JlGQi+qWxm1ltZBoKZmfXmQDAzM6BMA8FXjMzMeivLQDAzs97KMhDCd5XNzHopy0AwM7PeyjIQfH5gZtZbWQaCmZn15kAwMzMgYyBImitps6Q2SQuLbK+WtCJtXyupLm/botS+WdJlqW2GpAckbZLUKuljQzWhLHxP2cyst5KBIKkCuBO4HGgArpXUUFB2HbA7ImYCtwG3pr4N5J6vfB4wF7gr7a8L+LuIeAtwCXBDkX2amdkbKMsZwmygLSK2REQnsByYV1AzD1iWllcCcyQptS+PiIMR8SzQBsyOiB0R8TBARLwGbAKmDX46ZmY2UFkCYRqwLW+9nd4/vI/UREQXsAeoydI3XV66EFhb7MUlXS+pRVJLR0dHhuGamdlAZAkEFWkrvArfV02/fSWdAnwb+HhEvFrsxSNiSUQ0RkRjbW1thuGW5g+mmZn1liUQ2oEZeevTge191UiqBCYBu/rrK2k0uTD4RkR8ZyCDNzOzoZMlENYDsyTVS6oid5O4uaCmGViQlucDayL3a3gz0JTehVQPzALWpfsL9wCbIuJ/DMVEzMxscCpLFUREl6QbgdVABbA0IlolLQZaIqKZ3A/3+yS1kTszaEp9WyXdD2wk986iGyKiW9I7gD8DHpf0aHqpT0fEqqGeYNE5vREvYmZ2gikZCADpB/Wqgrab8pYPANf00fdm4OaCtgcpfn/BzMyGSXl+UtmnCGZmvZRnIJiZWS8OBDMzA8o0EHzFyMyst7IMBDMz660sA8GfVDYz660sA8HMzHory0DY19k93EMwMxtxyjIQAB7+ze7hHoKZ2YhStoHwy6deOrL8xR89ycoN7cM4GjOz4ZfpT1ecjG776VNseWkvX266kLt+/gwA8y+ePsyjMjMbPmUbCADfe3Q7M04dN9zDMDMbEcr2ktFhdzzQdmT5C6s28d1H2v22VDMrS2V9hlBoyS+2ANDZ1QPAp779OF+59kK2dOzl9p8+zTUXT+dbG9p5aNG72frSfv766xt48FPvYsKY0cM5bDOzIaET6bfhxsbGaGlpOeZ+dQt/MODXnF03hXVbdx29v5px7OvspuO1gwDc/1eXcv60iRzqDiaNHVg4rHp8B5PHjuZtM6cOeKxmZsVI2hARjaXqfIZQQmEYAGx9ef9R6//pfz302223XAnAMx176eoODhzqZszoCt58+gS2v/I633t0O+86t5ZzT5941D4+8o2Hj+o/EN9c9xu27drPJ+eeS0Twf5/q4I/OqSX3gDozs/5lCgRJc4Evk3ti2t0RcUvB9mrgXuBi4GXgAxGxNW1bBFwHdAMfjYjVWfZ5onrgyZ186Gvre7V/+opz+cKqJwG49UdP8n9ueDvdPT0cPNTD6tYXjtQ1P7ad+prx1E0dx4Qxo/nJxhf58L0tfO1D/4F3vvk0Ort6eHnfQU6fOIbvPbqdKy84g9EVuVtBi77zOAB/994382+PbefjKx7l81efzwcvOQuAA4dyZzUzpvR9I/1Qdw/7O7sHfKZjZieukpeMJFUATwF/DLSTe8bytRGxMa/mI8AFEfHXkpqAP4mID0hqAL4JzAbOBH4KnJO69bvPYobjktFI8sQ/Xsb5n1sNwF+8rY6v/ftWABZdfi5/cuE0Zn/hZ0dq/2DWVH75dO6zFn9/2ZupnVDNP/94My++epC/eFsdEcGyh57jyt89gy83vZWbmlv517W/4T1vOY2fbtrJL/7+XUydUMW4qkqeevE1zqoZR3VlBQBtO19jwpjRvGniGACee3kf46srmXpK9VHj/cGvd9Adwft/78zjfWjMrB9ZLxllCYRLgX+IiMvS+iKAiPhveTWrU81DkiqBF4BaYGF+7eG61K3ffRZT7oFwsplz7mn87Mmdx9zvrJpxjK4YRdvOvb22nTahmp3p3k7+8jtmTmXds7vo7O6hqmIU186ewfY9Bxg7uoIHntzJlFOqeC5dCvzc+xr44RMvsO7ZXfzVH51Nx6sHmV0/hRdfPcjdv9zCawe7+PAf1HPVBWeyv7Objy5/hM+9r4Eft77IFb97Bh9d/giXnF3Dy3sPcs6bJvCTjS/y55eexc7XDjLvrWdSP3U8+zu7ee7l/ezY8zqXnl3Dphde45X9ndTVjKfhzIms2bSTQz25sV52/ukArPr1Dh5rf4XnXt7PLX96AY8/v4f3NJzGj1tfZFxVBb8zZRxnTB7L5hdepSdgTGUFu/Z38tbpk6kePYrvPPw8004dy2kTqpk0djTPdOzlnDdNYMr4Kva8fojHn9/DBdMm0dndw5TxVVRXVtDTE2zf8zrTJo9FErv2dTJlfBXdPUFXTw8R8MhvXmHqKVXMmDKOB59+iXfMmkrFKNHVHfREUF05ip6A0RVix54DVFeOonLUKCaOrUQSe/YfYkzVKF7v7GbP64c4q2Y8Xd09dEcQAS++eoDfmTKOnoBR6epnT0DHaweZPG40+w52MWV8FT3pR1lFKuruCUYJImDUKBERHOzKnQFPGV9FpP13dvdQXfnbN1zmX2KNCCQd+fewnp5g1Cixe18np46vOtLe3RNHXr+zq4etL+9j2b9vZfG88znU3cOWjn2ce/oEeiKorBhFZ1cPVZXH/82eQxkI84G5EfGf0/qfAb8fETfm1TyRatrT+jPA75P74f+riPh6ar8H+GHq1u8+i3EgmFm52vz5uUfO0o9V1kDIEk3F7kgWpkhfNcfa3vvFpesltUhq6ejo6HegfRlXNbCDaGY2Uqjoj82hleWmcjswI299OrC9j5r2dMloErCrRN9S+wQgIpYASyB3hpBhvL1sXDx3IN3MzMpKljOE9cAsSfWSqoAmoLmgphlYkJbnA2sidy2qGWiSVC2pHpgFrMu4TzMzewOVPEOIiC5JNwKryb1FdGlEtEpaDLRERDNwD3CfpDZyZwZNqW+rpPuBjUAXcENEdAMU2+fQT8/MzLIqi08qm5mVs6G8qWxmZmXAgWBmZoADwczMEgeCmZkBDgQzM0tOqHcZSeoAnhtg96nAS0M4nBNNuc8ffAzAx6Bc539WRNSWKjqhAmEwJLVkedvVyarc5w8+BuBjUO7zL8WXjMzMDHAgmJlZUk6BsGS4BzDMyn3+4GMAPgblPv9+lc09BDMz6185nSGYmVk/TvpAkDRX0mZJbZIWDvd4BkvSUkk701PqDrdNkfQTSU+nf09N7ZL0lTT3X0u6KK/PglT/tKQFee0XS3o89fmK8p8bOAJImiHpAUmbJLVK+lhqL6djMEbSOkmPpWPwj6m9XtLaNJ8V6U/Lk/78/Io0n7WS6vL2tSi1b5Z0WV77iP++kVQh6RFJ30/rZTX/4yL3XNGT84vcn9Z+BjgbqAIeAxqGe1yDnNMfAhcBT+S1fRFYmJYXArem5SvIPbJUwCXA2tQ+BdiS/j01LZ+atq0DLk19fghcPtxzLpj/GcBFaXkC8BTQUGbHQMApaXk0sDbN7X6gKbX/C/A3afkjwL+k5SZgRVpuSN8T1UB9+l6pOFG+b4C/Bf4V+H5aL6v5H4+vk/0MYTbQFhFbIqITWA7MG+YxDUpE/ILcMyfyzQOWpeVlwNV57fdGzq+AyZLOAC4DfhIRuyJiN/ATYG7aNjEiHorcd8y9efsaESJiR0Q8nJZfAzYB0yivYxARsTetjk5fAbwbWJnaC4/B4WOzEpiTznrmAcsj4mBEPAu0kfueGfHfN5KmA1cCd6d1UUbzP15O9kCYBmzLW29PbSebN0XEDsj9wAROS+19zb+/9vYi7SNSOvW/kNxvyGV1DNLlkkeBneTC7BnglYjoSiX54z4y17R9D1DDsR+bkeR24JNAT1qvobzmf1yc7IFQ7NpvOb2tqq/5H2v7iCPpFODbwMcj4tX+Sou0nfDHICK6I+Kt5J5HPht4S7Gy9O9JdQwkXQXsjIgN+c1FSk/K+R9PJ3sgtAMz8tanA9uHaSzH04vpUgfp352pva/599c+vUj7iCJpNLkw+EZEfCc1l9UxOCwiXgF+Tu4ewmRJhx+Lmz/uI3NN2yeRu+x4rMdmpHg78H5JW8ldznk3uTOGcpn/8TPcNzGO5xe5Z0ZvIXfD6PDNofOGe1xDMK86jr6p/N85+obqF9PylRx9Q3Vdap8CPEvuZuqpaXlK2rY+1R6+oXrFcM+3YO4id13/9oL2cjoGtcDktDwW+CVwFfAtjr6p+pG0fANH31S9Py2fx9E3VbeQu6F6wnzfAO/ktzeVy27+Q348h3sAb8B/mCvIvRPlGeAzwz2eIZjPN4EdwCFyv8lcR+566M+Ap9O/h3+wCbgzzf1xoDFvP39J7iZaG/ChvPZG4InU5w7ShxdHyhfwDnKn778GHk1fV5TZMbgAeCQdgyeAm1L72eTeIdWWfjhWp/Yxab0tbT87b1+fSfPcTN67qU6U75uCQCi7+Q/1lz+pbGZmwMl/D8HMzDJyIJiZGeBAMDOzxIFgZmaAA8HMzBIHgpmZAQ4EMzNLHAhmZgbA/wfG2xD7NgV59gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(token_neg_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.3898e-05, 5.4593e-05, 3.2289e-03,  ..., 2.6247e-06, 3.9077e-06,\n",
       "         2.6247e-06]),\n",
       " array([3.38975668e-05, 5.45925746e-05, 3.22894152e-03, ...,\n",
       "        2.62474160e-06, 3.90771827e-06, 2.62474160e-06]))"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 5\n",
    "batch_size = 3\n",
    "arr = token_neg_probs # [1, 5, 6, 10]\n",
    "\n",
    "x_numpy = np.array(arr) / np.sum(arr)\n",
    "x_torch = torch.tensor(x_numpy, dtype=torch.float32)\n",
    "x_torch, x_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.  0.  ... 0.  0.  0.2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([   15, 25967,  1177,  5236, 23144])"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_torch = torch.multinomial(x_torch, k, replacement=True) #.view(batch_size, -1)\n",
    "print(np.bincount(sample_torch) / np.bincount(sample_torch).sum())\n",
    "sample_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.  0.2 0.8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_numpy = np.random.multinomial(1, x_numpy, size=k).nonzero()[1]\n",
    "print(np.bincount(sample_numpy) / np.bincount(sample_numpy).sum())\n",
    "sample_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
