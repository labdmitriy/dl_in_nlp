{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Посимвольная языковая модель.\n",
    "\n",
    "В первом задании Вам нужно написать и обучить посимвольную нейронную языковую модель для вычисления вероятностей буквенных последовательностей (то есть слов). Такие модели используются в задачах словоизменения и распознавания/порождения звучащей речи. Для обучения модели используйте данные для русского языка из [репозитория](https://github.com/sigmorphon/conll2018/tree/master/task1/surprise).\n",
    "\n",
    "**В процессе написания Вам нужно решить следующие проблемы:**\n",
    "    \n",
    "* как будет выглядеть обучающая выборка; что будет являться признаками, и что - метками классов.\n",
    "* как сделать так, чтобы модель при предсказании символа учитывала все предыдущие символы слова.\n",
    "* какие специальные символы нужно использовать.\n",
    "* как передавать в модель текущее состояние рекуррентной сети\n",
    "\n",
    "**Результаты:**\n",
    "\n",
    "* предобработчик данных,\n",
    "* генератор обучающих данных (батчей),\n",
    "* обученная модель\n",
    "* перплексия модели на настроечной выборке\n",
    "* посимвольные вероятности слов в контрольной выборке\n",
    "\n",
    "**Дополнительно:**\n",
    "\n",
    "* дополнительный вход модели (часть речи слова, другие морфологические признаки), влияет ли его добавление на перплексию\n",
    "* сравнение различных архитектур нейронной сети (FC, RNN, LSTM, QRNN, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подумайте, какие вспомогательные токены могут быть вам полезны. Выдайте им индексы от `0` до `len(AUXILIARY) - 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**План**\n",
    "- Данные\n",
    "    - Признаки: набор символов токена, заканчивается токеном END\n",
    "    - Метки класса: набор символов того же токена, начинается с токена BEGIN\n",
    "- Для учета всех предыдущих символов, при предсказании следующего символа, дополнительно мы должны передавать на вход предыдущий токен\n",
    "- Специальные символы\n",
    "    - BEGIN, END, MASK, UNK\n",
    "- (???) Как передавать в модель текущее состояние рекуррентной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is better to do all imports at the first cell\n",
    "from pathlib import Path\n",
    "from itertools import islice\n",
    "from operator import itemgetter\n",
    "from functools import partial\n",
    "from argparse import Namespace\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download data\n",
    "# !wget https://github.com/sigmorphon/conll2018/blob/master/task1/surprise/russian-train-high\n",
    "# !wget https://github.com/sigmorphon/conll2018/blob/master/task1/surprise/russian-dev\n",
    "# !wget https://github.com/sigmorphon/conll2018/blob/master/task1/surprise/russian-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('./data')\n",
    "MODELS_PATH = Path('./models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = {'train': DATA_PATH/'russian-train-high',\n",
    "              'dev': DATA_PATH/'russian-dev',\n",
    "              'test': DATA_PATH/'russian-test'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        # Initialize mapping (token -> idx) if empty\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        \n",
    "        # Generate 2 mappings (tokens -> idx, idx -> token)\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        if token in self._token_to_idx:\n",
    "            # get index of token if it is already exists in vocabulary\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            # for new token, append it to mapping with new index\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        \n",
    "        # return index of token\n",
    "        return index\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        # return index by token\n",
    "        return self._token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        # return token by index\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        # override len function to get vocabulary size more easily\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None,\n",
    "                 unk_token='<UNK>',\n",
    "                 mask_token='<MASK>',\n",
    "                 begin_token='<BEGIN>',\n",
    "                 end_token='<END>'):\n",
    "        super().__init__(token_to_idx)\n",
    "        \n",
    "        # Save special token symbols\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_token = begin_token\n",
    "        self._end_token = end_token\n",
    "        \n",
    "        # Get and save indices for special token symbols\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)        \n",
    "        self.begin_index = self.add_token(self._begin_token)        \n",
    "        self.end_index = self.add_token(self._end_token)\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        # Override method to use <UNK> index \n",
    "        # if the token is not in vocabulary\n",
    "        return self._token_to_idx.get(token, self.unk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLMVectorizer:\n",
    "    def __init__(self, char_vocab):\n",
    "        # Save character vocabulary\n",
    "        self.char_vocab = char_vocab\n",
    "        \n",
    "    def vectorize(self, word, vector_length=-1):\n",
    "        # Wrap word with <BEGIN> and <END> tokens\n",
    "        indices = [self.char_vocab.begin_index]\n",
    "        indices.extend(self.char_vocab.lookup_token(token) for token in word)\n",
    "        indices.append(self.char_vocab.end_index)\n",
    "        \n",
    "        \n",
    "#         # If vector_length is provided (eval mode) as argument - use it\n",
    "#         # Otherwise, alculate length for source and target vectors (train mode):\n",
    "#         # length of word + 1 special token (<BEGIN> or <END>)\n",
    "#         # if vector_length == -1:\n",
    "#         #    vector_length = len(word) + 1\n",
    "#         vector_length = len(word) + 1\n",
    "        \n",
    "#         # Create padded version of the source vector\n",
    "#         # <BEGIN> <char1> ... <charN> <MASK> ... <MASK>\n",
    "#         # where N - length of original word\n",
    "#         source_vector = np.full(vector_length, self.char_vocab.mask_index,\n",
    "#                                  dtype=np.int64)\n",
    "#         source_indices = indices[:-1]\n",
    "#         source_vector[:len(source_indices)] = source_indices\n",
    "        \n",
    "#         # Create padded version of the target vector\n",
    "#         # <char1> ... <charN> <END> <MASK> ... <MASK>\n",
    "#         # where N - length of original word\n",
    "#         target_vector = np.full(vector_length, self.char_vocab.mask_index,\n",
    "#                                 dtype=np.int64)\n",
    "#         target_indices = indices[1:]\n",
    "#         target_vector[:len(target_indices)] = target_indices\n",
    "\n",
    "        source_vector = indices[:-1]\n",
    "        target_vector = indices[1:]\n",
    "        \n",
    "        length = len(source_vector)\n",
    "        \n",
    "        # Return padded versions of the source and target vectors\n",
    "        return {'source_vector': source_vector, \n",
    "                'target_vector': target_vector,\n",
    "                'length': length}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, full_df, data_type):\n",
    "        # Create sequence vocabulary\n",
    "        char_vocab = SequenceVocabulary()\n",
    "        \n",
    "        # Get dataframe subset to built vocabulary\n",
    "        target_df = full_df[full_df['data_type'].isin(data_type)]\n",
    "        \n",
    "        # Add tokens to vocabulary from train dataset\n",
    "        for _, row in target_df.iterrows():\n",
    "            for char in row['word']:\n",
    "                char_vocab.add_token(char)\n",
    "            \n",
    "        return cls(char_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLMDataset(Dataset):\n",
    "    def __init__(self, full_df, vectorizer):\n",
    "        # Save original dataset (train/dev/test)\n",
    "        self.full_df = full_df\n",
    "        \n",
    "        # Save vectorizer\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        # Calculate maximum word length (including BOS/EOS tokens) \n",
    "        # across whole dataset for further padding (not efficient)\n",
    "        # self._max_seq_length = max(map(len, self.full_df['word'])) + 1\n",
    "        \n",
    "        # Save train/dev/test datasets separately\n",
    "        # and save its sizes (number of rows)\n",
    "        self.train_df = self.full_df[self.full_df['data_type'] == 'train']\n",
    "        self.train_size = len(self.train_df)\n",
    "        \n",
    "        self.dev_df = self.full_df[self.full_df['data_type'] == 'dev']\n",
    "        self.dev_size = len(self.dev_df)\n",
    "        \n",
    "        self.test_df = self.full_df[self.full_df['data_type'] == 'test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        # Store information about datasets in dictionary\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'dev': (self.dev_df, self.dev_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "        \n",
    "        # Set train data as default\n",
    "        self.set_data_type('train')\n",
    "    \n",
    "    @classmethod\n",
    "    def read_dataset(cls, file_path, data_type):\n",
    "        # Read specific file and save its data type (train/dev/test)\n",
    "        df = pd.read_csv(file_path, sep='\\t', \n",
    "                         header=None, names=['word'], \n",
    "                         usecols=[0])\n",
    "        df['data_type'] = data_type\n",
    "        \n",
    "        # Return dataframe with data and its type\n",
    "        return df\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset(cls, files_path):\n",
    "        dfs_list = []\n",
    "        \n",
    "        # Read all datasets specified in files_path\n",
    "        for data_type, file_path in file_paths.items():\n",
    "            df = cls.read_dataset(file_path, data_type)\n",
    "            dfs_list.append(df)\n",
    "        \n",
    "        # Concatenate all datasets\n",
    "        full_df = pd.concat(dfs_list, axis=0, ignore_index=True)\n",
    "        \n",
    "        # Return concatenated dataframe with specified data types\n",
    "        return full_df\n",
    "    \n",
    "    @classmethod\n",
    "    def make_vectorizer(cls, files_path):\n",
    "        # Load all data from files specified in files_path\n",
    "        full_df = cls.load_dataset(files_path)\n",
    "        \n",
    "        # Create CharLMDataset class using full dataset and vectorizer\n",
    "        return cls(full_df, CharLMVectorizer.from_dataframe(full_df, \n",
    "                                                            data_type=['train']))\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        # Return vectorizer related to Dataset\n",
    "        return self._vectorizer\n",
    "    \n",
    "    def set_data_type(self, data_type='train'):\n",
    "        # Set type, data, and its size as current dataset\n",
    "        self._target_type = data_type\n",
    "        self._target_df, self._target_size = self._lookup_dict[data_type] \n",
    "        \n",
    "    def __len__(self):\n",
    "        # Return length of the current dataset\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Get example by index from the current dataset\n",
    "        row = self._target_df.iloc[index]\n",
    "        \n",
    "        # Generate source and target vector, and its length from the example\n",
    "        # with padding to maximum word size (including BOS/EOS tokens)\n",
    "        vector_dict = self._vectorizer.vectorize(row['word'])#, self._max_seq_length)\n",
    "        \n",
    "        # Return generated vectors with its length\n",
    "        return vector_dict\n",
    "    \n",
    "    def get_num_batches(self, batch_size):\n",
    "        # Calculate the number of full batches\n",
    "        # for tracking progress in tqdm\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "train    10000\n",
       "test      1000\n",
       "dev       1000\n",
       "Name: data_type, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset = CharLMDataset.make_vectorizer(file_paths)\n",
    "print(lm_dataset._target_type)\n",
    "lm_dataset.full_df['data_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<MASK>': 0, '<UNK>': 1, '<BEGIN>': 2, '<END>': 3, 'в': 4, 'а': 5, 'л': 6, 'о': 7, 'н': 8, 'с': 9, 'к': 10, 'и': 11, 'й': 12, 'е': 13, 'з': 14, 'ч': 15, 'ы': 16, 'т': 17, 'р': 18, 'ё': 19, 'п': 20, 'ь': 21, 'г': 22, 'б': 23, 'ю': 24, 'я': 25, 'д': 26, 'у': 27, 'ш': 28, 'м': 29, 'х': 30, 'ж': 31, 'ц': 32, ' ': 33, 'щ': 34, '-': 35, 'ф': 36, 'э': 37, 'ъ': 38, 'С': 39, 'Ш': 40, 'И': 41, 'З': 42, 'А': 43, 'Г': 44, 'Э': 45, 'Л': 46, 'Ф': 47, 'В': 48, 'П': 49, 'М': 50, 'Р': 51, 'Б': 52, 'Х': 53, 'Н': 54, 'Е': 55}\n",
      "56\n",
      "{0: '<MASK>', 1: '<UNK>', 2: '<BEGIN>', 3: '<END>', 4: 'в', 5: 'а', 6: 'л', 7: 'о', 8: 'н', 9: 'с', 10: 'к', 11: 'и', 12: 'й', 13: 'е', 14: 'з', 15: 'ч', 16: 'ы', 17: 'т', 18: 'р', 19: 'ё', 20: 'п', 21: 'ь', 22: 'г', 23: 'б', 24: 'ю', 25: 'я', 26: 'д', 27: 'у', 28: 'ш', 29: 'м', 30: 'х', 31: 'ж', 32: 'ц', 33: ' ', 34: 'щ', 35: '-', 36: 'ф', 37: 'э', 38: 'ъ', 39: 'С', 40: 'Ш', 41: 'И', 42: 'З', 43: 'А', 44: 'Г', 45: 'Э', 46: 'Л', 47: 'Ф', 48: 'В', 49: 'П', 50: 'М', 51: 'Р', 52: 'Б', 53: 'Х', 54: 'Н', 55: 'Е'}\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "print(lm_dataset.get_vectorizer().char_vocab._token_to_idx)\n",
    "print(len(lm_dataset.get_vectorizer().char_vocab._token_to_idx))\n",
    "print(lm_dataset.get_vectorizer().char_vocab._idx_to_token)\n",
    "print(len(lm_dataset.get_vectorizer().char_vocab._idx_to_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source_vector': [2, 8, 13, 14, 5, 10, 7, 8, 15, 13, 8, 8, 16, 12],\n",
       " 'target_vector': [8, 13, 14, 5, 10, 7, 8, 15, 13, 8, 8, 16, 12, 3],\n",
       " 'length': 14}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source_vector': [2, 54, 13, 1, 18, 1, 9, 55, 17, 1],\n",
       " 'target_vector': [54, 13, 1, 18, 1, 9, 55, 17, 1, 3],\n",
       " 'length': 10}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset.get_vectorizer().vectorize('НеЙрОсЕтЬ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if isinstance(batch[0], torch.Tensor):\n",
    "        out = None\n",
    "        if _use_shared_memory:\n",
    "            # If we're in a background process, concatenate directly into a\n",
    "            # shared memory tensor to avoid an extra copy\n",
    "            numel = sum([x.numel() for x in batch])\n",
    "            storage = batch[0].storage()._new_shared(numel)\n",
    "            out = batch[0].new(storage)\n",
    "        return torch.stack(batch, 0, out=out)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for batch in islice(generate_batches(lm_dataset, 2, shuffle=True), 1):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sort_idx = np.argsort(data_len)[::-1]\n",
    "padded_data[sort_idx]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "unsort_idx = np.argsort(sort_idx)\n",
    "padded_data[sort_idx][unsort_idx]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = [{'source_vector': np.array([ 2,  4,  5,  6,  6,  7,  8,  9, 10, 11, 12]), \n",
    "       'target_vector': np.array([ 4,  5,  6,  6,  7,  8,  9, 10, 11, 12,  3]), 'length': 11}, \n",
    "      {'source_vector': np.array([ 2,  8, 13, 14,  5, 10,  7,  8, 15, 13,  8,  8, 16, 12]), \n",
    "       'target_vector': np.array([ 8, 13, 14,  5, 10,  7,  8, 15, 13,  8,  8, 16, 12,  3]), 'length': 14}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(elem, item_name, max_length, value=0):\n",
    "    data = elem[item_name]\n",
    "    data_len = elem['length']\n",
    "    data = np.pad(data, (0, max_length - data_len), \n",
    "                  mode='constant', constant_values=value)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [x for x in range(3)]\n",
    "torch.tensor(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    get_length_item = itemgetter('length')\n",
    "    \n",
    "    batch_lengths = torch.tensor(list(map(get_length_item, batch)))\n",
    "    max_batch_length = torch.max(batch_lengths)\n",
    "    \n",
    "    padded_source_batch = partial(pad_sequence, item_name='source_vector', \n",
    "                                  max_length=max_batch_length, value=0)\n",
    "    padded_source_batch = list(map(padded_source_batch, batch))\n",
    "    padded_source_batch = np.vstack(padded_source_batch)\n",
    "    padded_source_batch = torch.from_numpy(padded_source_batch)\n",
    "    \n",
    "    padded_target_batch = partial(pad_sequence, item_name='target_vector', \n",
    "                                  max_length=max_batch_length, value=0)\n",
    "    padded_target_batch = list(map(padded_target_batch, batch))\n",
    "    padded_target_batch = np.vstack(padded_target_batch)\n",
    "    padded_target_batch = torch.from_numpy(padded_target_batch)\n",
    "    \n",
    "    return {'source_batch': padded_source_batch, \n",
    "            'target_batch': padded_target_batch,\n",
    "            'batch_lengths': batch_lengths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, collate_fn,\n",
    "                     shuffle=True, drop_last=True,\n",
    "                     device='cpu'):\n",
    "    data_loader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                             shuffle=shuffle, drop_last=drop_last,\n",
    "                             collate_fn=collate_fn)\n",
    "    \n",
    "    for data_dict in data_loader:\n",
    "        lengths = data_dict['batch_lengths'].numpy()\n",
    "        sort_idx = lengths.argsort()[::-1].tolist()\n",
    "        \n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name][sort_idx].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source_batch': tensor([[ 2,  8, 13, 14,  5, 10,  7,  8, 15, 13,  8,  8, 16, 12],\n",
      "        [ 2, 11,  9, 17, 18, 19, 20, 16,  4,  5, 17, 21,  0,  0],\n",
      "        [ 2,  4,  5,  6,  6,  7,  8,  9, 10, 11, 12,  0,  0,  0]]), 'target_batch': tensor([[ 8, 13, 14,  5, 10,  7,  8, 15, 13,  8,  8, 16, 12,  3],\n",
      "        [11,  9, 17, 18, 19, 20, 16,  4,  5, 17, 21,  3,  0,  0],\n",
      "        [ 4,  5,  6,  6,  7,  8,  9, 10, 11, 12,  3,  0,  0,  0]]), 'batch_lengths': tensor([14, 12, 11])}\n",
      "tensor([[ 2,  8, 13, 14,  5, 10,  7,  8, 15, 13,  8,  8, 16, 12],\n",
      "        [ 2, 11,  9, 17, 18, 19, 20, 16,  4,  5, 17, 21,  0,  0],\n",
      "        [ 2,  4,  5,  6,  6,  7,  8,  9, 10, 11, 12,  0,  0,  0]]) tensor([14, 12, 11])\n"
     ]
    }
   ],
   "source": [
    "for batch in islice(generate_batches(lm_dataset, batch_size=3, \n",
    "                                     shuffle=False, collate_fn=collate_fn), 1):\n",
    "    print(batch)\n",
    "    x_source = batch['source_batch']\n",
    "    lengths = batch['batch_lengths']\n",
    "    print(x_source, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLMModel(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_size,\n",
    "                 hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_size, \n",
    "                                      padding_idx=0)\n",
    "        self.rnn = nn.GRU(embedding_size, hidden_size, \n",
    "                          bidirectional=False, batch_first=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=hidden_size,\n",
    "                             out_features=num_classes)\n",
    "        \n",
    "    def forward(self, x_source, x_lengths, apply_softmax=False):\n",
    "        x_embedded = self.embedding(x_source)\n",
    "        x_packed = pack_padded_sequence(x_embedded, x_lengths.detach().cpu().numpy(),\n",
    "                                        batch_first=True)\n",
    "        x_rnn_out, x_rnn_h = self.rnn(x_packed)\n",
    "#         x_rnn_h = x_rnn_h.permute(1, 0, 2)\n",
    "#         x_rnn_h = x_rnn_h.reshape(x_rnn_h.shape[0], -1)\n",
    "        x_unpacked, _ = pad_packed_sequence(x_rnn_out, batch_first=True)\n",
    "        y_out = self.fc1(x_unpacked)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=2)\n",
    "        \n",
    "        return y_out # x_unpacked #, x_rnn_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = lm_dataset.get_vectorizer()\n",
    "vocab_size = len(vectorizer.char_vocab)\n",
    "\n",
    "model = CharLMModel(num_embeddings=vocab_size,\n",
    "                    embedding_size=20,\n",
    "                    hidden_size=10,\n",
    "                    num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 14, 56])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_out = model(x_source, lengths)\n",
    "y_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting all possible random states to fixed number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create namespace with all parameters for training (specified values were used for the final model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    file_paths = {'train': DATA_PATH/'russian-train-high',\n",
    "                  'dev': DATA_PATH/'russian-dev',\n",
    "                  'test': DATA_PATH/'russian-test'},\n",
    "    model_state_path = MODELS_PATH/'charLMModel.pth',\n",
    "    \n",
    "    embedding_size = 50,\n",
    "    hidden_size = 10,\n",
    "    \n",
    "    seed = 42,\n",
    "    \n",
    "    num_epochs = 3,\n",
    "    batch_size = 1024,\n",
    "    learning_rate = 0.03,\n",
    "    save_iterations = 1e8,\n",
    "    early_stopping_criteria = 1e8,\n",
    "    factor=1e8,\n",
    "    patience=1e8,\n",
    "    \n",
    "    cuda=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create functions for creating and updating necessary parameters while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': [], \n",
    "            'epoch_idx': 0,\n",
    "            'batch_idx': 0,\n",
    "            'loss': [],\n",
    "            'model_file_name': args.model_state_path}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    if train_state['batch_idx'] == 0:\n",
    "        train_state['stop_early'] = False\n",
    "        torch.save(model.state_dict(), train_state['model_file_name'])\n",
    "    else:\n",
    "        loss = train_state['loss'][-1]\n",
    "\n",
    "        if loss < train_state['early_stopping_best_val']:\n",
    "            train_state['early_stopping_best_val'] = loss\n",
    "            train_state['early_stopping_step'] = 0\n",
    "            \n",
    "            if train_state['batch_idx'] % args.save_iterations == 0:\n",
    "                torch.save(model.state_dict(), train_state['model_file_name'])\n",
    "        else:\n",
    "            train_state['early_stopping_step'] += 1 \n",
    "    \n",
    "        train_state['stop_early'] = train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if we can use GPU or CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    args.cuda=False\n",
    "    \n",
    "print(f'Using CUDA: {args.cuda}')\n",
    "args.device = torch.device('cuda' if args.cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8c5584fa2941f48a1e5447fc9e510b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epochs', max=3, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d613e5a17d43f68cc7b8cb2b482198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train data', max=9, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f99d8bc5a548588088aa06615b02c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Dev data', max=1, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.1607, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7125, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4096, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1547, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9171, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6945, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4886, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3034, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1382, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9875, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8530, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7300, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6163, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5115, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4154, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3254, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2427, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1640, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0937, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0260, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9648, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9075, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8568, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8102, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7660, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7249, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6861, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "set_seeds(args.seed)\n",
    "\n",
    "lm_dataset = CharLMDataset.make_vectorizer(args.file_paths)\n",
    "\n",
    "vectorizer = lm_dataset.get_vectorizer()\n",
    "mask_index = vectorizer.char_vocab.mask_index\n",
    "vocab_size = len(vectorizer.char_vocab)\n",
    "\n",
    "model = CharLMModel(num_embeddings=vocab_size,\n",
    "                    embedding_size=args.embedding_size,\n",
    "                    hidden_size=args.hidden_size,\n",
    "                    num_classes=vocab_size)\n",
    "model = model.to(args.device)\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(),\n",
    "                      lr=args.learning_rate)\n",
    "\n",
    "sequence_loss = nn.CrossEntropyLoss(ignore_index=mask_index)\n",
    "\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "#                                                  mode='min',\n",
    "#                                                  factor=args.factor,\n",
    "#                                                  patience=args.patience)\n",
    "\n",
    "epoch_bar = tqdm_notebook(desc='Epochs', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "lm_dataset.set_data_type('train')\n",
    "train_bar = tqdm_notebook(desc='Train data',\n",
    "                          total=lm_dataset.get_num_batches(args.batch_size), \n",
    "                          position=1)\n",
    "\n",
    "lm_dataset.set_data_type('dev')\n",
    "val_bar = tqdm_notebook(desc='Dev data',\n",
    "                        total=lm_dataset.get_num_batches(args.batch_size), \n",
    "                        position=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "writer = SummaryWriter(log_dir='logs', comment='task_1')\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(1, args.num_epochs + 1):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "        \n",
    "        lm_dataset.set_data_type('train')\n",
    "        batch_generator = generate_batches(dataset=lm_dataset, \n",
    "                                           batch_size=args.batch_size,\n",
    "                                           collate_fn=collate_fn,\n",
    "                                           shuffle=False,\n",
    "                                           drop_last=True,\n",
    "                                           device=args.device)\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        for batch_idx, batch_dict in enumerate(batch_generator, 1):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(batch['source_batch'], \n",
    "                           batch['batch_lengths'])\n",
    "            y_pred = y_pred.reshape(-1, y_pred.shape[2])\n",
    "            \n",
    "            y_true = batch['target_batch']\n",
    "            y_true = y_true.reshape(-1)\n",
    "            \n",
    "            loss = sequence_loss(y_pred, y_true)\n",
    "\n",
    "#             loss = loss / args.batch_size\n",
    "            loss_value = loss.item()\n",
    "            running_loss += (loss_value - running_loss) / (batch_idx)\n",
    "            loss.backward()\n",
    "            \n",
    "            print(loss)\n",
    "\n",
    "            learning_rate = optimizer.param_groups[0]['lr']\n",
    "\n",
    "            train_state['batch_idx'] = batch_idx\n",
    "            train_state['loss'].append(running_loss)\n",
    "            train_state['learning_rate'].append(learning_rate)\n",
    "\n",
    "            train_state = update_train_state(args=args,\n",
    "                                             model=model,\n",
    "                                             train_state=train_state)\n",
    "\n",
    "            train_params = dict(loss=running_loss,\n",
    "                                lr=learning_rate,\n",
    "                                early_step=train_state['early_stopping_step'],\n",
    "                                early_best=train_state['early_stopping_best_val'])\n",
    "            train_bar.set_postfix(train_params)\n",
    "            train_bar.update()\n",
    "\n",
    "            optimizer.step()\n",
    "#             scheduler.step(train_state['loss'][-1])\n",
    "            \n",
    "#             iter_idx += 1\n",
    "\n",
    "            if train_state['stop_early']:\n",
    "                break\n",
    "        \n",
    "        train_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "        \n",
    "#     torch.save(classifier.state_dict(), str(train_state['model_file_name']) + '_last')\n",
    "except KeyboardInterrupt:\n",
    "    print('Exit training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
