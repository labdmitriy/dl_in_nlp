{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Посимвольная языковая модель.\n",
    "\n",
    "В первом задании Вам нужно написать и обучить посимвольную нейронную языковую модель для вычисления вероятностей буквенных последовательностей (то есть слов). Такие модели используются в задачах словоизменения и распознавания/порождения звучащей речи. Для обучения модели используйте данные для русского языка из [репозитория](https://github.com/sigmorphon/conll2018/tree/master/task1/surprise).\n",
    "\n",
    "**В процессе написания Вам нужно решить следующие проблемы:**\n",
    "    \n",
    "* как будет выглядеть обучающая выборка; что будет являться признаками, и что - метками классов.\n",
    "* как сделать так, чтобы модель при предсказании символа учитывала все предыдущие символы слова.\n",
    "* какие специальные символы нужно использовать.\n",
    "* как передавать в модель текущее состояние рекуррентной сети\n",
    "\n",
    "**Результаты:**\n",
    "\n",
    "* предобработчик данных,\n",
    "* генератор обучающих данных (батчей),\n",
    "* обученная модель\n",
    "* перплексия модели на настроечной выборке\n",
    "* посимвольные вероятности слов в контрольной выборке\n",
    "\n",
    "**Дополнительно:**\n",
    "\n",
    "* дополнительный вход модели (часть речи слова, другие морфологические признаки), влияет ли его добавление на перплексию\n",
    "* сравнение различных архитектур нейронной сети (FC, RNN, LSTM, QRNN, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подумайте, какие вспомогательные токены могут быть вам полезны. Выдайте им индексы от `0` до `len(AUXILIARY) - 1`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is better to do all imports at the first cell\n",
    "from pathlib import Path\n",
    "from itertools import islice\n",
    "from operator import itemgetter\n",
    "from functools import partial\n",
    "from argparse import Namespace\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Uncomment to download data\n",
    "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-train-high\n",
    "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-dev\n",
    "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify base paths for data an model folders\n",
    "DATA_PATH = Path('./data')\n",
    "MODELS_PATH = Path('./models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary for storing char vocabulary\n",
    "# with required operations: \n",
    "# add token, lookup index by token, lookup token by index\n",
    "class Vocabulary:\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        # Initialize mapping (token -> idx) if empty\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        \n",
    "        # Generate 2 mappings (tokens -> idx, idx -> token)\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        if token in self._token_to_idx:\n",
    "            # get index of token if it is already exists in vocabulary\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            # for new token, append it to mapping with new index\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        \n",
    "        # return index of token\n",
    "        return index\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        # return index by token\n",
    "        return self._token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        # return token by index\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        # override len function to get vocabulary size more easily\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None,\n",
    "                 unk_token='<UNK>',\n",
    "                 mask_token='<MASK>',\n",
    "                 begin_token='<BEGIN>',\n",
    "                 end_token='<END>'):\n",
    "        super().__init__(token_to_idx)\n",
    "        \n",
    "        # Save special token symbols\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_token = begin_token\n",
    "        self._end_token = end_token\n",
    "        \n",
    "        # Get and save indices for special token symbols\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)        \n",
    "        self.begin_index = self.add_token(self._begin_token)        \n",
    "        self.end_index = self.add_token(self._end_token)\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        # Override method to use <UNK> index \n",
    "        # if the token is not in vocabulary\n",
    "        return self._token_to_idx.get(token, self.unk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorizer for transforming the word \n",
    "# to source/target vectors and saving its lengths\n",
    "class CharLMVectorizer:\n",
    "    def __init__(self, char_vocab):\n",
    "        # Save character vocabulary\n",
    "        self.char_vocab = char_vocab\n",
    "        \n",
    "    def vectorize(self, word):\n",
    "        # Wrap word with <BEGIN> and <END> tokens\n",
    "        indices = [self.char_vocab.begin_index]\n",
    "        indices.extend(self.char_vocab.lookup_token(token) for token in word)\n",
    "        indices.append(self.char_vocab.end_index)\n",
    "        \n",
    "        # Create source vector\n",
    "        # <BEGIN> <char1> ... <charN>\n",
    "        # where N - length of original word\n",
    "        source_vector = indices[:-1]\n",
    "        \n",
    "        # Create target vector\n",
    "        # <char1> ... <charN> <END> \n",
    "        # where N - length of original word\n",
    "        target_vector = indices[1:]\n",
    "        \n",
    "        # Calculate length of both created vectors\n",
    "        length = len(source_vector)\n",
    "        \n",
    "        # Return ource and target vectors with its length\n",
    "        return {'source_vector': source_vector, \n",
    "                'target_vector': target_vector,\n",
    "                'length': length}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, full_df, data_type):\n",
    "        # Create sequence vocabulary\n",
    "        char_vocab = SequenceVocabulary()\n",
    "        \n",
    "        # Get dataframe subset to built vocabulary\n",
    "        target_df = full_df[full_df['data_type'].isin(data_type)]\n",
    "        \n",
    "        # Add tokens to vocabulary from dataset\n",
    "        for _, row in target_df.iterrows():\n",
    "            for char in row['word']:\n",
    "                char_vocab.add_token(char)\n",
    "            \n",
    "        return cls(char_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for char language model\n",
    "class CharLMDataset(Dataset):\n",
    "    def __init__(self, full_df, vectorizer):\n",
    "        # Save original dataset (train/val/test)\n",
    "        self.full_df = full_df\n",
    "        \n",
    "        # Save vectorizer\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        # Save train/val/test datasets separately\n",
    "        # and save its sizes (number of rows)\n",
    "        self.train_df = self.full_df[self.full_df['data_type'] == 'train']\n",
    "        self.train_size = len(self.train_df)\n",
    "        \n",
    "        self.val_df = self.full_df[self.full_df['data_type'] == 'val']\n",
    "        self.val_size = len(self.val_df)\n",
    "        \n",
    "        self.test_df = self.full_df[self.full_df['data_type'] == 'test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        # Store information about datasets in dictionary\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.val_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "        \n",
    "        # Set train data as default\n",
    "        self.set_data_type('train')\n",
    "    \n",
    "    @classmethod\n",
    "    def read_dataset(cls, file_path, data_type):\n",
    "        # Read specific file and save its data type (train/dev/test)\n",
    "        df = pd.read_csv(file_path, sep='\\t', \n",
    "                         header=None, names=['word'], \n",
    "                         usecols=[0])\n",
    "        df['data_type'] = data_type\n",
    "        \n",
    "        # Return dataframe with data and its type\n",
    "        return df\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset(cls, file_paths):\n",
    "        dfs_list = []\n",
    "        \n",
    "        # Read all datasets specified in files_path\n",
    "        for data_type, file_path in file_paths.items():\n",
    "            df = cls.read_dataset(file_path, data_type)\n",
    "            dfs_list.append(df)\n",
    "        \n",
    "        # Concatenate all datasets\n",
    "        full_df = pd.concat(dfs_list, axis=0, ignore_index=True)\n",
    "        \n",
    "        # Return concatenated dataframe with specified data types\n",
    "        return full_df\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file_paths(cls, file_paths):\n",
    "        # Load all data from files specified in files_path\n",
    "        full_df = cls.load_dataset(file_paths)\n",
    "        \n",
    "        # Create CharLMDataset class using full dataset and vectorizer\n",
    "        return cls(full_df, CharLMVectorizer.from_dataframe(full_df, \n",
    "                                                            data_type=['train']))\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        # Return vectorizer related to Dataset\n",
    "        return self._vectorizer\n",
    "    \n",
    "    def set_data_type(self, data_type='train'):\n",
    "        # Set type, data, and its size as current dataset\n",
    "        self._target_type = data_type\n",
    "        self._target_df, self._target_size = self._lookup_dict[data_type] \n",
    "        \n",
    "    def __len__(self):\n",
    "        # Return length of the current dataset\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Get example by index from the current dataset\n",
    "        row = self._target_df.iloc[index]\n",
    "        \n",
    "        # Vectorize example (generate source/target vector and its length)\n",
    "        vector_dict = self._vectorizer.vectorize(row['word'])\n",
    "        \n",
    "        # Return generated vectors with its length\n",
    "        return vector_dict\n",
    "    \n",
    "    def get_num_batches(self, batch_size):\n",
    "        # Calculate the number of full batches\n",
    "        # for tracking progress in tqdm\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad batch element to specified max length\n",
    "def pad_sequence(elem, item_name, max_length, value=0):\n",
    "    \n",
    "    data = elem[item_name]\n",
    "    data_len = elem['length']\n",
    "    data = np.pad(data, (0, max_length - data_len), \n",
    "                  mode='constant', constant_values=value)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine padded source/target vectors and its lengths in batch for DataLoader\n",
    "def collate_fn(batch):\n",
    "    # Get length of batch elements\n",
    "    get_length_item = itemgetter('length')\n",
    "    batch_lengths = torch.tensor(list(map(get_length_item, batch)))\n",
    "    \n",
    "    # Find max length of element in batch\n",
    "    max_batch_length = torch.max(batch_lengths)\n",
    "    \n",
    "    # Pad source vectors with <MASK> token\n",
    "    padded_source_batch = partial(pad_sequence, item_name='source_vector', \n",
    "                                  max_length=max_batch_length, value=0)\n",
    "    padded_source_batch = list(map(padded_source_batch, batch))\n",
    "    padded_source_batch = np.vstack(padded_source_batch)\n",
    "    padded_source_batch = torch.from_numpy(padded_source_batch)\n",
    "    \n",
    "    # Pad target vectors with <MASK> token\n",
    "    padded_target_batch = partial(pad_sequence, item_name='target_vector', \n",
    "                                  max_length=max_batch_length, value=0)\n",
    "    padded_target_batch = list(map(padded_target_batch, batch))\n",
    "    padded_target_batch = np.vstack(padded_target_batch)\n",
    "    padded_target_batch = torch.from_numpy(padded_target_batch)\n",
    "    \n",
    "    # Return dictionary with source/target vectors and its lengths\n",
    "    return {'source_batch': padded_source_batch, \n",
    "            'target_batch': padded_target_batch,\n",
    "            'batch_lengths': batch_lengths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate batches with padding within specific batch\n",
    "def generate_batches(dataset, batch_size, collate_fn,\n",
    "                     shuffle=True, drop_last=True,\n",
    "                     device='cpu'):\n",
    "    # Create DataLoader from dataset with additional parameters\n",
    "    # Use collate_fn to pad sequences in batches \n",
    "    data_loader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                             shuffle=shuffle, drop_last=drop_last,\n",
    "                             collate_fn=collate_fn)\n",
    "    \n",
    "    for data_dict in data_loader:\n",
    "        # Find indices for sorting of batch elements\n",
    "        # in decreasing order\n",
    "        lengths = data_dict['batch_lengths'].numpy()\n",
    "        sort_idx = lengths.argsort()[::-1].tolist()\n",
    "        \n",
    "        # Sort batch in decreasing order and yield it\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name][sort_idx].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create character model\n",
    "class CharLMModel(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_size,\n",
    "                 hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create embedding with zero vectors for <MASK> token \n",
    "        # for ignoring it while backprop\n",
    "        self.embedding = nn.utils.weight_norm(nn.Embedding(num_embeddings, embedding_size, \n",
    "                                              padding_idx=0), dim=1)\n",
    "        \n",
    "        # Use unidirectional 1-layer GRU\n",
    "        # For input and output, consider batch dimension at dim 0        \n",
    "        self.rnn = nn.GRU(embedding_size, hidden_size, \n",
    "                          bidirectional=False, batch_first=True)\n",
    "        \n",
    "        # Linear layer for prediction\n",
    "        self.fc1 = nn.Linear(in_features=hidden_size,\n",
    "                             out_features=num_classes)\n",
    "        \n",
    "    def forward(self, x_source, x_lengths):\n",
    "        # Get embedding for source vectors\n",
    "        x_embedded = self.embedding(x_source)\n",
    "        \n",
    "        # Pack sequences for RNN\n",
    "        x_packed = pack_padded_sequence(x_embedded, x_lengths.detach().cpu().numpy(),\n",
    "                                        batch_first=True)\n",
    "        \n",
    "        # Forward sequences through RNN \n",
    "        x_rnn_out, x_rnn_h = self.rnn(x_packed)\n",
    "        \n",
    "        # Unpack sequences\n",
    "        x_unpacked, _ = pad_packed_sequence(x_rnn_out, batch_first=True)\n",
    "        \n",
    "        # Transform sequences to vocabulary size dimension\n",
    "        y_out = self.fc1(x_unpacked)\n",
    "        \n",
    "        # Return scores\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting all possible random states to fixed number\n",
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create namespace with all parameters for training\n",
    "args = Namespace(\n",
    "    # File paths for train/val/test dataset\n",
    "    file_paths = {'train': DATA_PATH/'russian-train-high',\n",
    "                  'val': DATA_PATH/'russian-dev',\n",
    "                  'test': DATA_PATH/'russian-test'},\n",
    "    # File name for model saving\n",
    "    model_state_path = MODELS_PATH/'charLMModel.pth',\n",
    "    \n",
    "    # NN hyperparameters\n",
    "    embedding_size = 100,\n",
    "    hidden_size = 100,\n",
    "    \n",
    "    # Random seed\n",
    "    seed = 42,\n",
    "    \n",
    "    # Learning hyperparameters\n",
    "    num_epochs = 100,\n",
    "    batch_size = 100,\n",
    "    learning_rate = 0.01,\n",
    "    save_iterations = 1e8,\n",
    "    early_stopping_criteria = 3,\n",
    "    factor=0.5,\n",
    "    patience=1,\n",
    "    clip_norm=5,\n",
    "    \n",
    "    # Flag whether to use GPU (if available)\n",
    "    cuda=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functions for creating and updating necessary parameters while training\n",
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': [], \n",
    "            'epoch_idx': 0,\n",
    "            'train_loss': [],\n",
    "            'train_perplexity': [],\n",
    "            'val_loss': [],\n",
    "            'val_perplexity': [],\n",
    "            'test_loss': -1,\n",
    "            'test_perplexity': -1,\n",
    "            'model_file_name': args.model_state_path}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    if train_state['epoch_idx'] == 0:\n",
    "        train_state['stop_early'] = False\n",
    "        torch.save(model.state_dict(), train_state['model_file_name'])\n",
    "    else:\n",
    "        loss = train_state['val_loss'][-1]\n",
    "\n",
    "        if loss < train_state['early_stopping_best_val']:\n",
    "            train_state['early_stopping_best_val'] = loss\n",
    "            train_state['early_stopping_step'] = 0\n",
    "            \n",
    "            if train_state['batch_idx'] % args.save_iterations == 0:\n",
    "                torch.save(model.state_dict(), train_state['model_file_name'])\n",
    "        else:\n",
    "            train_state['early_stopping_step'] += 1 \n",
    "    \n",
    "        train_state['stop_early'] = (train_state['early_stopping_step'] >= args.early_stopping_criteria)\n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "# Check if we can use GPU or CPU\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda=False\n",
    "    \n",
    "print(f'Using CUDA: {args.cuda}')\n",
    "args.device = torch.device('cuda' if args.cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d021bc7804848749759944a73ac6c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epochs', style=ProgressStyle(description_width='initial')), H…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793a9cf963cd421da2c6856ef720f28a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train data', style=ProgressStyle(description_width='initial')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9966e85d60064831b6b1adcfc22f84f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Validation data', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exit training\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "set_seeds(args.seed)\n",
    "\n",
    "# Create dataset from train/val/test file paths\n",
    "lm_dataset = CharLMDataset.from_file_paths(args.file_paths)\n",
    "\n",
    "# Get vectorizer\n",
    "vectorizer = lm_dataset.get_vectorizer()\n",
    "\n",
    "# Get mask index for training process\n",
    "mask_index = vectorizer.char_vocab.mask_index\n",
    "\n",
    "# Get vocabulary size\n",
    "vocab_size = len(vectorizer.char_vocab)\n",
    "\n",
    "# Create language model and set device\n",
    "model = CharLMModel(num_embeddings=vocab_size,\n",
    "                    embedding_size=args.embedding_size,\n",
    "                    hidden_size=args.hidden_size,\n",
    "                    num_classes=vocab_size)\n",
    "model = model.to(args.device)\n",
    "\n",
    "# Create optimizer & scheduler\n",
    "optimizer = optim.Adam(params=model.parameters(),\n",
    "                      lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', \n",
    "                                                 factor=args.factor,\n",
    "                                                 patience=args.patience)\n",
    "\n",
    "# Create tqdm progress bars\n",
    "epoch_bar = tqdm_notebook(desc='Epochs', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "lm_dataset.set_data_type('train')\n",
    "train_bar = tqdm_notebook(desc='Train data',\n",
    "                          total=lm_dataset.get_num_batches(args.batch_size), \n",
    "                          position=0)\n",
    "\n",
    "lm_dataset.set_data_type('val')\n",
    "val_bar = tqdm_notebook(desc='Validation data',\n",
    "                        total=lm_dataset.get_num_batches(args.batch_size), \n",
    "                        position=0)\n",
    "\n",
    "# Create dictionary with training state\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "try:\n",
    "    # Epochs loop\n",
    "    for epoch_index in range(1, args.num_epochs + 1):\n",
    "        # Save epoch index\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "        \n",
    "        # Create generator based on train data\n",
    "        lm_dataset.set_data_type('train')\n",
    "        batch_generator = generate_batches(dataset=lm_dataset, \n",
    "                                           batch_size=args.batch_size,\n",
    "                                           collate_fn=collate_fn,\n",
    "                                           shuffle=True,\n",
    "                                           drop_last=False,\n",
    "                                           device=args.device)\n",
    "        \n",
    "        # Init values for calculating loss and cross-entropy\n",
    "        running_loss = 0.0\n",
    "        ce_sum = 0.0\n",
    "        ce_len = 0\n",
    "        \n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "        \n",
    "        for batch_idx, batch_dict in enumerate(batch_generator, 1):\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get predictions for batch and reshape for loss calculation\n",
    "            y_pred = model(batch_dict['source_batch'], \n",
    "                           batch_dict['batch_lengths'])\n",
    "            y_pred = y_pred.reshape(-1, y_pred.shape[2])\n",
    "            \n",
    "            # Get classes for batch and reshape for loss calculation\n",
    "            y_true = batch_dict['target_batch']\n",
    "            y_true = y_true.reshape(-1)\n",
    "            \n",
    "            # Get cross-entropy for each element without aggregation\n",
    "            # Ignore <MASK> indices for calculation\n",
    "            loss = F.cross_entropy(y_pred, y_true, ignore_index=mask_index,\n",
    "                                   reduction='none')\n",
    "            \n",
    "            # Accumulate sum of cross-entropy for perplexity calculation\n",
    "            ce_sum += loss.sum().detach().item()\n",
    "            \n",
    "            # Calculate loss on non-mask tokens\n",
    "            ce_values = loss[torch.nonzero(loss).flatten()]\n",
    "            loss = ce_values.mean()\n",
    "            \n",
    "            # Run backpropagation\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients for avoiding exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_norm)\n",
    "            \n",
    "            # Accumulate number of tokens (chars) for perplexity calculation\n",
    "            ce_len += len(ce_values.detach())\n",
    "            \n",
    "            # Calculate running loss\n",
    "            loss_value = loss.item()\n",
    "            running_loss += (loss_value - running_loss) / batch_idx\n",
    "            \n",
    "            # Calculate current value of perplexity\n",
    "            perplexity = np.exp(ce_sum / ce_len)\n",
    "            \n",
    "            # Get current learning rate\n",
    "            learning_rate = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            # Update training progress bar\n",
    "            train_params = dict(loss=running_loss,\n",
    "                                perplexity=perplexity,\n",
    "                                lr=learning_rate)\n",
    "            train_bar.set_postfix(train_params)\n",
    "            train_bar.update()\n",
    "            \n",
    "            # Update model parameters\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Save training params & metrics in current epoch\n",
    "        train_state['learning_rate'].append(learning_rate)\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_perplexity'].append(perplexity)\n",
    "        \n",
    "        \n",
    "        # Create generator based on validation data\n",
    "        lm_dataset.set_data_type('val')\n",
    "        batch_generator = generate_batches(dataset=lm_dataset, \n",
    "                                           batch_size=args.batch_size,\n",
    "                                           collate_fn=collate_fn,\n",
    "                                           shuffle=False,\n",
    "                                           drop_last=False,\n",
    "                                           device=args.device)\n",
    "        \n",
    "        # Init values for calculating loss and cross-entropy\n",
    "        running_loss = 0.0\n",
    "        ce_sum = 0.0\n",
    "        ce_len = 0\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Do not calculate gradients\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch_dict in enumerate(batch_generator, 1):\n",
    "                # Get predictions for batch and reshape for loss calculation\n",
    "                y_pred = model(batch_dict['source_batch'], \n",
    "                               batch_dict['batch_lengths'])\n",
    "                y_pred = y_pred.reshape(-1, y_pred.shape[2])\n",
    "                \n",
    "                # Get classes for batch and reshape for loss calculation\n",
    "                y_true = batch_dict['target_batch']\n",
    "                y_true = y_true.reshape(-1)\n",
    "                \n",
    "                # Get cross-entropy for each element without aggregation\n",
    "                # Ignore <MASK> indices for calculation\n",
    "                loss = F.cross_entropy(y_pred, y_true, ignore_index=mask_index,\n",
    "                               reduction='none')\n",
    "                \n",
    "                # Accumulate sum of cross-entropy for perplexity calculation\n",
    "                ce_sum += loss.sum().detach().item()\n",
    "                \n",
    "                # Calculate loss on non-mask tokens\n",
    "                ce_values = loss[torch.nonzero(loss).flatten()]\n",
    "                loss = ce_values.mean()\n",
    "                \n",
    "                # Accumulate number of tokens (chars) for perplexity calculation\n",
    "                ce_len += len(ce_values.detach())\n",
    "                \n",
    "                # Calculate running loss\n",
    "                loss_value = loss.item()\n",
    "                running_loss += (loss_value - running_loss) / batch_idx\n",
    "                \n",
    "                # Calculate current value of perplexity\n",
    "                perplexity = np.exp(ce_sum / ce_len)\n",
    "                \n",
    "                # Update validation progress bar\n",
    "                val_params = dict(loss=running_loss, \n",
    "                                  perplexity=perplexity)\n",
    "                val_bar.set_postfix(val_params)\n",
    "                val_bar.update()\n",
    "        \n",
    "        # Save validation metrics in current epoch\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_perplexity'].append(perplexity)\n",
    "        \n",
    "        # Update train state\n",
    "        train_state = update_train_state(args=args, \n",
    "                                         model=model, \n",
    "                                         train_state=train_state)\n",
    "        \n",
    "        # Update scheduling \n",
    "        # Decrease learning rate by factor of args.factor\n",
    "        # if validation loss is not decreasing \n",
    "        # for args.patience epochs\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "        \n",
    "        # Early stop if validation loss is not decreasing\n",
    "        # for args.early_stopping_criteria epochs\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "        \n",
    "        # Zero & update progress bars\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print('Exit training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final perplexity on validation data: 6.26\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print validation dataset perplexity\n",
    "# and visualize loss/perplexity values per epoch\n",
    "val_perplexity_final = train_state['val_perplexity'][-1]\n",
    "\n",
    "print(f'Final perplexity on validation data: {np.round(val_perplexity_final, 2)}')\n",
    "train_state_df = pd.DataFrame(train_state)\n",
    "train_state_df.filter(regex='(train|val)_loss').plot()\n",
    "train_state_df.filter(regex='(train|val)_perplexity').plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate prediction probabilites for word/chars in given dataset\n",
    "def calculate_probs(dataset, model, data_type='test',\n",
    "                    batch_size=5):\n",
    "    # Set specific dataset type\n",
    "    dataset.set_data_type(data_type)\n",
    "    \n",
    "    # Create batch generator\n",
    "    batch_generator = generate_batches(dataset=lm_dataset, \n",
    "                                   batch_size=batch_size,\n",
    "                                   collate_fn=collate_fn,\n",
    "                                   shuffle=True,\n",
    "                                   drop_last=False,\n",
    "                                   device=args.device)\n",
    "    \n",
    "    # Create list for storing prediction probabilities\n",
    "    probs = []\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Generate 1 batch with specified size\n",
    "    for batch_dict in islice(batch_generator, 1):\n",
    "        \n",
    "        # Get elements of batch dictionary\n",
    "        source_batch = batch_dict['source_batch']\n",
    "        target_batch = batch_dict['target_batch']\n",
    "        batch_lengths = batch_dict['batch_lengths']\n",
    "        \n",
    "        # Initialize hiddent state with zeros\n",
    "        h_t = torch.zeros(1, source_batch.shape[0], \n",
    "                          model.rnn.hidden_size).to(args.device)\n",
    "        \n",
    "        # Do not calculate gradients\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Iterate for each time step\n",
    "            for time_step in range(source_batch.shape[1]):\n",
    "                \n",
    "                # Get data for given time step\n",
    "                x_t = source_batch[:, time_step].unsqueeze(1)\n",
    "                \n",
    "                # Get embeddings\n",
    "                emb_t = model.embedding(x_t)\n",
    "                \n",
    "                # Forward data in RNN\n",
    "                rnn_out_t, h_t = model.rnn(emb_t, h_t)\n",
    "                \n",
    "                # Calculate prediction probabilities for given data\n",
    "                y_pred = model.fc1(rnn_out_t.squeeze(1))\n",
    "                y_pred_proba = F.softmax(y_pred, dim=1)\n",
    "                y_true_proba = y_pred_proba[range(source_batch.shape[0]), \n",
    "                                                  target_batch[:, time_step]]\n",
    "                \n",
    "                # Add probabilities to list\n",
    "                probs.append(y_true_proba)\n",
    "        \n",
    "        # Stack probabilities horizontally\n",
    "        probs = torch.stack(probs, dim=1)\n",
    "    \n",
    "    return batch_dict, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print words and word/chars probabilities \n",
    "# including <END> as important token\n",
    "def print_word_probs(batch_dict, probs):\n",
    "    \n",
    "    # Get elements of batch dictionary\n",
    "    target_batch = batch_dict['target_batch']\n",
    "    batch_lengths = batch_dict['batch_lengths']\n",
    "    \n",
    "    # Iterate for each sample in batch\n",
    "    for sample_idx in range(target_batch.shape[0]):\n",
    "        word_prob = 1.\n",
    "        word = ''\n",
    "        char_probs = ''\n",
    "        \n",
    "        # Iterate for each time step in sample\n",
    "        for time_step in range(batch_lengths[sample_idx]):\n",
    "            # Get char index, symbol and its probability\n",
    "            char_idx = target_batch[sample_idx, time_step].item()\n",
    "            char = vectorizer.char_vocab.lookup_index(char_idx)\n",
    "            char_prob = probs[sample_idx, time_step]\n",
    "            \n",
    "            # Construct word, word/char probs information\n",
    "            word += char\n",
    "            char_probs += f'{char}: {char_prob:.5f} '\n",
    "            word_prob *= char_prob     \n",
    "        \n",
    "        # Print word, word/char probs information\n",
    "        print(word)\n",
    "        print(char_probs)\n",
    "        print(f'Word propability: {word_prob}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "семплерный<END>\n",
      "с: 0.09811 е: 0.07994 м: 0.06278 п: 0.16552 л: 0.46091 е: 0.60625 р: 0.39050 н: 0.18473 ы: 0.74688 й: 0.99978 <END>: 0.99153 \n",
      "Word propability: 1.2163073961346527e-06\n",
      "\n",
      "неудобный<END>\n",
      "н: 0.05874 е: 0.50530 у: 0.04694 д: 0.10603 о: 0.14941 б: 0.03293 н: 0.05261 ы: 0.49296 й: 0.97614 <END>: 0.98967 \n",
      "Word propability: 1.8208217156256978e-08\n",
      "\n",
      "уверовать<END>\n",
      "у: 0.03433 в: 0.07317 е: 0.42261 р: 0.48582 о: 0.03565 в: 0.08167 а: 0.36470 т: 0.82224 ь: 0.86586 <END>: 0.80160 \n",
      "Word propability: 3.124544036836596e-07\n",
      "\n",
      "хмыкать<END>\n",
      "х: 0.01057 м: 0.02187 ы: 0.03370 к: 0.01947 а: 0.39803 т: 0.61021 ь: 0.99227 <END>: 0.55476 \n",
      "Word propability: 2.028495948991349e-08\n",
      "\n",
      "счётный<END>\n",
      "с: 0.09811 ч: 0.00866 ё: 0.09523 т: 0.68937 н: 0.19023 ы: 0.91963 й: 0.99976 <END>: 0.98149 \n",
      "Word propability: 9.576437150826678e-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_dict, test_probs = calculate_probs(lm_dataset, model, batch_size=5)\n",
    "print_word_probs(batch_dict, test_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words(model, vectorizer, \n",
    "                   batch_size=5,\n",
    "                   max_length=20):\n",
    "    indices = []\n",
    "    probs = []\n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "    h_t = torch.zeros(1, batch_size, \n",
    "                      model.rnn.hidden_size).to(args.device)\n",
    "\n",
    "    x_t = torch.full((batch_size, 1), \n",
    "                     vectorizer.char_vocab.begin_index,\n",
    "                     dtype=torch.int64).to(args.device)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for time_step in range(max_length):\n",
    "            emb_t = model.embedding(x_t)\n",
    "            rnn_out_t, h_t = model.rnn(emb_t, h_t)\n",
    "            y_pred = model.fc1(rnn_out_t.squeeze(1))\n",
    "            y_pred_proba = F.softmax(y_pred, dim=1)\n",
    "            y_pred_idx_best = torch.multinomial(y_pred_proba, num_samples=1).squeeze(1)\n",
    "            y_pred_proba_best = y_pred_proba[range(batch_size), y_pred_idx_best]\n",
    "\n",
    "            indices.append(y_pred_idx_best)\n",
    "            probs.append(y_pred_proba_best)\n",
    "            x_t = y_pred_idx_best.unsqueeze(1)\n",
    "        \n",
    "    indices = torch.stack(indices, dim=1)\n",
    "    probs = torch.stack(probs, dim=1)\n",
    "    \n",
    "    return indices, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_generated_word_probs(vectorizer, \n",
    "                               indices, probs):\n",
    "    batch_size = indices.shape[0]\n",
    "    max_length = indices.shape[1]\n",
    "    \n",
    "    for sample_idx in range(batch_size):\n",
    "        word_prob = 1.\n",
    "        word = ''\n",
    "        char_probs = ''\n",
    "        \n",
    "        for time_step in range(max_length):\n",
    "            char_idx = indices[sample_idx, time_step].item()\n",
    "                \n",
    "            char = vectorizer.char_vocab.lookup_index(char_idx)\n",
    "            char_prob = probs[sample_idx, time_step]\n",
    "            \n",
    "            word += char\n",
    "            char_probs += f'{char}: {char_prob:.5f} '\n",
    "            word_prob *= char_prob\n",
    "            \n",
    "            if char_idx == vectorizer.char_vocab.end_index:\n",
    "                break\n",
    "        \n",
    "        print(word)\n",
    "        print(char_probs)\n",
    "        print(f'Word propability: {word_prob}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "крадильный<END>\n",
      "к: 0.05894 р: 0.16861 а: 0.33172 д: 0.08167 и: 0.68109 л: 0.21882 ь: 0.74728 н: 0.89712 ы: 0.80852 й: 0.99930 <END>: 0.96816 \n",
      "Word propability: 2.1040743376943283e-05\n",
      "\n",
      "надостиня<END>\n",
      "н: 0.05874 а: 0.38588 д: 0.06862 о: 0.07532 с: 0.08529 т: 0.55245 и: 0.32448 н: 0.17028 я: 0.55993 <END>: 0.31938 \n",
      "Word propability: 5.4538443094998e-08\n",
      "\n",
      "агисат<END>\n",
      "а: 0.04182 г: 0.03265 и: 0.07433 с: 0.11592 а: 0.13510 т: 0.72051 <END>: 0.24920 \n",
      "Word propability: 2.853988974038657e-07\n",
      "\n",
      "индеко-поворыть<END>\n",
      "и: 0.02833 н: 0.24402 д: 0.21088 е: 0.34998 к: 0.61070 о: 0.05353 -: 0.01370 п: 0.07692 о: 0.38093 в: 0.07564 о: 0.65794 р: 0.63410 ы: 0.00512 т: 0.02648 ь: 0.79579 <END>: 0.56138 \n",
      "Word propability: 1.2807774284228409e-14\n",
      "\n",
      "стьия<END>\n",
      "с: 0.09811 т: 0.14697 ь: 0.00345 и: 0.00434 я: 0.59564 <END>: 0.93932 \n",
      "Word propability: 1.2073205368778872e-07\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indices, probs = generate_words(model, vectorizer)\n",
    "print_generated_word_probs(vectorizer, indices, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
