{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 50 # default - 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('../data')\n",
    "RANDOM_SEED = 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train.csv')\n",
    "valid_df = pd.read_csv(DATA_PATH/'valid.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH/'X_train_ftfy_spacy.pkl', 'rb') as  f:\n",
    "    X_train_clean = pickle.load(f)\n",
    "\n",
    "train_df['title'] = X_train_clean['clean_title']\n",
    "train_df['text'] = X_train_clean['clean_text']\n",
    "\n",
    "with open(DATA_PATH/'X_valid_ftfy_spacy.pkl', 'rb') as  f:\n",
    "    X_valid_clean = pickle.load(f)\n",
    "\n",
    "valid_df['title'] = X_valid_clean['clean_title']\n",
    "valid_df['text'] = X_valid_clean['clean_text']\n",
    "\n",
    "with open(DATA_PATH/'X_test_ftfy_spacy.pkl', 'rb') as  f:\n",
    "    X_test_clean = pickle.load(f)\n",
    "\n",
    "test_df['title'] = X_test_clean['clean_title']\n",
    "test_df['text'] = X_test_clean['clean_text']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.fillna('')\n",
    "X_valid = valid_df.fillna('')\n",
    "X_test = test_df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "X_train['class'] = le.fit_transform(X_train['label'])\n",
    "X_valid['class'] = le.transform(X_valid['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['clickbait', 'news', 'other'], dtype=object)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        y = y.values\n",
    "        \n",
    "        pos_count = X[y==1].sum(0) \n",
    "        neg_count = X[y==0].sum(0)\n",
    "        n = X.shape[1]\n",
    "        p = (pos_count + self.alpha) / (pos_count.sum() + self.alpha * n)\n",
    "        q = (neg_count + self.alpha) / (neg_count.sum() + self.alpha * n)\n",
    "        self.r_ = np.log(p / q)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X.multiply(self.r_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfVectorizerPlus(TfidfVectorizer):\n",
    "    def __init__(self, fit_add=None, norm_type=None, pivot=5, slope=0.2, \n",
    "                       input='content', encoding='utf-8', decode_error='strict', \n",
    "                       strip_accents=None, lowercase=True, preprocessor=None, \n",
    "                       tokenizer=None, analyzer='word', stop_words=None, \n",
    "                       token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), \n",
    "                       max_df=1.0, min_df=1, max_features=None, vocabulary=None, \n",
    "                       binary=False, dtype=np.float64, norm='l2', \n",
    "                       use_idf=True, smooth_idf=True, sublinear_tf=False):\n",
    "        super().__init__(input, encoding, decode_error,\n",
    "                         strip_accents, lowercase, preprocessor,\n",
    "                         tokenizer, analyzer, stop_words,\n",
    "                         token_pattern, ngram_range,\n",
    "                         max_df, min_df, max_features, vocabulary,\n",
    "                         binary, dtype, norm,\n",
    "                         use_idf, smooth_idf, sublinear_tf)\n",
    "        \n",
    "        self.fit_add = fit_add\n",
    "        self.norm_type = norm_type\n",
    "        self.pivot = pivot\n",
    "        self.slope = slope\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        if self.fit_add is not None:\n",
    "            X_new = pd.concat([X, self.fit_add])\n",
    "        else:\n",
    "            X_new = X\n",
    "        \n",
    "        super().fit(X_new, y)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        res = super().transform(X)\n",
    "            \n",
    "        if self.norm_type == 'pivot_cosine':\n",
    "            norm_factor = (1 - self.slope) * self.pivot + self.slope * sparse.linalg.norm(res, axis=1).reshape(-1, 1)\n",
    "            res = sparse.csr_matrix(res.multiply(1 / norm_factor))\n",
    "        elif self.norm_type == 'pivot_unique':\n",
    "            unique_terms_num = (res > 0).sum(axis=1)\n",
    "            norm_factor = (1 - self.slope) * self.pivot + self.slope * unique_terms_num\n",
    "            res = sparse.csr_matrix(res.multiply(1 / norm_factor))\n",
    "        elif self.norm_type is not None:\n",
    "            raise ValueError('Incorrect normalization type')\n",
    "            \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTruncater(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_length=None):\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if self.max_length is not None:\n",
    "            return X.str[:self.max_length]\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pattern = re.compile(r'(\\s)+')\n",
    "\n",
    "def tokenize(s):\n",
    "    return pattern.split(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('title', Pipeline([\n",
    "            ('extract', ColumnExtractor(columns='title')),\n",
    "            ('vec', TfidfVectorizer()),\n",
    "#             ('nb_features', NBTransformer())\n",
    "            \n",
    "        ])),\n",
    "        ('text', Pipeline([\n",
    "            ('extract', ColumnExtractor(columns='text')),\n",
    "            ('vec', TfidfVectorizer()),\n",
    "#             ('nb_features', NBTransformer())\n",
    "        ])),       \n",
    "    ], \n",
    "#         transformer_weights={\n",
    "#             'title': 0.4,\n",
    "#             'text': 0.6,\n",
    "#         }\n",
    "    )),\n",
    "    ('clf', LinearSVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pipe.fit_transform(X_train, X_train['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "              'features__title__vec': [TfidfVectorizer()],\n",
    "              'features__title__vec__strip_accents': ['ascii'], #[None, 'unicode', 'ascii'],\n",
    "              'features__title__vec__lowercase': [True], #[True, False],\n",
    "              'features__title__vec__analyzer': ['word'], #['word', 'char', 'char_wb'],\n",
    "              'features__title__vec__stop_words': [None], #[None, 'english'],\n",
    "              'features__title__vec__token_pattern': [r'\\b\\w+\\b'], #[r'\\b\\w+\\b', r'(?u)\\b\\w\\w+\\b'],\n",
    "              'features__title__vec__ngram_range': [(1, 3)], #[(1, 1), (1, 2), (1, 3)],\n",
    "              'features__title__vec__max_df': [0.3], #[0.3], #[0.3, 0.4, 0.5],\n",
    "              'features__title__vec__min_df': [1], #[1, 2, 3],\n",
    "              'features__title__vec__max_features': [None], #[None, 100000, 200000, 300000],\n",
    "              'features__title__vec__binary': [False],\n",
    "              'features__title__vec__use_idf': [True], #[True, False],\n",
    "              'features__title__vec__smooth_idf': [False], #[True, False],\n",
    "              'features__title__vec__sublinear_tf': [False], #[True, False],\n",
    "                                \n",
    "    \n",
    "              'features__text__vec': [TfidfVectorizer()],\n",
    "              'features__text__vec__strip_accents': ['ascii'], #[None, 'unicode', 'ascii'],\n",
    "              'features__text__vec__lowercase': [False], #[True, False],\n",
    "              'features__text__vec__analyzer': ['word'], #['word', 'char', 'char_wb'],\n",
    "              'features__text__vec__stop_words': [None], #[None, 'english'],\n",
    "              'features__text__vec__token_pattern': [r'\\b\\w+\\b'], #[r'\\b\\w+\\b', r'(?u)\\b\\w\\w+\\b'],\n",
    "              'features__text__vec__ngram_range': [(1, 2)], #[(1, 1), (1, 2), (1, 3)],\n",
    "              'features__text__vec__max_df': [0.8],\n",
    "              'features__text__vec__min_df': [1],\n",
    "              'features__text__vec__max_features': [200000], #[50000, 100000, 150000],\n",
    "              'features__text__vec__binary': [False], #[True, False],\n",
    "              'features__text__vec__use_idf': [True], #[True, False],\n",
    "              'features__text__vec__smooth_idf': [False], #[True, False],\n",
    "              'features__text__vec__sublinear_tf': [True], #[True, False],\n",
    "              \n",
    "              \n",
    "              'clf': [LogisticRegression()],\n",
    "              'clf__penalty': ['l2'], # ['l1', 'l2'], # ['l2'],\n",
    "              'clf__C': [5], #np.logspace(-2, 2, 5), # [2], \n",
    "              'clf__class_weight': ['balanced'], #[None, 'balanced'], #['balanced']\n",
    "              'clf__random_state': [RANDOM_SEED],\n",
    "              'clf__solver':  ['lbfgs'], #['lbfgs']\n",
    "              'clf__max_iter': [200],\n",
    "              'clf__multi_class': ['multinomial'], #['ovr', 'multinomial'],\n",
    "              \n",
    "              \n",
    "#               'features__title__vec': [TfidfVectorizer()],\n",
    "#               'features__title__vec__strip_accents': [None], #[None, 'unicode', 'ascii'],\n",
    "#               'features__title__vec__lowercase': [True], #[True, False],\n",
    "#               'features__title__vec__analyzer': ['word'], #['word', 'char', 'char_wb'],\n",
    "#               'features__title__vec__stop_words': [None], #[None, 'english'],\n",
    "#               'features__title__vec__token_pattern': [r'\\b\\w+\\b'], #[r'\\b\\w+\\b', r'(?u)\\b\\w\\w+\\b'],\n",
    "#               'features__title__vec__ngram_range': [(1, 4)],\n",
    "#               'features__title__vec__max_df': [0.8],\n",
    "#               'features__title__vec__min_df': [1], #[1, 5, 10],\n",
    "#               'features__title__vec__max_features': [70000],\n",
    "#               'features__title__vec__binary': [True], #[True, False],\n",
    "#               'features__title__vec__use_idf': [True], #[True, False],\n",
    "#               'features__title__vec__smooth_idf': [True], #[True, False],\n",
    "#               'features__title__vec__sublinear_tf': [True], #[True, False],\n",
    "\n",
    "#               'clf': [LinearSVC()],\n",
    "#               'clf__penalty': ['l2'],\n",
    "#               'clf__loss': ['squared_hinge'], #['squared_hinge', 'hinge'],\n",
    "#               'clf__dual': [False], #[True, False],\n",
    "#               'clf__C': [0.4],\n",
    "#               'clf__class_weight': ['balanced'],\n",
    "#               'clf__random_state': [random_seed],\n",
    "              \n",
    "#               'features__text__vec': [TfidfVectorizer()],\n",
    "#               'features__text__vec__strip_accents': ['ascii'], #[None, 'unicode', 'ascii'],\n",
    "#               'features__text__vec__lowercase': [False], #[True, False],\n",
    "#               'features__text__vec__analyzer': ['word'], #['word', 'char', 'char_wb'],\n",
    "#               'features__text__vec__stop_words': [None], #[None, 'english'],\n",
    "#               'features__text__vec__token_pattern': [r'\\b\\w+\\b'], #[r'\\b\\w+\\b', r'(?u)\\b\\w\\w+\\b'],\n",
    "#               'features__text__vec__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "#               'features__text__vec__max_df': [0.9],\n",
    "#               'features__text__vec__min_df': [1],\n",
    "#               'features__text__vec__max_features': [200000, 300000, 400000, 500000],\n",
    "#               'features__text__vec__binary': [False], #[True, False],\n",
    "#               'features__text__vec__use_idf': [True], #[True, False],\n",
    "#               'features__text__vec__smooth_idf': [False], #[True, False],\n",
    "#               'features__text__vec__sublinear_tf': [True], #[True, False],\n",
    "              \n",
    "#               'features__text__nb_features__alpha': np.linspace(0.1, 1, 10),\n",
    "    \n",
    "#               'clf': [LinearSVC()],\n",
    "# #               'clf__penalty': ['l2'],\n",
    "# #               'clf__loss': ['squared_hinge', 'hinge'],\n",
    "# #               'clf__dual': [False], #[True, False],\n",
    "#               'clf__C': [1],\n",
    "#               'clf__multi_class': ['crammer_singer'], #['ovr', 'crammer_singer']\n",
    "#               'clf__class_weight': ['balanced'], #[None, 'balanced'],\n",
    "#               'clf__random_state': [RANDOM_SEED],\n",
    "                 \n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=2, random_state=17, shuffle=True),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('title', Pipeline(memory=None,\n",
       "     steps=[('extract', ColumnExtractor(columns='title')), ('vec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', inp...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'features__title__vec': [TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.3, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=No...state': [17], 'clf__solver': ['lbfgs'], 'clf__max_iter': [200], 'clf__multi_class': ['multinomial']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "       scoring='f1_macro', verbose=2)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(pipe, param_grid, scoring='f1_macro', \n",
    "                           cv=cv, n_jobs=-1, return_train_score=False,\n",
    "                           verbose=2, iid=True)\n",
    "\n",
    "grid_search.fit(X_train, X_train['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8077506273242253,\n",
       " {'clf': LogisticRegression(C=5, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=200,\n",
       "            multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "            random_state=17, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "            warm_start=False),\n",
       "  'clf__C': 5,\n",
       "  'clf__class_weight': 'balanced',\n",
       "  'clf__max_iter': 200,\n",
       "  'clf__multi_class': 'multinomial',\n",
       "  'clf__penalty': 'l2',\n",
       "  'clf__random_state': 17,\n",
       "  'clf__solver': 'lbfgs',\n",
       "  'features__text__vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "          dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "          lowercase=False, max_df=0.8, max_features=200000, min_df=1,\n",
       "          ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=False,\n",
       "          stop_words=None, strip_accents='ascii', sublinear_tf=True,\n",
       "          token_pattern='\\\\b\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "          vocabulary=None),\n",
       "  'features__text__vec__analyzer': 'word',\n",
       "  'features__text__vec__binary': False,\n",
       "  'features__text__vec__lowercase': False,\n",
       "  'features__text__vec__max_df': 0.8,\n",
       "  'features__text__vec__max_features': 200000,\n",
       "  'features__text__vec__min_df': 1,\n",
       "  'features__text__vec__ngram_range': (1, 2),\n",
       "  'features__text__vec__smooth_idf': False,\n",
       "  'features__text__vec__stop_words': None,\n",
       "  'features__text__vec__strip_accents': 'ascii',\n",
       "  'features__text__vec__sublinear_tf': True,\n",
       "  'features__text__vec__token_pattern': '\\\\b\\\\w+\\\\b',\n",
       "  'features__text__vec__use_idf': True,\n",
       "  'features__title__vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "          dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "          lowercase=True, max_df=0.3, max_features=None, min_df=1,\n",
       "          ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=False,\n",
       "          stop_words=None, strip_accents='ascii', sublinear_tf=False,\n",
       "          token_pattern='\\\\b\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "          vocabulary=None),\n",
       "  'features__title__vec__analyzer': 'word',\n",
       "  'features__title__vec__binary': False,\n",
       "  'features__title__vec__lowercase': True,\n",
       "  'features__title__vec__max_df': 0.3,\n",
       "  'features__title__vec__max_features': None,\n",
       "  'features__title__vec__min_df': 1,\n",
       "  'features__title__vec__ngram_range': (1, 3),\n",
       "  'features__title__vec__smooth_idf': False,\n",
       "  'features__title__vec__stop_words': None,\n",
       "  'features__title__vec__strip_accents': 'ascii',\n",
       "  'features__title__vec__sublinear_tf': False,\n",
       "  'features__title__vec__token_pattern': '\\\\b\\\\w+\\\\b',\n",
       "  'features__title__vec__use_idf': True})"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_, grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>80.6462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>2.30501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>16.027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.0552391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf</th>\n",
       "      <td>LogisticRegression(C=5, class_weight='balanced...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__C</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__class_weight</th>\n",
       "      <td>balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__max_iter</th>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__multi_class</th>\n",
       "      <td>multinomial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__penalty</th>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__random_state</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__solver</th>\n",
       "      <td>lbfgs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec</th>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec__analyzer</th>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec__binary</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec__lowercase</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec__max_df</th>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec__max_features</th>\n",
       "      <td>200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec__min_df</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec__ngram_range</th>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec__smooth_idf</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec__stop_words</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec__strip_accents</th>\n",
       "      <td>ascii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec__sublinear_tf</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec__token_pattern</th>\n",
       "      <td>\\b\\w+\\b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec__use_idf</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec</th>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec__analyzer</th>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec__binary</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec__lowercase</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec__max_df</th>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec__max_features</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec__min_df</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec__ngram_range</th>\n",
       "      <td>(1, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec__smooth_idf</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec__stop_words</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec__strip_accents</th>\n",
       "      <td>ascii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec__sublinear_tf</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec__token_pattern</th>\n",
       "      <td>\\b\\w+\\b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec__use_idf</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'clf': LogisticRegression(C=5, class_weight='...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_score</th>\n",
       "      <td>0.808394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_score</th>\n",
       "      <td>0.807107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_score</th>\n",
       "      <td>0.807751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_score</th>\n",
       "      <td>0.000643503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           0\n",
       "mean_fit_time                                                                        80.6462\n",
       "std_fit_time                                                                         2.30501\n",
       "mean_score_time                                                                       16.027\n",
       "std_score_time                                                                     0.0552391\n",
       "param_clf                                  LogisticRegression(C=5, class_weight='balanced...\n",
       "param_clf__C                                                                               5\n",
       "param_clf__class_weight                                                             balanced\n",
       "param_clf__max_iter                                                                      200\n",
       "param_clf__multi_class                                                           multinomial\n",
       "param_clf__penalty                                                                        l2\n",
       "param_clf__random_state                                                                   17\n",
       "param_clf__solver                                                                      lbfgs\n",
       "param_features__text__vec                  TfidfVectorizer(analyzer='word', binary=False,...\n",
       "param_features__text__vec__analyzer                                                     word\n",
       "param_features__text__vec__binary                                                      False\n",
       "param_features__text__vec__lowercase                                                   False\n",
       "param_features__text__vec__max_df                                                        0.8\n",
       "param_features__text__vec__max_features                                               200000\n",
       "param_features__text__vec__min_df                                                          1\n",
       "param_features__text__vec__ngram_range                                                (1, 2)\n",
       "param_features__text__vec__smooth_idf                                                  False\n",
       "param_features__text__vec__stop_words                                                   None\n",
       "param_features__text__vec__strip_accents                                               ascii\n",
       "param_features__text__vec__sublinear_tf                                                 True\n",
       "param_features__text__vec__token_pattern                                             \\b\\w+\\b\n",
       "param_features__text__vec__use_idf                                                      True\n",
       "param_features__title__vec                 TfidfVectorizer(analyzer='word', binary=False,...\n",
       "param_features__title__vec__analyzer                                                    word\n",
       "param_features__title__vec__binary                                                     False\n",
       "param_features__title__vec__lowercase                                                   True\n",
       "param_features__title__vec__max_df                                                       0.3\n",
       "param_features__title__vec__max_features                                                None\n",
       "param_features__title__vec__min_df                                                         1\n",
       "param_features__title__vec__ngram_range                                               (1, 3)\n",
       "param_features__title__vec__smooth_idf                                                 False\n",
       "param_features__title__vec__stop_words                                                  None\n",
       "param_features__title__vec__strip_accents                                              ascii\n",
       "param_features__title__vec__sublinear_tf                                               False\n",
       "param_features__title__vec__token_pattern                                            \\b\\w+\\b\n",
       "param_features__title__vec__use_idf                                                     True\n",
       "params                                     {'clf': LogisticRegression(C=5, class_weight='...\n",
       "split0_test_score                                                                   0.808394\n",
       "split1_test_score                                                                   0.807107\n",
       "mean_test_score                                                                     0.807751\n",
       "std_test_score                                                                   0.000643503\n",
       "rank_test_score                                                                            1"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results_df = pd.DataFrame(grid_search.cv_results_).T\n",
    "cv_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "2911532\n"
     ]
    }
   ],
   "source": [
    "print(len(grid_search.best_estimator_.get_params()['features__text__vec'].vocabulary_))\n",
    "print(len(grid_search.best_estimator_.get_params()['features__text__vec'].stop_words_))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "grid_search.best_estimator_.get_params()['features__text__vec'].vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 2, 1, 1])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_pred = grid_search.predict_proba(X_valid).argmax(axis=1)\n",
    "y_val_pred"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_margins = grid_search.decision_function(X_valid)\n",
    "y_val_pred = (y_margins - y_margins.min()) / (y_margins.max() - y_margins.min())\n",
    "y_val_pred = y_val_pred.argmax(axis=1)\n",
    "y_val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['news', 'news', 'news', ..., 'other', 'news', 'news'], dtype=object)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13316441, 0.60416667, 0.26266892])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_val_pred) / len(y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, X_train['class'])\n",
    "y_val_pred = best_model.predict_proba(X_valid)[:, 1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, X_train['class'])\n",
    "y_margins = best_model.decision_function(X_valid)\n",
    "y_val_pred = (y_margins - y_margins.min()) / (y_margins.max() - y_margins.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8080476964917223"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(X_valid['class'], y_val_pred, average='macro')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Importance**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "perm = PermutationImportance(grid_search, random_state=RANDOM_SEED).fit(X_valid, X_valid['class'])\n",
    "\n",
    "eli5.show_weights(perm, feature_names = X_valid.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict & Submit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_df = pd.concat([X_train, X_valid], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('title', Pipeline(memory=None,\n",
       "     steps=[('extract', ColumnExtractor(columns='title')), ('vec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', inp...l2',\n",
       "          random_state=17, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "          warm_start=False))])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(full_train_df, full_train_df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Amazon_PROPN CEO_PROPN Jeff_PROPN Bezos_PROPN ...</td>\n",
       "      <td>More_ADJ Try_VERB Yahoo_PROPN Finance_PROPN on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Does_VERB Laura_PROPN Dern_PROPN Handle_PROPN ...</td>\n",
       "      <td>More_ADJ Laura_PROPN Dern_PROPN seems_VERB to_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>In_ADP this_DET photographer_NOUN 's_PART home...</td>\n",
       "      <td>Kirkuk_PROPN is_VERB a_DET city_NOUN of_ADP No...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>8_NUM Ways_PROPN To_PART Get_VERB Your_ADJ Spo...</td>\n",
       "      <td>Experts_NOUN say_VERB that_ADP communication_N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>US_PROPN says_VERB claim_VERB it_PRON supporte...</td>\n",
       "      <td>Share_VERB this_DET with_ADP Email_PROPN Faceb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0  Amazon_PROPN CEO_PROPN Jeff_PROPN Bezos_PROPN ...   \n",
       "1   1  Does_VERB Laura_PROPN Dern_PROPN Handle_PROPN ...   \n",
       "2   2  In_ADP this_DET photographer_NOUN 's_PART home...   \n",
       "3   3  8_NUM Ways_PROPN To_PART Get_VERB Your_ADJ Spo...   \n",
       "4   4  US_PROPN says_VERB claim_VERB it_PRON supporte...   \n",
       "\n",
       "                                                text  \n",
       "0  More_ADJ Try_VERB Yahoo_PROPN Finance_PROPN on...  \n",
       "1  More_ADJ Laura_PROPN Dern_PROPN seems_VERB to_...  \n",
       "2  Kirkuk_PROPN is_VERB a_DET city_NOUN of_ADP No...  \n",
       "3  Experts_NOUN say_VERB that_ADP communication_N...  \n",
       "4  Share_VERB this_DET with_ADP Email_PROPN Faceb...  "
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.columns = ['id', 'title', 'text']#, 'title_length', 'text_length',\n",
    "       #'is_title_na', 'is_text_na']\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_df.index == X_test.id).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = best_model.predict_proba(X_test).argmax(axis=1)\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['clickbait', 'clickbait', 'news', ..., 'news', 'news', 'news'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_label = le.inverse_transform(y_test_pred)\n",
    "y_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17496016, 0.74375775, 0.0812821 ])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_test_pred) / len(y_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      label\n",
       "0   0  clickbait\n",
       "1   1  clickbait\n",
       "2   2       news\n",
       "3   3  clickbait\n",
       "4   4       news"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = pd.DataFrame({'id': X_test['id'], 'label': y_test_label})\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,label\n",
      "0,clickbait\n",
      "1,clickbait\n",
      "2,news\n",
      "3,clickbait\n",
      "4,news\n",
      "5,news\n",
      "6,news\n",
      "7,news\n",
      "8,news\n"
     ]
    }
   ],
   "source": [
    "!head submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5648 submission.csv\n"
     ]
    }
   ],
   "source": [
    "!wc -l submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                               | 0.00/59.3k [00:00<?, ?B/s]"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c dlinnlp-spring-2019-clf -f submission.csv -m \"LR (mn)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
