{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d36dc3fca54619bc0caa50f3e81b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 50 # default - 50\n",
    "tqdm_notebook().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('../data')\n",
    "RANDOM_SEED = 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train.csv')\n",
    "valid_df = pd.read_csv(DATA_PATH/'valid.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH/'X_train_ftfy_spacy.pkl', 'rb') as  f:\n",
    "    X_train_clean = pickle.load(f)\n",
    "\n",
    "train_df['title'] = X_train_clean['clean_title']\n",
    "train_df['text'] = X_train_clean['clean_text']\n",
    "\n",
    "with open(DATA_PATH/'X_valid_ftfy_spacy.pkl', 'rb') as  f:\n",
    "    X_valid_clean = pickle.load(f)\n",
    "\n",
    "valid_df['title'] = X_valid_clean['clean_title']\n",
    "valid_df['text'] = X_valid_clean['clean_text']\n",
    "\n",
    "with open(DATA_PATH/'X_test_ftfy_spacy.pkl', 'rb') as  f:\n",
    "    X_test_clean = pickle.load(f)\n",
    "\n",
    "test_df['title'] = X_test_clean['clean_title']\n",
    "test_df['text'] = X_test_clean['clean_text']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.fillna('')\n",
    "X_valid = valid_df.fillna('')\n",
    "X_test = test_df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "X_train['class'] = le.fit_transform(X_train['label'])\n",
    "X_valid['class'] = le.transform(X_valid['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['clickbait', 'news', 'other'], dtype=object)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7236415302324b4c9b05c89d92f344a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24871), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26fce04582dd4caa80775fc319b8dd23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24871), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0    PROPN CCONJ PROPN PROPN PUNCT PROPN PROPN PART...\n",
       " 1    NOUN PART VERB DET PROPN NOUN ADP PROPN PROPN ...\n",
       " 2    DET PROPN PROPN ADP DET PROPN PUNCT VERB ADP P...\n",
       " 3    PROPN PART PROPN VERB PROPN PROPN ADP PROPN PR...\n",
       " 4    NOUN VERB PRON VERB NOUN NOUN PART NOUN ADP DE...\n",
       " Name: title, dtype: object,\n",
       " 0    NOUN ADV VERB PUNCT PROPN VERB VERB ADJ ADJ CC...\n",
       " 1    PROPN NUM ADJ PROPN PROPN PROPN PROPN VERB PRO...\n",
       " 2    NOUN VERB ADP DET NOUN ADP NOUN CCONJ NOUN ADP...\n",
       " 3    DET NOUN ADP NOUN VERB ADP PROPN PROPN PUNCT D...\n",
       " 4    DET ADJ NOUN NOUN VERB VERB ADP NOUN ADP PRON ...\n",
       " Name: text, dtype: object)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pos_title = X_train['title'].str.findall(r'_([A-Z]+)').progress_apply(lambda x: ' '.join(x))\n",
    "X_train_pos_text = X_train['text'].str.findall(r'_([A-Z]+)').progress_apply(lambda x: ' '.join(x))\n",
    "X_train_pos_title.head(), X_train_pos_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5762014c53a947d6b8b5bf014695981c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3552), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7215a7dfc18d433dbd7815d62817e079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3552), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0    PROPN VERB PRON VERB VERB NOUN PART ADJ ADP NO...\n",
       " 1    PROPN PROPN PART NOUN VERB ADJ ADJ NOUN ADP PROPN\n",
       " 2    PROPN PROPN VERB SYM NUM NUM ADP PROPN PROPN P...\n",
       " 3    NOUN VERB ADJ ADP DET PROPN PROPN PROPN PROPN ...\n",
       " 4    PROPN PROPN CCONJ PROPN PROPN PROPN PROPN PART...\n",
       " Name: title, dtype: object,\n",
       " 0    PROPN PROPN PUNCT PROPN PROPN NUM PUNCT NUM NU...\n",
       " 1    PROPN VERB VERB VERB DET NOUN ADP PROPN ADP PR...\n",
       " 2    PROPN PUNCT DET PROPN NOUN VERB PROPN PRON VER...\n",
       " 3    DET ADJ NOUN NOUN VERB NOUN NOUN NOUN VERB VER...\n",
       " 4    DET NOUN ADP PROPN PROPN VERB ADJ PROPN NOUN P...\n",
       " Name: text, dtype: object)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid_pos_title = X_valid['title'].str.findall(r'_([A-Z]+)').progress_apply(lambda x: ' '.join(x))\n",
    "X_valid_pos_text = X_valid['text'].str.findall(r'_([A-Z]+)').progress_apply(lambda x: ' '.join(x))\n",
    "X_valid_pos_title.head(), X_valid_pos_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c83e9c01734f4313941c9f56da0ab26f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5647), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2fdbcb6ce04a5c98d2dab5734bc48b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5647), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0    PROPN PROPN PROPN PROPN VERB ADV DET ADJ ADJ N...\n",
       " 1    VERB PROPN PROPN PROPN DET PROPN ADP DET PROPN...\n",
       " 2    ADP DET NOUN PART NOUN NOUN PUNCT VERB ADP ADP...\n",
       " 3    NUM PROPN PART VERB ADJ NOUN PART VERB PART AD...\n",
       " 4    PROPN VERB VERB PRON VERB VERB ADP PROPN VERB ...\n",
       " Name: title, dtype: object,\n",
       " 0    ADJ VERB PROPN PROPN ADP PROPN SPACE PROPN PRO...\n",
       " 1    ADJ PROPN PROPN VERB PART VERB ADV DET NOUN PU...\n",
       " 2    PROPN VERB DET NOUN ADP ADJ PROPN ADP DET ADJ ...\n",
       " 3    NOUN VERB ADP NOUN VERB DET NOUN ADP DET ADJ N...\n",
       " 4    VERB DET ADP PROPN PROPN PROPN PROPN PROPN PRO...\n",
       " Name: text, dtype: object)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_pos_title = X_test['title'].str.findall(r'_([A-Z]+)').progress_apply(lambda x: ' '.join(x))\n",
    "X_test_pos_text = X_test['text'].str.findall(r'_([A-Z]+)').progress_apply(lambda x: ' '.join(x))\n",
    "X_test_pos_title.head(), X_test_pos_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['title_pos'] = X_train_pos_title\n",
    "X_train['text_pos'] = X_train_pos_text\n",
    "\n",
    "X_valid['title_pos'] = X_valid_pos_title\n",
    "X_valid['text_pos'] = X_valid_pos_text\n",
    "\n",
    "X_test['title_pos'] = X_test_pos_title\n",
    "X_test['text_pos'] = X_test_pos_text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        y = y.values\n",
    "        \n",
    "        pos_count = X[y==1].sum(0) \n",
    "        neg_count = X[y==0].sum(0)\n",
    "        n = X.shape[1]\n",
    "        p = (pos_count + self.alpha) / (pos_count.sum() + self.alpha * n)\n",
    "        q = (neg_count + self.alpha) / (neg_count.sum() + self.alpha * n)\n",
    "        self.r_ = np.log(p / q)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X.multiply(self.r_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfVectorizerPlus(TfidfVectorizer):\n",
    "    def __init__(self, fit_add=None, norm_type=None, pivot=5, slope=0.2, \n",
    "                       input='content', encoding='utf-8', decode_error='strict', \n",
    "                       strip_accents=None, lowercase=True, preprocessor=None, \n",
    "                       tokenizer=None, analyzer='word', stop_words=None, \n",
    "                       token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), \n",
    "                       max_df=1.0, min_df=1, max_features=None, vocabulary=None, \n",
    "                       binary=False, dtype=np.float64, norm='l2', \n",
    "                       use_idf=True, smooth_idf=True, sublinear_tf=False):\n",
    "        super().__init__(input, encoding, decode_error,\n",
    "                         strip_accents, lowercase, preprocessor,\n",
    "                         tokenizer, analyzer, stop_words,\n",
    "                         token_pattern, ngram_range,\n",
    "                         max_df, min_df, max_features, vocabulary,\n",
    "                         binary, dtype, norm,\n",
    "                         use_idf, smooth_idf, sublinear_tf)\n",
    "        \n",
    "        self.fit_add = fit_add\n",
    "        self.norm_type = norm_type\n",
    "        self.pivot = pivot\n",
    "        self.slope = slope\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        if self.fit_add is not None:\n",
    "            X_new = pd.concat([X, self.fit_add])\n",
    "        else:\n",
    "            X_new = X\n",
    "        \n",
    "        super().fit(X_new, y)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        res = super().transform(X)\n",
    "            \n",
    "        if self.norm_type == 'pivot_cosine':\n",
    "            norm_factor = (1 - self.slope) * self.pivot + self.slope * sparse.linalg.norm(res, axis=1).reshape(-1, 1)\n",
    "            res = sparse.csr_matrix(res.multiply(1 / norm_factor))\n",
    "        elif self.norm_type == 'pivot_unique':\n",
    "            unique_terms_num = (res > 0).sum(axis=1)\n",
    "            norm_factor = (1 - self.slope) * self.pivot + self.slope * unique_terms_num\n",
    "            res = sparse.csr_matrix(res.multiply(1 / norm_factor))\n",
    "        elif self.norm_type is not None:\n",
    "            raise ValueError('Incorrect normalization type')\n",
    "            \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTruncater(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_length=None):\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if self.max_length is not None:\n",
    "            return X.str[:self.max_length]\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTransformer(TransformerMixin):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        self.model.fit(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return DataFrame(self.model.predict_proba(X))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pattern = re.compile(r'(\\s)+')\n",
    "\n",
    "def tokenize(s):\n",
    "    return pattern.split(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('title', Pipeline([\n",
    "            ('extract', ColumnExtractor(columns='title')),\n",
    "            ('vec', TfidfVectorizer())\n",
    "        ])),\n",
    "        ('text', Pipeline([\n",
    "            ('extract', ColumnExtractor(columns='text')),\n",
    "            ('vec', TfidfVectorizer())\n",
    "        ])),\n",
    "#         ('title_pos', Pipeline([\n",
    "#             ('extract', ColumnExtractor('title_pos')),\n",
    "#             ('vec', TfidfVectorizer())\n",
    "#         ])),\n",
    "        ('text_pos', Pipeline([\n",
    "            ('extract', ColumnExtractor('text_pos')),\n",
    "            ('vec', TfidfVectorizer())\n",
    "        ])),\n",
    "    ], \n",
    "#         transformer_weights={\n",
    "#             'title': 0.4,\n",
    "#             'text': 0.6,\n",
    "#         }\n",
    "    )),\n",
    "    ('clf', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pipe.fit_transform(X_train, X_train['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'features__title__vec': [TfidfVectorizer()],\n",
    "    'features__title__vec__strip_accents': ['unicode'], #[None, 'unicode', 'ascii'],\n",
    "    'features__title__vec__lowercase': [False], #[True, False],\n",
    "    'features__title__vec__analyzer': ['word'], #['word', 'char', 'char_wb'],\n",
    "    'features__title__vec__stop_words': ['english'], #[None, 'english'],\n",
    "    'features__title__vec__token_pattern': [r'\\b\\w{5,}\\b'], #, r'(?u)\\b\\w\\w+\\b'], #['r'\\b\\w+\\b',']\n",
    "    'features__title__vec__ngram_range': [(1, 3)], #[(1, 3), (1, 4), (1, 5)],\n",
    "    'features__title__vec__max_df': [0.5],\n",
    "    'features__title__vec__min_df': [1],\n",
    "    'features__title__vec__max_features': [None], #[None, 50000, 100000],\n",
    "    'features__title__vec__binary': [True], #[True, False],\n",
    "    'features__title__vec__use_idf': [True], #[True, False],\n",
    "    'features__title__vec__smooth_idf': [True], #[True, False],\n",
    "    'features__title__vec__sublinear_tf': [True], #[True, False],\n",
    "\n",
    "\n",
    "    'features__text__vec': [TfidfVectorizer()],\n",
    "    'features__text__vec__strip_accents': ['ascii'], #[None, 'unicode', 'ascii'],\n",
    "    'features__text__vec__lowercase': [False], #[True, False],\n",
    "    'features__text__vec__analyzer': ['word'], #['word', 'char', 'char_wb'],\n",
    "    'features__text__vec__stop_words': [None], #[None, 'english'],\n",
    "    #               'features__text__vec__token_pattern': [r'\\b\\w+\\b'], #[r'\\b\\w+\\b', r'(?u)\\b\\w\\w+\\b', r'\\b\\w{3,}\\b', r'\\b\\w{4,}\\b'],\n",
    "    'features__text__vec__ngram_range': [(1, 2)],\n",
    "    'features__text__vec__max_df': [0.4],\n",
    "    'features__text__vec__min_df': [2],\n",
    "    'features__text__vec__max_features': [150000],\n",
    "    'features__text__vec__binary': [True], #[True, False],\n",
    "    'features__text__vec__use_idf': [True], #[True, False],\n",
    "    'features__text__vec__smooth_idf': [False], #[True, False],\n",
    "    'features__text__vec__sublinear_tf': [True], #[True, False],\n",
    "     \n",
    "#     'features__title_pos__vec': [TfidfVectorizer()],\n",
    "#     'features__title_pos__vec__strip_accents': [None], #[None, 'unicode', 'ascii'],\n",
    "#     'features__title_pos__vec__lowercase': [True], #[True, False],\n",
    "#     'features__title_pos__vec__analyzer': ['word'], #['word', 'char', 'char_wb'],\n",
    "#     'features__title_pos__vec__stop_words': [None], #[None, 'english'],\n",
    "#     'features__title_pos__vec__ngram_range': [(1, 6)],\n",
    "#     'features__title_pos__vec__max_df': [0.4],\n",
    "#     'features__title_pos__vec__min_df': [60],\n",
    "#     'features__title_pos__vec__max_features': [None], #[None, 50000, 100000],\n",
    "#     'features__title_pos__vec__binary': [False], #[True, False],\n",
    "#     'features__title_pos__vec__use_idf': [True], #[True, False],\n",
    "#     'features__title_pos__vec__smooth_idf': [True], #[True, False],\n",
    "#     'features__title_pos__vec__sublinear_tf': [True], #[True, False],\n",
    "    \n",
    "    'features__text_pos__vec': [TfidfVectorizer()],\n",
    "    'features__text_pos__vec__strip_accents': [None], #[None, 'unicode', 'ascii'],\n",
    "    'features__text_pos__vec__lowercase': [True], #[True, False],\n",
    "    'features__text_pos__vec__analyzer': ['word'], #['char', 'char_wb'],\n",
    "    'features__text_pos__vec__stop_words': [None], #[None, 'english'],\n",
    "    'features__text_pos__vec__ngram_range': [(1, 3)],\n",
    "    'features__text_pos__vec__max_df': [1.0],\n",
    "    'features__text_pos__vec__min_df': [10],\n",
    "    'features__text_pos__vec__max_features': [None], #[None, 50000, 100000],\n",
    "    'features__text_pos__vec__binary': [True], #[True, False],\n",
    "    'features__text_pos__vec__use_idf': [False], #[True, False],\n",
    "    'features__text_pos__vec__smooth_idf': [True], #[True, False],\n",
    "    'features__text_pos__vec__sublinear_tf': [True], #[True, False],\n",
    "\n",
    "\n",
    "    'clf': [LogisticRegression()],\n",
    "    'clf__penalty': ['l2'], #['l1', 'l2'], # ['l2'],\n",
    "    'clf__C': [1], #np.logspace(-2, 2, 5), # [2], \n",
    "    'clf__class_weight': ['balanced'], #['balanced']\n",
    "    'clf__random_state': [RANDOM_SEED],\n",
    "    'clf__solver':  ['lbfgs'], #['lbfgs']\n",
    "    'clf__max_iter': [200],\n",
    "    'clf__multi_class': ['multinomial'], #['ovr', 'multinomial'],\n",
    "              \n",
    "              \n",
    "#               'features__title__vec': [TfidfVectorizer()],\n",
    "#               'features__title__vec__strip_accents': [None, 'unicode', 'ascii'],\n",
    "#               'features__title__vec__lowercase': [True, False],\n",
    "#               'features__title__vec__analyzer': ['word', 'char', 'char_wb'],\n",
    "#               'features__title__vec__stop_words': [None, 'english'],\n",
    "#               'features__title__vec__token_pattern': [r'\\b\\w+\\b'], #[r'\\b\\w+\\b', r'(?u)\\b\\w\\w+\\b'],\n",
    "#               'features__title__vec__ngram_range': [(1, 4)],\n",
    "#               'features__title__vec__max_df': [0.8],\n",
    "#               'features__title__vec__min_df': [1], #[1, 5, 10],\n",
    "#               'features__title__vec__max_features': [70000],\n",
    "#               'features__title__vec__binary': [True], #[True, False],\n",
    "#               'features__title__vec__use_idf': [True], #[True, False],\n",
    "#               'features__title__vec__smooth_idf': [True], #[True, False],\n",
    "#               'features__title__vec__sublinear_tf': [True], #[True, False],\n",
    "\n",
    "#               'clf': [LinearSVC()],\n",
    "#               'clf__penalty': ['l2'],\n",
    "#               'clf__loss': ['squared_hinge'], #['squared_hinge', 'hinge'],\n",
    "#               'clf__dual': [False], #[True, False],\n",
    "#               'clf__C': [0.4],\n",
    "#               'clf__class_weight': ['balanced'],\n",
    "#               'clf__random_state': [random_seed],\n",
    "              \n",
    "#               'features__text__vec': [TfidfVectorizer()],\n",
    "#               'features__text__vec__strip_accents': ['ascii'], #[None, 'unicode', 'ascii'],\n",
    "#               'features__text__vec__lowercase': [False], #[True, False],\n",
    "#               'features__text__vec__analyzer': ['word'], #['word', 'char', 'char_wb'],\n",
    "#               'features__text__vec__stop_words': [None], #[None, 'english'],\n",
    "#               'features__text__vec__token_pattern': [r'\\b\\w+\\b'], #[r'\\b\\w+\\b', r'(?u)\\b\\w\\w+\\b'],\n",
    "#               'features__text__vec__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "#               'features__text__vec__max_df': [0.9],\n",
    "#               'features__text__vec__min_df': [1],\n",
    "#               'features__text__vec__max_features': [200000, 300000, 400000, 500000],\n",
    "#               'features__text__vec__binary': [False], #[True, False],\n",
    "#               'features__text__vec__use_idf': [True], #[True, False],\n",
    "#               'features__text__vec__smooth_idf': [False], #[True, False],\n",
    "#               'features__text__vec__sublinear_tf': [True], #[True, False],\n",
    "              \n",
    "#               'features__text__nb_features__alpha': np.linspace(0.1, 1, 10),\n",
    "    \n",
    "#               'clf': [LinearSVC()],\n",
    "# #               'clf__penalty': ['l2'],\n",
    "# #               'clf__loss': ['squared_hinge', 'hinge'],\n",
    "# #               'clf__dual': [False], #[True, False],\n",
    "#               'clf__C': [1],\n",
    "#               'clf__multi_class': ['crammer_singer'], #['ovr', 'crammer_singer']\n",
    "#               'clf__class_weight': ['balanced'], #[None, 'balanced'],\n",
    "#               'clf__random_state': [RANDOM_SEED],\n",
    "                 \n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=2, random_state=17, shuffle=True),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('title', Pipeline(memory=None,\n",
       "     steps=[('extract', ColumnExtractor(columns='title')), ('vec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', inp...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'features__title__vec': [TfidfVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=0.5, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=No...state': [17], 'clf__solver': ['lbfgs'], 'clf__max_iter': [200], 'clf__multi_class': ['multinomial']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "       scoring='f1_macro', verbose=2)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(pipe, param_grid, scoring='f1_macro', \n",
    "                           cv=cv, n_jobs=-1, return_train_score=False,\n",
    "                           verbose=2, iid=True)\n",
    "\n",
    "grid_search.fit(X_train, X_train['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.8058100230051004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8018456296764946,\n",
       " {'clf': LogisticRegression(C=1, class_weight='balanced', dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=200,\n",
       "            multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "            random_state=17, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "            warm_start=False),\n",
       "  'clf__C': 1,\n",
       "  'clf__class_weight': 'balanced',\n",
       "  'clf__max_iter': 200,\n",
       "  'clf__multi_class': 'multinomial',\n",
       "  'clf__penalty': 'l2',\n",
       "  'clf__random_state': 17,\n",
       "  'clf__solver': 'lbfgs',\n",
       "  'features__text__vec': TfidfVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "          dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "          lowercase=False, max_df=0.4, max_features=150000, min_df=2,\n",
       "          ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=False,\n",
       "          stop_words=None, strip_accents='ascii', sublinear_tf=True,\n",
       "          token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "          vocabulary=None),\n",
       "  'features__text__vec__analyzer': 'word',\n",
       "  'features__text__vec__binary': True,\n",
       "  'features__text__vec__lowercase': False,\n",
       "  'features__text__vec__max_df': 0.4,\n",
       "  'features__text__vec__max_features': 150000,\n",
       "  'features__text__vec__min_df': 2,\n",
       "  'features__text__vec__ngram_range': (1, 2),\n",
       "  'features__text__vec__smooth_idf': False,\n",
       "  'features__text__vec__stop_words': None,\n",
       "  'features__text__vec__strip_accents': 'ascii',\n",
       "  'features__text__vec__sublinear_tf': True,\n",
       "  'features__text__vec__use_idf': True,\n",
       "  'features__text_pos__vec': TfidfVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "          dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "          lowercase=True, max_df=1.0, max_features=None, min_df=10,\n",
       "          ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "          stop_words=None, strip_accents=None, sublinear_tf=True,\n",
       "          token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=False,\n",
       "          vocabulary=None),\n",
       "  'features__text_pos__vec__analyzer': 'word',\n",
       "  'features__text_pos__vec__binary': True,\n",
       "  'features__text_pos__vec__lowercase': True,\n",
       "  'features__text_pos__vec__max_df': 1.0,\n",
       "  'features__text_pos__vec__max_features': None,\n",
       "  'features__text_pos__vec__min_df': 10,\n",
       "  'features__text_pos__vec__ngram_range': (1, 3),\n",
       "  'features__text_pos__vec__smooth_idf': True,\n",
       "  'features__text_pos__vec__stop_words': None,\n",
       "  'features__text_pos__vec__strip_accents': None,\n",
       "  'features__text_pos__vec__sublinear_tf': True,\n",
       "  'features__text_pos__vec__use_idf': False,\n",
       "  'features__title__vec': TfidfVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "          dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "          lowercase=False, max_df=0.5, max_features=None, min_df=1,\n",
       "          ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "          stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
       "          token_pattern='\\\\b\\\\w{5,}\\\\b', tokenizer=None, use_idf=True,\n",
       "          vocabulary=None),\n",
       "  'features__title__vec__analyzer': 'word',\n",
       "  'features__title__vec__binary': True,\n",
       "  'features__title__vec__lowercase': False,\n",
       "  'features__title__vec__max_df': 0.5,\n",
       "  'features__title__vec__max_features': None,\n",
       "  'features__title__vec__min_df': 1,\n",
       "  'features__title__vec__ngram_range': (1, 3),\n",
       "  'features__title__vec__smooth_idf': True,\n",
       "  'features__title__vec__stop_words': 'english',\n",
       "  'features__title__vec__strip_accents': 'unicode',\n",
       "  'features__title__vec__sublinear_tf': True,\n",
       "  'features__title__vec__token_pattern': '\\\\b\\\\w{5,}\\\\b',\n",
       "  'features__title__vec__use_idf': True})"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_, grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>84.8705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>1.59469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>27.6077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.2943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf</th>\n",
       "      <td>LogisticRegression(C=1, class_weight='balanced...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__C</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__class_weight</th>\n",
       "      <td>balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__max_iter</th>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__multi_class</th>\n",
       "      <td>multinomial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__penalty</th>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__random_state</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__solver</th>\n",
       "      <td>lbfgs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec</th>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=True, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec__analyzer</th>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec__binary</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec__lowercase</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec__max_df</th>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec__max_features</th>\n",
       "      <td>150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec__min_df</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec__ngram_range</th>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec__smooth_idf</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec__stop_words</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec__strip_accents</th>\n",
       "      <td>ascii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec__sublinear_tf</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text__vec__use_idf</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text_pos__vec</th>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=True, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text_pos__vec__analyzer</th>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text_pos__vec__binary</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text_pos__vec__lowercase</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text_pos__vec__max_df</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text_pos__vec__max_features</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text_pos__vec__min_df</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text_pos__vec__ngram_range</th>\n",
       "      <td>(1, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text_pos__vec__smooth_idf</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text_pos__vec__stop_words</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text_pos__vec__strip_accents</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text_pos__vec__sublinear_tf</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__text_pos__vec__use_idf</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec</th>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=True, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec__analyzer</th>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec__binary</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec__lowercase</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec__max_df</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec__max_features</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec__min_df</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec__ngram_range</th>\n",
       "      <td>(1, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec__smooth_idf</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec__stop_words</th>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec__strip_accents</th>\n",
       "      <td>unicode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec__sublinear_tf</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec__token_pattern</th>\n",
       "      <td>\\b\\w{5,}\\b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_features__title__vec__use_idf</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'clf': LogisticRegression(C=1, class_weight='...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_score</th>\n",
       "      <td>0.800839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_score</th>\n",
       "      <td>0.802852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_score</th>\n",
       "      <td>0.801846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_score</th>\n",
       "      <td>0.00100671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              0\n",
       "mean_fit_time                                                                           84.8705\n",
       "std_fit_time                                                                            1.59469\n",
       "mean_score_time                                                                         27.6077\n",
       "std_score_time                                                                           0.2943\n",
       "param_clf                                     LogisticRegression(C=1, class_weight='balanced...\n",
       "param_clf__C                                                                                  1\n",
       "param_clf__class_weight                                                                balanced\n",
       "param_clf__max_iter                                                                         200\n",
       "param_clf__multi_class                                                              multinomial\n",
       "param_clf__penalty                                                                           l2\n",
       "param_clf__random_state                                                                      17\n",
       "param_clf__solver                                                                         lbfgs\n",
       "param_features__text__vec                     TfidfVectorizer(analyzer='word', binary=True, ...\n",
       "param_features__text__vec__analyzer                                                        word\n",
       "param_features__text__vec__binary                                                          True\n",
       "param_features__text__vec__lowercase                                                      False\n",
       "param_features__text__vec__max_df                                                           0.4\n",
       "param_features__text__vec__max_features                                                  150000\n",
       "param_features__text__vec__min_df                                                             2\n",
       "param_features__text__vec__ngram_range                                                   (1, 2)\n",
       "param_features__text__vec__smooth_idf                                                     False\n",
       "param_features__text__vec__stop_words                                                      None\n",
       "param_features__text__vec__strip_accents                                                  ascii\n",
       "param_features__text__vec__sublinear_tf                                                    True\n",
       "param_features__text__vec__use_idf                                                         True\n",
       "param_features__text_pos__vec                 TfidfVectorizer(analyzer='word', binary=True, ...\n",
       "param_features__text_pos__vec__analyzer                                                    word\n",
       "param_features__text_pos__vec__binary                                                      True\n",
       "param_features__text_pos__vec__lowercase                                                   True\n",
       "param_features__text_pos__vec__max_df                                                         1\n",
       "param_features__text_pos__vec__max_features                                                None\n",
       "param_features__text_pos__vec__min_df                                                        10\n",
       "param_features__text_pos__vec__ngram_range                                               (1, 3)\n",
       "param_features__text_pos__vec__smooth_idf                                                  True\n",
       "param_features__text_pos__vec__stop_words                                                  None\n",
       "param_features__text_pos__vec__strip_accents                                               None\n",
       "param_features__text_pos__vec__sublinear_tf                                                True\n",
       "param_features__text_pos__vec__use_idf                                                    False\n",
       "param_features__title__vec                    TfidfVectorizer(analyzer='word', binary=True, ...\n",
       "param_features__title__vec__analyzer                                                       word\n",
       "param_features__title__vec__binary                                                         True\n",
       "param_features__title__vec__lowercase                                                     False\n",
       "param_features__title__vec__max_df                                                          0.5\n",
       "param_features__title__vec__max_features                                                   None\n",
       "param_features__title__vec__min_df                                                            1\n",
       "param_features__title__vec__ngram_range                                                  (1, 3)\n",
       "param_features__title__vec__smooth_idf                                                     True\n",
       "param_features__title__vec__stop_words                                                  english\n",
       "param_features__title__vec__strip_accents                                               unicode\n",
       "param_features__title__vec__sublinear_tf                                                   True\n",
       "param_features__title__vec__token_pattern                                            \\b\\w{5,}\\b\n",
       "param_features__title__vec__use_idf                                                        True\n",
       "params                                        {'clf': LogisticRegression(C=1, class_weight='...\n",
       "split0_test_score                                                                      0.800839\n",
       "split1_test_score                                                                      0.802852\n",
       "mean_test_score                                                                        0.801846\n",
       "std_test_score                                                                       0.00100671\n",
       "rank_test_score                                                                               1"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results_df = pd.DataFrame(grid_search.cv_results_).T\n",
    "cv_results_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(len(grid_search.best_estimator_.get_params()['features__text__vec'].vocabulary_))\n",
    "print(len(grid_search.best_estimator_.get_params()['features__text__vec'].stop_words_))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "grid_search.best_estimator_.get_params()['features__text__vec'].vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 2, 1, 1])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_pred = grid_search.predict_proba(X_valid).argmax(axis=1)\n",
    "y_val_pred"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_margins = grid_search.decision_function(X_valid)\n",
    "y_val_pred = (y_margins - y_margins.min()) / (y_margins.max() - y_margins.min())\n",
    "y_val_pred = y_val_pred.argmax(axis=1)\n",
    "y_val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['news', 'news', 'news', ..., 'other', 'news', 'news'], dtype=object)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15990991, 0.57516892, 0.26492117])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_val_pred) / len(y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, X_train['class'])\n",
    "y_val_pred = best_model.predict_proba(X_valid)[:, 1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, X_train['class'])\n",
    "y_margins = best_model.decision_function(X_valid)\n",
    "y_val_pred = (y_margins - y_margins.min()) / (y_margins.max() - y_margins.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80614719136929"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(X_valid['class'], y_val_pred, average='macro')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Importance**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "perm = PermutationImportance(grid_search, random_state=RANDOM_SEED).fit(X_valid, X_valid['class'])\n",
    "\n",
    "eli5.show_weights(perm, feature_names = X_valid.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict & Submit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_df = pd.concat([X_train, X_valid], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('title', Pipeline(memory=None,\n",
       "     steps=[('extract', ColumnExtractor(columns='title')), ('vec', TfidfVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', inpu...l2',\n",
       "          random_state=17, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "          warm_start=False))])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(full_train_df, full_train_df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'title', 'text', 'title_pos', 'text_pos'], dtype='object')"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>title_pos</th>\n",
       "      <th>text_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Amazon_PROPN CEO_PROPN Jeff_PROPN Bezos_PROPN ...</td>\n",
       "      <td>More_ADJ Try_VERB Yahoo_PROPN Finance_PROPN on...</td>\n",
       "      <td>PROPN PROPN PROPN PROPN VERB ADV DET ADJ ADJ N...</td>\n",
       "      <td>ADJ VERB PROPN PROPN ADP PROPN SPACE PROPN PRO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Does_VERB Laura_PROPN Dern_PROPN Handle_PROPN ...</td>\n",
       "      <td>More_ADJ Laura_PROPN Dern_PROPN seems_VERB to_...</td>\n",
       "      <td>VERB PROPN PROPN PROPN DET PROPN ADP DET PROPN...</td>\n",
       "      <td>ADJ PROPN PROPN VERB PART VERB ADV DET NOUN PU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>In_ADP this_DET photographer_NOUN 's_PART home...</td>\n",
       "      <td>Kirkuk_PROPN is_VERB a_DET city_NOUN of_ADP No...</td>\n",
       "      <td>ADP DET NOUN PART NOUN NOUN PUNCT VERB ADP ADP...</td>\n",
       "      <td>PROPN VERB DET NOUN ADP ADJ PROPN ADP DET ADJ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>8_NUM Ways_PROPN To_PART Get_VERB Your_ADJ Spo...</td>\n",
       "      <td>Experts_NOUN say_VERB that_ADP communication_N...</td>\n",
       "      <td>NUM PROPN PART VERB ADJ NOUN PART VERB PART AD...</td>\n",
       "      <td>NOUN VERB ADP NOUN VERB DET NOUN ADP DET ADJ N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>US_PROPN says_VERB claim_VERB it_PRON supporte...</td>\n",
       "      <td>Share_VERB this_DET with_ADP Email_PROPN Faceb...</td>\n",
       "      <td>PROPN VERB VERB PRON VERB VERB ADP PROPN VERB ...</td>\n",
       "      <td>VERB DET ADP PROPN PROPN PROPN PROPN PROPN PRO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0  Amazon_PROPN CEO_PROPN Jeff_PROPN Bezos_PROPN ...   \n",
       "1   1  Does_VERB Laura_PROPN Dern_PROPN Handle_PROPN ...   \n",
       "2   2  In_ADP this_DET photographer_NOUN 's_PART home...   \n",
       "3   3  8_NUM Ways_PROPN To_PART Get_VERB Your_ADJ Spo...   \n",
       "4   4  US_PROPN says_VERB claim_VERB it_PRON supporte...   \n",
       "\n",
       "                                                text  \\\n",
       "0  More_ADJ Try_VERB Yahoo_PROPN Finance_PROPN on...   \n",
       "1  More_ADJ Laura_PROPN Dern_PROPN seems_VERB to_...   \n",
       "2  Kirkuk_PROPN is_VERB a_DET city_NOUN of_ADP No...   \n",
       "3  Experts_NOUN say_VERB that_ADP communication_N...   \n",
       "4  Share_VERB this_DET with_ADP Email_PROPN Faceb...   \n",
       "\n",
       "                                           title_pos  \\\n",
       "0  PROPN PROPN PROPN PROPN VERB ADV DET ADJ ADJ N...   \n",
       "1  VERB PROPN PROPN PROPN DET PROPN ADP DET PROPN...   \n",
       "2  ADP DET NOUN PART NOUN NOUN PUNCT VERB ADP ADP...   \n",
       "3  NUM PROPN PART VERB ADJ NOUN PART VERB PART AD...   \n",
       "4  PROPN VERB VERB PRON VERB VERB ADP PROPN VERB ...   \n",
       "\n",
       "                                            text_pos  \n",
       "0  ADJ VERB PROPN PROPN ADP PROPN SPACE PROPN PRO...  \n",
       "1  ADJ PROPN PROPN VERB PART VERB ADV DET NOUN PU...  \n",
       "2  PROPN VERB DET NOUN ADP ADJ PROPN ADP DET ADJ ...  \n",
       "3  NOUN VERB ADP NOUN VERB DET NOUN ADP DET ADJ N...  \n",
       "4  VERB DET ADP PROPN PROPN PROPN PROPN PROPN PRO...  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.columns = ['id', 'title', 'text', 'title_pos', 'text_pos']#, 'title_length', 'text_length',\n",
    "       #'is_title_na', 'is_text_na']\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_df.index == X_test.id).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = best_model.predict_proba(X_test).argmax(axis=1)\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['clickbait', 'clickbait', 'news', ..., 'news', 'news', 'news'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_label = le.inverse_transform(y_test_pred)\n",
    "y_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2066584 , 0.7060386 , 0.08730299])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_test_pred) / len(y_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      label\n",
       "0   0  clickbait\n",
       "1   1  clickbait\n",
       "2   2       news\n",
       "3   3  clickbait\n",
       "4   4       news"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = pd.DataFrame({'id': X_test['id'], 'label': y_test_label})\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,label\n",
      "0,clickbait\n",
      "1,clickbait\n",
      "2,news\n",
      "3,clickbait\n",
      "4,news\n",
      "5,news\n",
      "6,news\n",
      "7,news\n",
      "8,news\n"
     ]
    }
   ],
   "source": [
    "!head submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5648 submission.csv\n"
     ]
    }
   ],
   "source": [
    "!wc -l submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 60.3k/60.3k [00:03<00:00, 16.0kB/s]\n",
      "Successfully submitted to DL in NLP Spring 2019. Classification"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c dlinnlp-spring-2019-clf -f submission.csv -m \"LR (mn)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59144613, 0.77370618, 0.36949007],\n",
       "       [0.4860577 , 0.67253872, 0.57604596],\n",
       "       [0.42553417, 0.79839178, 0.51071644],\n",
       "       ...,\n",
       "       [0.44698388, 0.51817221, 0.7694863 ],\n",
       "       [0.66384886, 0.84248302, 0.22831051],\n",
       "       [0.58173126, 0.84916822, 0.30374291]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_margins = grid_search.decision_function(X_valid)\n",
    "y_val_pred = (y_margins - y_margins.min()) / (y_margins.max() - y_margins.min())\n",
    "# y_val_pred = y_val_pred.argmax(axis=1)\n",
    "y_val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>pred_0</th>\n",
       "      <th>pred_1</th>\n",
       "      <th>pred_2</th>\n",
       "      <th>class_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>news</td>\n",
       "      <td>Trump_PROPN says_VERB he_PRON is_VERB releasin...</td>\n",
       "      <td>Bob_PROPN Bryan_PROPN ,_PUNCT Business_PROPN I...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.591446</td>\n",
       "      <td>0.773706</td>\n",
       "      <td>0.369490</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news</td>\n",
       "      <td>Fidel_PROPN Castro_PROPN 's_PART ashes_NOUN ma...</td>\n",
       "      <td>Cubans_PROPN have_VERB been_VERB lining_VERB t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.486058</td>\n",
       "      <td>0.672539</td>\n",
       "      <td>0.576046</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news</td>\n",
       "      <td>Obama_PROPN Administration_PROPN Sending_VERB ...</td>\n",
       "      <td>WASHINGTON_PROPN —_PUNCT The_DET Obama_PROPN a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.425534</td>\n",
       "      <td>0.798392</td>\n",
       "      <td>0.510716</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news</td>\n",
       "      <td>Insurers_NOUN Are_VERB Worried_ADJ About_ADP T...</td>\n",
       "      <td>The_DET main_ADJ industry_NOUN groups_NOUN rep...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575398</td>\n",
       "      <td>0.764276</td>\n",
       "      <td>0.394968</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>news</td>\n",
       "      <td>Kobe_PROPN Bryant_PROPN and_CCONJ Nike_PROPN F...</td>\n",
       "      <td>A_DET year_NOUN after_ADP Kobe_PROPN Bryant_PR...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.543219</td>\n",
       "      <td>0.730411</td>\n",
       "      <td>0.461013</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                              title  \\\n",
       "0  news  Trump_PROPN says_VERB he_PRON is_VERB releasin...   \n",
       "1  news  Fidel_PROPN Castro_PROPN 's_PART ashes_NOUN ma...   \n",
       "2  news  Obama_PROPN Administration_PROPN Sending_VERB ...   \n",
       "3  news  Insurers_NOUN Are_VERB Worried_ADJ About_ADP T...   \n",
       "4  news  Kobe_PROPN Bryant_PROPN and_CCONJ Nike_PROPN F...   \n",
       "\n",
       "                                                text  class    pred_0  \\\n",
       "0  Bob_PROPN Bryan_PROPN ,_PUNCT Business_PROPN I...      1  0.591446   \n",
       "1  Cubans_PROPN have_VERB been_VERB lining_VERB t...      1  0.486058   \n",
       "2  WASHINGTON_PROPN —_PUNCT The_DET Obama_PROPN a...      1  0.425534   \n",
       "3  The_DET main_ADJ industry_NOUN groups_NOUN rep...      1  0.575398   \n",
       "4  A_DET year_NOUN after_ADP Kobe_PROPN Bryant_PR...      1  0.543219   \n",
       "\n",
       "     pred_1    pred_2  class_pred  \n",
       "0  0.773706  0.369490           1  \n",
       "1  0.672539  0.576046           1  \n",
       "2  0.798392  0.510716           1  \n",
       "3  0.764276  0.394968           1  \n",
       "4  0.730411  0.461013           1  "
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_df = X_valid.copy()\n",
    "validate_df = validate_df.assign(pred_0 = y_val_pred[:, 0], \n",
    "                                 pred_1 = y_val_pred[:, 1],\n",
    "                                 pred_2 = y_val_pred[:, 2],\n",
    "                                 class_pred = y_val_pred.argmax(axis=1))\n",
    "validate_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>pred_0</th>\n",
       "      <th>pred_1</th>\n",
       "      <th>pred_2</th>\n",
       "      <th>class_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>news</td>\n",
       "      <td>How_ADV to_PART Talk_VERB to_ADP Your_ADJ Teen...</td>\n",
       "      <td>More_ADJ For_ADP parents_NOUN ,_PUNCT talking_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.746912</td>\n",
       "      <td>0.720410</td>\n",
       "      <td>0.267320</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>news</td>\n",
       "      <td>Genius_PROPN Bar_PROPN secrets_NOUN revealed_V...</td>\n",
       "      <td>One_NUM of_ADP the_DET biggest_ADJ things_NOUN...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.598378</td>\n",
       "      <td>0.581579</td>\n",
       "      <td>0.554686</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>news</td>\n",
       "      <td>13_NUM Gifts_NOUN For_ADP People_NOUN Who_NOUN...</td>\n",
       "      <td>Love_NOUN is_VERB n't_ADV all_DET we_PRON need...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.635953</td>\n",
       "      <td>0.620099</td>\n",
       "      <td>0.478591</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>news</td>\n",
       "      <td>Deer_PROPN Debacle_NOUN Exacerbates_VERB New_P...</td>\n",
       "      <td>Desc_PROPN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.416454</td>\n",
       "      <td>0.626502</td>\n",
       "      <td>0.691686</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>news</td>\n",
       "      <td>Donald_PROPN Trump_PROPN Pays_VERB Respects_NO...</td>\n",
       "      <td>President_PROPN Donald_PROPN Trump_PROPN trave...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.712625</td>\n",
       "      <td>0.705318</td>\n",
       "      <td>0.316700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                              title  \\\n",
       "123  news  How_ADV to_PART Talk_VERB to_ADP Your_ADJ Teen...   \n",
       "229  news  Genius_PROPN Bar_PROPN secrets_NOUN revealed_V...   \n",
       "347  news  13_NUM Gifts_NOUN For_ADP People_NOUN Who_NOUN...   \n",
       "391  news  Deer_PROPN Debacle_NOUN Exacerbates_VERB New_P...   \n",
       "490  news  Donald_PROPN Trump_PROPN Pays_VERB Respects_NO...   \n",
       "\n",
       "                                                  text  class    pred_0  \\\n",
       "123  More_ADJ For_ADP parents_NOUN ,_PUNCT talking_...      1  0.746912   \n",
       "229  One_NUM of_ADP the_DET biggest_ADJ things_NOUN...      1  0.598378   \n",
       "347  Love_NOUN is_VERB n't_ADV all_DET we_PRON need...      1  0.635953   \n",
       "391                                         Desc_PROPN      1  0.416454   \n",
       "490  President_PROPN Donald_PROPN Trump_PROPN trave...      1  0.712625   \n",
       "\n",
       "       pred_1    pred_2  class_pred  \n",
       "123  0.720410  0.267320           0  \n",
       "229  0.581579  0.554686           0  \n",
       "347  0.620099  0.478591           0  \n",
       "391  0.626502  0.691686           2  \n",
       "490  0.705318  0.316700           0  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_df = validate_df[validate_df['class'] != validate_df['class_pred']]\n",
    "incorrect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class  class_pred\n",
       "0      0              542\n",
       "       1                1\n",
       "1      0               19\n",
       "       1             2065\n",
       "       2                4\n",
       "2      2              921\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_df.groupby(['class', 'class_pred'])['label'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class  class_pred\n",
       "0      1              1\n",
       "1      0             19\n",
       "       2              4\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_df.groupby(['class', 'class_pred'])['label'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>pred_0</th>\n",
       "      <th>pred_1</th>\n",
       "      <th>pred_2</th>\n",
       "      <th>class_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>news</td>\n",
       "      <td>How_ADV to_PART Talk_VERB to_ADP Your_ADJ Teen...</td>\n",
       "      <td>More_ADJ For_ADP parents_NOUN ,_PUNCT talking_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.746912</td>\n",
       "      <td>0.720410</td>\n",
       "      <td>0.267320</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>news</td>\n",
       "      <td>Genius_PROPN Bar_PROPN secrets_NOUN revealed_V...</td>\n",
       "      <td>One_NUM of_ADP the_DET biggest_ADJ things_NOUN...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.598378</td>\n",
       "      <td>0.581579</td>\n",
       "      <td>0.554686</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>news</td>\n",
       "      <td>13_NUM Gifts_NOUN For_ADP People_NOUN Who_NOUN...</td>\n",
       "      <td>Love_NOUN is_VERB n't_ADV all_DET we_PRON need...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.635953</td>\n",
       "      <td>0.620099</td>\n",
       "      <td>0.478591</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>news</td>\n",
       "      <td>Deer_PROPN Debacle_NOUN Exacerbates_VERB New_P...</td>\n",
       "      <td>Desc_PROPN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.416454</td>\n",
       "      <td>0.626502</td>\n",
       "      <td>0.691686</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>news</td>\n",
       "      <td>Donald_PROPN Trump_PROPN Pays_VERB Respects_NO...</td>\n",
       "      <td>President_PROPN Donald_PROPN Trump_PROPN trave...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.712625</td>\n",
       "      <td>0.705318</td>\n",
       "      <td>0.316700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>news</td>\n",
       "      <td>These_DET trapdoor_NOUN waterslides_NOUN plung...</td>\n",
       "      <td>David_PROPN Ibekwe_PROPN Apr._PROPN 12_NUM ,_P...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.634602</td>\n",
       "      <td>0.630173</td>\n",
       "      <td>0.469868</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>news</td>\n",
       "      <td>How_ADV '_PUNCT Jurassic_PROPN World_PROPN '_P...</td>\n",
       "      <td>It_PRON is_VERB well_ADV -_PUNCT known_VERB at...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.720751</td>\n",
       "      <td>0.715298</td>\n",
       "      <td>0.298594</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>news</td>\n",
       "      <td>10_NUM things_NOUN you_PRON need_VERB to_PART ...</td>\n",
       "      <td>Jonathan_PROPN Garber_PROPN ,_PUNCT Business_P...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.759789</td>\n",
       "      <td>0.700965</td>\n",
       "      <td>0.273889</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>news</td>\n",
       "      <td>Regis_PROPN Philbin_PROPN :_PUNCT Trump_PROPN ...</td>\n",
       "      <td>Daytime_ADJ talk_NOUN show_NOUN legend_NOUN Re...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.780099</td>\n",
       "      <td>0.662219</td>\n",
       "      <td>0.292325</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>news</td>\n",
       "      <td>What_NOUN will_VERB change_VERB for_ADP your_A...</td>\n",
       "      <td>More_ADJ Try_VERB Yahoo_PROPN Finance_PROPN on...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.759607</td>\n",
       "      <td>0.702148</td>\n",
       "      <td>0.272887</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>news</td>\n",
       "      <td>Two_NUM -_PUNCT Thirds_PROPN of_ADP Americans_...</td>\n",
       "      <td>More_ADJ Try_VERB Yahoo_PROPN Finance_PROPN on...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.723229</td>\n",
       "      <td>0.688762</td>\n",
       "      <td>0.322651</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>news</td>\n",
       "      <td>BBC_PROPN :_PUNCT Workers_NOUN suing_VERB airl...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0.501834</td>\n",
       "      <td>0.614691</td>\n",
       "      <td>0.618117</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>news</td>\n",
       "      <td>Megyn_PROPN Kelly_PROPN Does_VERB n't_ADV Deny...</td>\n",
       "      <td>Megyn_PROPN Kelly_PROPN has_VERB responded_VER...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.743612</td>\n",
       "      <td>0.714411</td>\n",
       "      <td>0.276619</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>news</td>\n",
       "      <td>Geller_PROPN :_PUNCT Why_ADV Trump_PROPN Is_VE...</td>\n",
       "      <td>Make_VERB no_DET mistake_NOUN :_PUNCT the_DET ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.748755</td>\n",
       "      <td>0.735809</td>\n",
       "      <td>0.250078</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>clickbait</td>\n",
       "      <td>NBC_PROPN NewsVerifizierter_PROPN Account_PROPN</td>\n",
       "      <td>The_DET leading_VERB source_NOUN of_ADP global...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.624125</td>\n",
       "      <td>0.667451</td>\n",
       "      <td>0.443066</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559</th>\n",
       "      <td>news</td>\n",
       "      <td>Saudi_PROPN Arabia_PROPN 's_PART Stock_PROPN M...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0.463860</td>\n",
       "      <td>0.605489</td>\n",
       "      <td>0.665294</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>news</td>\n",
       "      <td>Chugging_VERB Water_PROPN Between_ADP Meals_PR...</td>\n",
       "      <td>The_DET worse_ADJ thing_NOUN we_PRON can_VERB ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.602480</td>\n",
       "      <td>0.600474</td>\n",
       "      <td>0.531688</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>news</td>\n",
       "      <td>Google_PROPN</td>\n",
       "      <td>Search_NOUN without_ADP lifting_VERB a_DET fin...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.514016</td>\n",
       "      <td>0.581575</td>\n",
       "      <td>0.639051</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>news</td>\n",
       "      <td>Gift_PROPN Guide_PROPN :_PUNCT 10_NUM hottest_...</td>\n",
       "      <td>More_ADJ Try_VERB Yahoo_PROPN Finance_PROPN on...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.721736</td>\n",
       "      <td>0.667892</td>\n",
       "      <td>0.345014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1832</th>\n",
       "      <td>news</td>\n",
       "      <td>The_DET Top_PROPN 5_NUM Bollywood_PROPN Films_...</td>\n",
       "      <td>_SPACE Baahubali_PROPN 2_NUM :_PUNCT The_DET ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.708898</td>\n",
       "      <td>0.699883</td>\n",
       "      <td>0.325861</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>news</td>\n",
       "      <td>From_ADP me_PRON to_ADP you_PRON :_PUNCT reade...</td>\n",
       "      <td>For_ADP last_ADJ week_NOUN 's_PART photography...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.680474</td>\n",
       "      <td>0.601090</td>\n",
       "      <td>0.453079</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>news</td>\n",
       "      <td>BBC_PROPN :_PUNCT Even_ADV Liberals_NOUN Are_V...</td>\n",
       "      <td>On_ADP December_PROPN 20_NUM ,_PUNCT the_DET B...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.709533</td>\n",
       "      <td>0.642872</td>\n",
       "      <td>0.382237</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>news</td>\n",
       "      <td>LIVE_ADJ WIRE_PROPN :_PUNCT Macron_PROPN ,_PUN...</td>\n",
       "      <td>Exit_NOUN polls_NOUN have_VERB shown_VERB left...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.703234</td>\n",
       "      <td>0.585002</td>\n",
       "      <td>0.446407</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3395</th>\n",
       "      <td>news</td>\n",
       "      <td>How_ADV well_ADV do_VERB you_PRON know_VERB yo...</td>\n",
       "      <td>You_PRON may_VERB think_VERB you_PRON 've_VERB...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.603811</td>\n",
       "      <td>0.600475</td>\n",
       "      <td>0.530357</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          label                                              title  \\\n",
       "123        news  How_ADV to_PART Talk_VERB to_ADP Your_ADJ Teen...   \n",
       "229        news  Genius_PROPN Bar_PROPN secrets_NOUN revealed_V...   \n",
       "347        news  13_NUM Gifts_NOUN For_ADP People_NOUN Who_NOUN...   \n",
       "391        news  Deer_PROPN Debacle_NOUN Exacerbates_VERB New_P...   \n",
       "490        news  Donald_PROPN Trump_PROPN Pays_VERB Respects_NO...   \n",
       "497        news  These_DET trapdoor_NOUN waterslides_NOUN plung...   \n",
       "694        news  How_ADV '_PUNCT Jurassic_PROPN World_PROPN '_P...   \n",
       "726        news  10_NUM things_NOUN you_PRON need_VERB to_PART ...   \n",
       "754        news  Regis_PROPN Philbin_PROPN :_PUNCT Trump_PROPN ...   \n",
       "991        news  What_NOUN will_VERB change_VERB for_ADP your_A...   \n",
       "1086       news  Two_NUM -_PUNCT Thirds_PROPN of_ADP Americans_...   \n",
       "1104       news  BBC_PROPN :_PUNCT Workers_NOUN suing_VERB airl...   \n",
       "1232       news  Megyn_PROPN Kelly_PROPN Does_VERB n't_ADV Deny...   \n",
       "1244       news  Geller_PROPN :_PUNCT Why_ADV Trump_PROPN Is_VE...   \n",
       "1517  clickbait    NBC_PROPN NewsVerifizierter_PROPN Account_PROPN   \n",
       "1559       news  Saudi_PROPN Arabia_PROPN 's_PART Stock_PROPN M...   \n",
       "1567       news  Chugging_VERB Water_PROPN Between_ADP Meals_PR...   \n",
       "1573       news                                       Google_PROPN   \n",
       "1688       news  Gift_PROPN Guide_PROPN :_PUNCT 10_NUM hottest_...   \n",
       "1832       news  The_DET Top_PROPN 5_NUM Bollywood_PROPN Films_...   \n",
       "1880       news  From_ADP me_PRON to_ADP you_PRON :_PUNCT reade...   \n",
       "1891       news  BBC_PROPN :_PUNCT Even_ADV Liberals_NOUN Are_V...   \n",
       "1963       news  LIVE_ADJ WIRE_PROPN :_PUNCT Macron_PROPN ,_PUN...   \n",
       "3395       news  How_ADV well_ADV do_VERB you_PRON know_VERB yo...   \n",
       "\n",
       "                                                   text  class    pred_0  \\\n",
       "123   More_ADJ For_ADP parents_NOUN ,_PUNCT talking_...      1  0.746912   \n",
       "229   One_NUM of_ADP the_DET biggest_ADJ things_NOUN...      1  0.598378   \n",
       "347   Love_NOUN is_VERB n't_ADV all_DET we_PRON need...      1  0.635953   \n",
       "391                                          Desc_PROPN      1  0.416454   \n",
       "490   President_PROPN Donald_PROPN Trump_PROPN trave...      1  0.712625   \n",
       "497   David_PROPN Ibekwe_PROPN Apr._PROPN 12_NUM ,_P...      1  0.634602   \n",
       "694   It_PRON is_VERB well_ADV -_PUNCT known_VERB at...      1  0.720751   \n",
       "726   Jonathan_PROPN Garber_PROPN ,_PUNCT Business_P...      1  0.759789   \n",
       "754   Daytime_ADJ talk_NOUN show_NOUN legend_NOUN Re...      1  0.780099   \n",
       "991   More_ADJ Try_VERB Yahoo_PROPN Finance_PROPN on...      1  0.759607   \n",
       "1086  More_ADJ Try_VERB Yahoo_PROPN Finance_PROPN on...      1  0.723229   \n",
       "1104                                                         1  0.501834   \n",
       "1232  Megyn_PROPN Kelly_PROPN has_VERB responded_VER...      1  0.743612   \n",
       "1244  Make_VERB no_DET mistake_NOUN :_PUNCT the_DET ...      1  0.748755   \n",
       "1517  The_DET leading_VERB source_NOUN of_ADP global...      0  0.624125   \n",
       "1559                                                         1  0.463860   \n",
       "1567  The_DET worse_ADJ thing_NOUN we_PRON can_VERB ...      1  0.602480   \n",
       "1573  Search_NOUN without_ADP lifting_VERB a_DET fin...      1  0.514016   \n",
       "1688  More_ADJ Try_VERB Yahoo_PROPN Finance_PROPN on...      1  0.721736   \n",
       "1832   _SPACE Baahubali_PROPN 2_NUM :_PUNCT The_DET ...      1  0.708898   \n",
       "1880  For_ADP last_ADJ week_NOUN 's_PART photography...      1  0.680474   \n",
       "1891  On_ADP December_PROPN 20_NUM ,_PUNCT the_DET B...      1  0.709533   \n",
       "1963  Exit_NOUN polls_NOUN have_VERB shown_VERB left...      1  0.703234   \n",
       "3395  You_PRON may_VERB think_VERB you_PRON 've_VERB...      1  0.603811   \n",
       "\n",
       "        pred_1    pred_2  class_pred  \n",
       "123   0.720410  0.267320           0  \n",
       "229   0.581579  0.554686           0  \n",
       "347   0.620099  0.478591           0  \n",
       "391   0.626502  0.691686           2  \n",
       "490   0.705318  0.316700           0  \n",
       "497   0.630173  0.469868           0  \n",
       "694   0.715298  0.298594           0  \n",
       "726   0.700965  0.273889           0  \n",
       "754   0.662219  0.292325           0  \n",
       "991   0.702148  0.272887           0  \n",
       "1086  0.688762  0.322651           0  \n",
       "1104  0.614691  0.618117           2  \n",
       "1232  0.714411  0.276619           0  \n",
       "1244  0.735809  0.250078           0  \n",
       "1517  0.667451  0.443066           1  \n",
       "1559  0.605489  0.665294           2  \n",
       "1567  0.600474  0.531688           0  \n",
       "1573  0.581575  0.639051           2  \n",
       "1688  0.667892  0.345014           0  \n",
       "1832  0.699883  0.325861           0  \n",
       "1880  0.601090  0.453079           0  \n",
       "1891  0.642872  0.382237           0  \n",
       "1963  0.585002  0.446407           0  \n",
       "3395  0.600475  0.530357           0  "
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_df = pd.concat([incorrect_df, \n",
    "                          incorrect_df.title.str.findall(r'_([A-Z]+)').apply(Counter).apply(pd.Series).fillna(0).astype(int)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>pred_0</th>\n",
       "      <th>pred_1</th>\n",
       "      <th>pred_2</th>\n",
       "      <th>class_pred</th>\n",
       "      <th>ADV</th>\n",
       "      <th>PART</th>\n",
       "      <th>...</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>NUM</th>\n",
       "      <th>DET</th>\n",
       "      <th>PRON</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>CCONJ</th>\n",
       "      <th>SYM</th>\n",
       "      <th>SPACE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>news</td>\n",
       "      <td>How_ADV to_PART Talk_VERB to_ADP Your_ADJ Teen...</td>\n",
       "      <td>More_ADJ For_ADP parents_NOUN ,_PUNCT talking_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.746912</td>\n",
       "      <td>0.720410</td>\n",
       "      <td>0.267320</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>news</td>\n",
       "      <td>Genius_PROPN Bar_PROPN secrets_NOUN revealed_V...</td>\n",
       "      <td>One_NUM of_ADP the_DET biggest_ADJ things_NOUN...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.598378</td>\n",
       "      <td>0.581579</td>\n",
       "      <td>0.554686</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>news</td>\n",
       "      <td>13_NUM Gifts_NOUN For_ADP People_NOUN Who_NOUN...</td>\n",
       "      <td>Love_NOUN is_VERB n't_ADV all_DET we_PRON need...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.635953</td>\n",
       "      <td>0.620099</td>\n",
       "      <td>0.478591</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>news</td>\n",
       "      <td>Deer_PROPN Debacle_NOUN Exacerbates_VERB New_P...</td>\n",
       "      <td>Desc_PROPN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.416454</td>\n",
       "      <td>0.626502</td>\n",
       "      <td>0.691686</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>news</td>\n",
       "      <td>Donald_PROPN Trump_PROPN Pays_VERB Respects_NO...</td>\n",
       "      <td>President_PROPN Donald_PROPN Trump_PROPN trave...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.712625</td>\n",
       "      <td>0.705318</td>\n",
       "      <td>0.316700</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>news</td>\n",
       "      <td>These_DET trapdoor_NOUN waterslides_NOUN plung...</td>\n",
       "      <td>David_PROPN Ibekwe_PROPN Apr._PROPN 12_NUM ,_P...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.634602</td>\n",
       "      <td>0.630173</td>\n",
       "      <td>0.469868</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>news</td>\n",
       "      <td>How_ADV '_PUNCT Jurassic_PROPN World_PROPN '_P...</td>\n",
       "      <td>It_PRON is_VERB well_ADV -_PUNCT known_VERB at...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.720751</td>\n",
       "      <td>0.715298</td>\n",
       "      <td>0.298594</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>news</td>\n",
       "      <td>10_NUM things_NOUN you_PRON need_VERB to_PART ...</td>\n",
       "      <td>Jonathan_PROPN Garber_PROPN ,_PUNCT Business_P...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.759789</td>\n",
       "      <td>0.700965</td>\n",
       "      <td>0.273889</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>news</td>\n",
       "      <td>Regis_PROPN Philbin_PROPN :_PUNCT Trump_PROPN ...</td>\n",
       "      <td>Daytime_ADJ talk_NOUN show_NOUN legend_NOUN Re...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.780099</td>\n",
       "      <td>0.662219</td>\n",
       "      <td>0.292325</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>news</td>\n",
       "      <td>What_NOUN will_VERB change_VERB for_ADP your_A...</td>\n",
       "      <td>More_ADJ Try_VERB Yahoo_PROPN Finance_PROPN on...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.759607</td>\n",
       "      <td>0.702148</td>\n",
       "      <td>0.272887</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>news</td>\n",
       "      <td>Two_NUM -_PUNCT Thirds_PROPN of_ADP Americans_...</td>\n",
       "      <td>More_ADJ Try_VERB Yahoo_PROPN Finance_PROPN on...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.723229</td>\n",
       "      <td>0.688762</td>\n",
       "      <td>0.322651</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>news</td>\n",
       "      <td>BBC_PROPN :_PUNCT Workers_NOUN suing_VERB airl...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0.501834</td>\n",
       "      <td>0.614691</td>\n",
       "      <td>0.618117</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>news</td>\n",
       "      <td>Megyn_PROPN Kelly_PROPN Does_VERB n't_ADV Deny...</td>\n",
       "      <td>Megyn_PROPN Kelly_PROPN has_VERB responded_VER...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.743612</td>\n",
       "      <td>0.714411</td>\n",
       "      <td>0.276619</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>news</td>\n",
       "      <td>Geller_PROPN :_PUNCT Why_ADV Trump_PROPN Is_VE...</td>\n",
       "      <td>Make_VERB no_DET mistake_NOUN :_PUNCT the_DET ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.748755</td>\n",
       "      <td>0.735809</td>\n",
       "      <td>0.250078</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>clickbait</td>\n",
       "      <td>NBC_PROPN NewsVerifizierter_PROPN Account_PROPN</td>\n",
       "      <td>The_DET leading_VERB source_NOUN of_ADP global...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.624125</td>\n",
       "      <td>0.667451</td>\n",
       "      <td>0.443066</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559</th>\n",
       "      <td>news</td>\n",
       "      <td>Saudi_PROPN Arabia_PROPN 's_PART Stock_PROPN M...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0.463860</td>\n",
       "      <td>0.605489</td>\n",
       "      <td>0.665294</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>news</td>\n",
       "      <td>Chugging_VERB Water_PROPN Between_ADP Meals_PR...</td>\n",
       "      <td>The_DET worse_ADJ thing_NOUN we_PRON can_VERB ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.602480</td>\n",
       "      <td>0.600474</td>\n",
       "      <td>0.531688</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>news</td>\n",
       "      <td>Google_PROPN</td>\n",
       "      <td>Search_NOUN without_ADP lifting_VERB a_DET fin...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.514016</td>\n",
       "      <td>0.581575</td>\n",
       "      <td>0.639051</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>news</td>\n",
       "      <td>Gift_PROPN Guide_PROPN :_PUNCT 10_NUM hottest_...</td>\n",
       "      <td>More_ADJ Try_VERB Yahoo_PROPN Finance_PROPN on...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.721736</td>\n",
       "      <td>0.667892</td>\n",
       "      <td>0.345014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1832</th>\n",
       "      <td>news</td>\n",
       "      <td>The_DET Top_PROPN 5_NUM Bollywood_PROPN Films_...</td>\n",
       "      <td>_SPACE Baahubali_PROPN 2_NUM :_PUNCT The_DET ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.708898</td>\n",
       "      <td>0.699883</td>\n",
       "      <td>0.325861</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>news</td>\n",
       "      <td>From_ADP me_PRON to_ADP you_PRON :_PUNCT reade...</td>\n",
       "      <td>For_ADP last_ADJ week_NOUN 's_PART photography...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.680474</td>\n",
       "      <td>0.601090</td>\n",
       "      <td>0.453079</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>news</td>\n",
       "      <td>BBC_PROPN :_PUNCT Even_ADV Liberals_NOUN Are_V...</td>\n",
       "      <td>On_ADP December_PROPN 20_NUM ,_PUNCT the_DET B...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.709533</td>\n",
       "      <td>0.642872</td>\n",
       "      <td>0.382237</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>news</td>\n",
       "      <td>LIVE_ADJ WIRE_PROPN :_PUNCT Macron_PROPN ,_PUN...</td>\n",
       "      <td>Exit_NOUN polls_NOUN have_VERB shown_VERB left...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.703234</td>\n",
       "      <td>0.585002</td>\n",
       "      <td>0.446407</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3395</th>\n",
       "      <td>news</td>\n",
       "      <td>How_ADV well_ADV do_VERB you_PRON know_VERB yo...</td>\n",
       "      <td>You_PRON may_VERB think_VERB you_PRON 've_VERB...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.603811</td>\n",
       "      <td>0.600475</td>\n",
       "      <td>0.530357</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24 rows × 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          label                                              title  \\\n",
       "123        news  How_ADV to_PART Talk_VERB to_ADP Your_ADJ Teen...   \n",
       "229        news  Genius_PROPN Bar_PROPN secrets_NOUN revealed_V...   \n",
       "347        news  13_NUM Gifts_NOUN For_ADP People_NOUN Who_NOUN...   \n",
       "391        news  Deer_PROPN Debacle_NOUN Exacerbates_VERB New_P...   \n",
       "490        news  Donald_PROPN Trump_PROPN Pays_VERB Respects_NO...   \n",
       "497        news  These_DET trapdoor_NOUN waterslides_NOUN plung...   \n",
       "694        news  How_ADV '_PUNCT Jurassic_PROPN World_PROPN '_P...   \n",
       "726        news  10_NUM things_NOUN you_PRON need_VERB to_PART ...   \n",
       "754        news  Regis_PROPN Philbin_PROPN :_PUNCT Trump_PROPN ...   \n",
       "991        news  What_NOUN will_VERB change_VERB for_ADP your_A...   \n",
       "1086       news  Two_NUM -_PUNCT Thirds_PROPN of_ADP Americans_...   \n",
       "1104       news  BBC_PROPN :_PUNCT Workers_NOUN suing_VERB airl...   \n",
       "1232       news  Megyn_PROPN Kelly_PROPN Does_VERB n't_ADV Deny...   \n",
       "1244       news  Geller_PROPN :_PUNCT Why_ADV Trump_PROPN Is_VE...   \n",
       "1517  clickbait    NBC_PROPN NewsVerifizierter_PROPN Account_PROPN   \n",
       "1559       news  Saudi_PROPN Arabia_PROPN 's_PART Stock_PROPN M...   \n",
       "1567       news  Chugging_VERB Water_PROPN Between_ADP Meals_PR...   \n",
       "1573       news                                       Google_PROPN   \n",
       "1688       news  Gift_PROPN Guide_PROPN :_PUNCT 10_NUM hottest_...   \n",
       "1832       news  The_DET Top_PROPN 5_NUM Bollywood_PROPN Films_...   \n",
       "1880       news  From_ADP me_PRON to_ADP you_PRON :_PUNCT reade...   \n",
       "1891       news  BBC_PROPN :_PUNCT Even_ADV Liberals_NOUN Are_V...   \n",
       "1963       news  LIVE_ADJ WIRE_PROPN :_PUNCT Macron_PROPN ,_PUN...   \n",
       "3395       news  How_ADV well_ADV do_VERB you_PRON know_VERB yo...   \n",
       "\n",
       "                                                   text  class    pred_0  \\\n",
       "123   More_ADJ For_ADP parents_NOUN ,_PUNCT talking_...      1  0.746912   \n",
       "229   One_NUM of_ADP the_DET biggest_ADJ things_NOUN...      1  0.598378   \n",
       "347   Love_NOUN is_VERB n't_ADV all_DET we_PRON need...      1  0.635953   \n",
       "391                                          Desc_PROPN      1  0.416454   \n",
       "490   President_PROPN Donald_PROPN Trump_PROPN trave...      1  0.712625   \n",
       "497   David_PROPN Ibekwe_PROPN Apr._PROPN 12_NUM ,_P...      1  0.634602   \n",
       "694   It_PRON is_VERB well_ADV -_PUNCT known_VERB at...      1  0.720751   \n",
       "726   Jonathan_PROPN Garber_PROPN ,_PUNCT Business_P...      1  0.759789   \n",
       "754   Daytime_ADJ talk_NOUN show_NOUN legend_NOUN Re...      1  0.780099   \n",
       "991   More_ADJ Try_VERB Yahoo_PROPN Finance_PROPN on...      1  0.759607   \n",
       "1086  More_ADJ Try_VERB Yahoo_PROPN Finance_PROPN on...      1  0.723229   \n",
       "1104                                                         1  0.501834   \n",
       "1232  Megyn_PROPN Kelly_PROPN has_VERB responded_VER...      1  0.743612   \n",
       "1244  Make_VERB no_DET mistake_NOUN :_PUNCT the_DET ...      1  0.748755   \n",
       "1517  The_DET leading_VERB source_NOUN of_ADP global...      0  0.624125   \n",
       "1559                                                         1  0.463860   \n",
       "1567  The_DET worse_ADJ thing_NOUN we_PRON can_VERB ...      1  0.602480   \n",
       "1573  Search_NOUN without_ADP lifting_VERB a_DET fin...      1  0.514016   \n",
       "1688  More_ADJ Try_VERB Yahoo_PROPN Finance_PROPN on...      1  0.721736   \n",
       "1832   _SPACE Baahubali_PROPN 2_NUM :_PUNCT The_DET ...      1  0.708898   \n",
       "1880  For_ADP last_ADJ week_NOUN 's_PART photography...      1  0.680474   \n",
       "1891  On_ADP December_PROPN 20_NUM ,_PUNCT the_DET B...      1  0.709533   \n",
       "1963  Exit_NOUN polls_NOUN have_VERB shown_VERB left...      1  0.703234   \n",
       "3395  You_PRON may_VERB think_VERB you_PRON 've_VERB...      1  0.603811   \n",
       "\n",
       "        pred_1    pred_2  class_pred  ADV  PART  ...  NOUN  PROPN  PUNCT  NUM  \\\n",
       "123   0.720410  0.267320           0    1     1  ...     2      1      2    0   \n",
       "229   0.581579  0.554686           0    0     0  ...     3      2      1    0   \n",
       "347   0.620099  0.478591           0    0     0  ...     4      0      0    1   \n",
       "391   0.626502  0.691686           2    0     0  ...     1      5      0    0   \n",
       "490   0.705318  0.316700           0    0     0  ...     1      6      0    0   \n",
       "497   0.630173  0.469868           0    0     0  ...     3      0      1    0   \n",
       "694   0.715298  0.298594           0    1     1  ...     4      2      2    0   \n",
       "726   0.700965  0.273889           0    0     1  ...     2      0      0    1   \n",
       "754   0.662219  0.292325           0    0     0  ...     2      4      5    0   \n",
       "991   0.702148  0.272887           0    0     0  ...     2      0      0    1   \n",
       "1086  0.688762  0.322651           0    1     0  ...     1      2      2    2   \n",
       "1104  0.614691  0.618117           2    0     0  ...     3      3      4    0   \n",
       "1232  0.714411  0.276619           0    1     0  ...     0      5      0    0   \n",
       "1244  0.735809  0.250078           0    1     0  ...     0      2      1    0   \n",
       "1517  0.667451  0.443066           1    0     0  ...     0      3      0    0   \n",
       "1559  0.605489  0.665294           2    0     1  ...     0      5      0    0   \n",
       "1567  0.600474  0.531688           0    0     0  ...     2      2      1    0   \n",
       "1573  0.581575  0.639051           2    0     0  ...     0      1      0    0   \n",
       "1688  0.667892  0.345014           0    0     0  ...     1      2      1    2   \n",
       "1832  0.699883  0.325861           0    1     0  ...     1      3      0    1   \n",
       "1880  0.601090  0.453079           0    0     1  ...     4      0      1    0   \n",
       "1891  0.642872  0.382237           0    1     0  ...     1      3      1    0   \n",
       "1963  0.585002  0.446407           0    0     0  ...     1      7      2    0   \n",
       "3395  0.600475  0.530357           0    2     0  ...     2      0      2    0   \n",
       "\n",
       "      DET  PRON  INTJ  CCONJ  SYM  SPACE  \n",
       "123     0     0     0      0    0      0  \n",
       "229     0     0     0      0    0      0  \n",
       "347     0     0     0      0    0      0  \n",
       "391     0     0     0      0    0      0  \n",
       "490     0     0     0      0    0      0  \n",
       "497     2     1     0      0    0      0  \n",
       "694     0     0     0      0    0      0  \n",
       "726     1     1     0      0    0      0  \n",
       "754     0     0     1      1    0      0  \n",
       "991     0     0     0      0    0      0  \n",
       "1086    0     0     0      0    0      0  \n",
       "1104    0     0     0      0    0      0  \n",
       "1232    0     0     0      0    0      0  \n",
       "1244    0     0     0      0    0      0  \n",
       "1517    0     0     0      0    0      0  \n",
       "1559    0     0     0      0    0      0  \n",
       "1567    0     1     0      0    0      0  \n",
       "1573    0     0     0      0    0      0  \n",
       "1688    0     0     0      0    1      0  \n",
       "1832    2     1     0      0    0      0  \n",
       "1880    1     2     0      0    0      0  \n",
       "1891    0     0     0      0    0      0  \n",
       "1963    0     0     0      0    0      0  \n",
       "3395    0     1     0      0    0      1  \n",
       "\n",
       "[24 rows x 116 columns]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.div()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pos_train_df = train_df.title.str.findall(r'_([A-Z]+)').apply(Counter).apply(pd.Series).fillna(0).div(train_df.title.str.len()).astype(int)\n",
    "pos_train_df['class'] = X_train['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(pos_train_df.groupby('class').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
