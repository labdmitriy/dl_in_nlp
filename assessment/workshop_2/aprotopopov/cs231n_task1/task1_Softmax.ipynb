{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "In this exercise you will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n",
    "\n",
    "Credit: [cs231n.stanford.edu](http://cs231n.stanford.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T11:30:59.003251Z",
     "start_time": "2019-03-18T11:30:58.721997Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T11:31:00.779466Z",
     "start_time": "2019-03-18T11:30:59.004938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "    del X_train, y_train\n",
    "    del X_test, y_test\n",
    "    print('Clear previously loaded data.')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py** and **cs231n/classifiers/linear_classifier.py**\n",
    "\n",
    "### Subtask 1\n",
    "First implement the naive softmax loss function with nested loops.\n",
    "Open the file cs231n/classifiers/softmax.py and implement the\n",
    "softmax_loss_naive function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T11:31:01.016324Z",
     "start_time": "2019-03-18T11:31:00.781112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.333156\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline Question (1 point)**:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.\n",
    "\n",
    "**Your answer:** *Because we assume that all classes are equally distributed and equation under the log will close to 1/10 = 0.1*\n",
    "\n",
    "\n",
    "### Subtask 2 (1 point)\n",
    "Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "version of the gradient that _uses nested loops_.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T11:31:06.956455Z",
     "start_time": "2019-03-18T11:31:01.018288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 1.059371 analytic: 1.059371, relative error: 4.776869e-08\n",
      "numerical: 1.186536 analytic: 1.186536, relative error: 4.714626e-08\n",
      "numerical: -1.381361 analytic: -1.381361, relative error: 8.285807e-09\n",
      "numerical: -2.614433 analytic: -2.614433, relative error: 8.792620e-10\n",
      "numerical: 1.510827 analytic: 1.510827, relative error: 9.636188e-09\n",
      "numerical: 2.834456 analytic: 2.834456, relative error: 1.536377e-08\n",
      "numerical: 2.369750 analytic: 2.369750, relative error: 5.695866e-10\n",
      "numerical: 0.447312 analytic: 0.447312, relative error: 2.909212e-09\n",
      "numerical: -0.856729 analytic: -0.856729, relative error: 3.070507e-10\n",
      "numerical: -0.253022 analytic: -0.253022, relative error: 3.433638e-08\n",
      "numerical: -3.278925 analytic: -3.270607, relative error: 1.270082e-03\n",
      "numerical: 0.628888 analytic: 0.633238, relative error: 3.446761e-03\n",
      "numerical: -0.810485 analytic: -0.810223, relative error: 1.613939e-04\n",
      "numerical: 1.642365 analytic: 1.644563, relative error: 6.686129e-04\n",
      "numerical: 1.282655 analytic: 1.283176, relative error: 2.030954e-04\n",
      "numerical: 0.384183 analytic: 0.382293, relative error: 2.465785e-03\n",
      "numerical: 1.743786 analytic: 1.745171, relative error: 3.968968e-04\n",
      "numerical: -2.130637 analytic: -2.126580, relative error: 9.530133e-04\n",
      "numerical: 1.585690 analytic: 1.591787, relative error: 1.919055e-03\n",
      "numerical: -3.150907 analytic: -3.150693, relative error: 3.402145e-05\n"
     ]
    }
   ],
   "source": [
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# Use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# Do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 3 (4 points)\n",
    "\n",
    "Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "implement a vectorized version in softmax_loss_vectorized.\n",
    "The two versions should compute the same results, but the vectorized version should be\n",
    "much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T11:31:07.140920Z",
     "start_time": "2019-03-18T11:31:06.958372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.333156e+00 computed in 0.163232s\n",
      "vectorized loss: 2.333156e+00 computed in 0.004153s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# We use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 4 (2 points)\n",
    "\n",
    "We now have vectorized and efficient expressions for the loss, the gradient and our gradient matches the numerical gradient. We are therefore ready to do SGD to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T11:31:18.325451Z",
     "start_time": "2019-03-18T11:31:07.142788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 777.875260\n",
      "iteration 100 / 1500: loss 470.959015\n",
      "iteration 200 / 1500: loss 285.479126\n",
      "iteration 300 / 1500: loss 173.454179\n",
      "iteration 400 / 1500: loss 105.684720\n",
      "iteration 500 / 1500: loss 64.792233\n",
      "iteration 600 / 1500: loss 39.996263\n",
      "iteration 700 / 1500: loss 25.008494\n",
      "iteration 800 / 1500: loss 15.997851\n",
      "iteration 900 / 1500: loss 10.552661\n",
      "iteration 1000 / 1500: loss 7.202392\n",
      "iteration 1100 / 1500: loss 5.178421\n",
      "iteration 1200 / 1500: loss 4.032343\n",
      "iteration 1300 / 1500: loss 3.282735\n",
      "iteration 1400 / 1500: loss 2.827498\n",
      "That took 11.171265s\n"
     ]
    }
   ],
   "source": [
    "# In the file linear_classifier.py, implement SGD in the function\n",
    "# LinearClassifier.train() and then run it with the code below.\n",
    "from cs231n.classifiers import Softmax\n",
    "model = Softmax()\n",
    "tic = time.time()\n",
    "loss_hist = model.train(X_train, y_train, learning_rate=1e-7, reg=2.5e4,\n",
    "                        num_iters=1500, verbose=True)\n",
    "toc = time.time()\n",
    "print('That took %fs' % (toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T11:31:18.479620Z",
     "start_time": "2019-03-18T11:31:18.327075Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A useful debugging strategy is to plot the loss as a function of\n",
    "# iteration number:\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T14:15:46.311366Z",
     "start_time": "2019-03-18T14:15:46.170840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.349143\n",
      "validation accuracy: 0.356000\n"
     ]
    }
   ],
   "source": [
    "# Write the LinearClassifier.predict function and evaluate the performance on both the\n",
    "# training and validation set\n",
    "y_train_pred = model.predict(X_train)\n",
    "print('training accuracy: %f' % (np.mean(y_train == y_train_pred), ))\n",
    "y_val_pred = model.predict(X_val)\n",
    "print('validation accuracy: %f' % (np.mean(y_val == y_val_pred), ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 5 (2 points)\n",
    "\n",
    "Use the validation set to tune hyperparameters (regularization strength and\n",
    "learning rate). You should experiment with different ranges for the learning\n",
    "rates and regularization strengths; if you are careful you should be able to\n",
    "get a classification accuracy of over 0.35 on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T14:43:28.283439Z",
     "start_time": "2019-03-18T14:15:46.316013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-09 100.0\n",
      "iteration 0 / 1500: loss 7.749648\n",
      "iteration 100 / 1500: loss 8.060811\n",
      "iteration 200 / 1500: loss 8.373218\n",
      "iteration 300 / 1500: loss 7.852227\n",
      "iteration 400 / 1500: loss 7.580586\n",
      "iteration 500 / 1500: loss 7.964782\n",
      "iteration 600 / 1500: loss 8.072543\n",
      "iteration 700 / 1500: loss 7.898639\n",
      "iteration 800 / 1500: loss 7.844779\n",
      "iteration 900 / 1500: loss 8.210068\n",
      "iteration 1000 / 1500: loss 8.540487\n",
      "iteration 1100 / 1500: loss 7.655335\n",
      "iteration 1200 / 1500: loss 7.862290\n",
      "iteration 1300 / 1500: loss 8.444639\n",
      "iteration 1400 / 1500: loss 8.126135\n",
      "1e-09 215.44346900318845\n",
      "iteration 0 / 1500: loss 12.256791\n",
      "iteration 100 / 1500: loss 12.020696\n",
      "iteration 200 / 1500: loss 12.513856\n",
      "iteration 300 / 1500: loss 11.465527\n",
      "iteration 400 / 1500: loss 11.936687\n",
      "iteration 500 / 1500: loss 11.571541\n",
      "iteration 600 / 1500: loss 12.458924\n",
      "iteration 700 / 1500: loss 11.815528\n",
      "iteration 800 / 1500: loss 11.294116\n",
      "iteration 900 / 1500: loss 12.428066\n",
      "iteration 1000 / 1500: loss 11.531676\n",
      "iteration 1100 / 1500: loss 11.566940\n",
      "iteration 1200 / 1500: loss 11.932666\n",
      "iteration 1300 / 1500: loss 11.538550\n",
      "iteration 1400 / 1500: loss 11.573608\n",
      "1e-09 464.15888336127773\n",
      "iteration 0 / 1500: loss 20.309011\n",
      "iteration 100 / 1500: loss 20.362231\n",
      "iteration 200 / 1500: loss 20.906221\n",
      "iteration 300 / 1500: loss 19.916782\n",
      "iteration 400 / 1500: loss 20.343818\n",
      "iteration 500 / 1500: loss 20.231925\n",
      "iteration 600 / 1500: loss 20.033563\n",
      "iteration 700 / 1500: loss 20.277677\n",
      "iteration 800 / 1500: loss 19.939711\n",
      "iteration 900 / 1500: loss 20.187209\n",
      "iteration 1000 / 1500: loss 20.125897\n",
      "iteration 1100 / 1500: loss 19.781361\n",
      "iteration 1200 / 1500: loss 19.787671\n",
      "iteration 1300 / 1500: loss 19.967314\n",
      "iteration 1400 / 1500: loss 19.743450\n",
      "1e-09 1000.0\n",
      "iteration 0 / 1500: loss 36.999826\n",
      "iteration 100 / 1500: loss 37.066482\n",
      "iteration 200 / 1500: loss 37.712077\n",
      "iteration 300 / 1500: loss 37.331561\n",
      "iteration 400 / 1500: loss 36.732756\n",
      "iteration 500 / 1500: loss 36.970750\n",
      "iteration 600 / 1500: loss 37.068314\n",
      "iteration 700 / 1500: loss 36.583687\n",
      "iteration 800 / 1500: loss 37.056758\n",
      "iteration 900 / 1500: loss 37.700489\n",
      "iteration 1000 / 1500: loss 36.808314\n",
      "iteration 1100 / 1500: loss 37.288201\n",
      "iteration 1200 / 1500: loss 37.067109\n",
      "iteration 1300 / 1500: loss 36.415088\n",
      "iteration 1400 / 1500: loss 36.709502\n",
      "1e-09 2154.4346900318824\n",
      "iteration 0 / 1500: loss 71.221805\n",
      "iteration 100 / 1500: loss 71.614929\n",
      "iteration 200 / 1500: loss 71.772075\n",
      "iteration 300 / 1500: loss 71.448228\n",
      "iteration 400 / 1500: loss 70.769934\n",
      "iteration 500 / 1500: loss 71.185916\n",
      "iteration 600 / 1500: loss 71.098465\n",
      "iteration 700 / 1500: loss 71.183546\n",
      "iteration 800 / 1500: loss 70.638108\n",
      "iteration 900 / 1500: loss 70.648181\n",
      "iteration 1000 / 1500: loss 70.796422\n",
      "iteration 1100 / 1500: loss 70.776606\n",
      "iteration 1200 / 1500: loss 70.889745\n",
      "iteration 1300 / 1500: loss 70.944073\n",
      "iteration 1400 / 1500: loss 70.907511\n",
      "1e-09 4641.588833612777\n",
      "iteration 0 / 1500: loss 149.219334\n",
      "iteration 100 / 1500: loss 149.167289\n",
      "iteration 200 / 1500: loss 148.786842\n",
      "iteration 300 / 1500: loss 148.872956\n",
      "iteration 400 / 1500: loss 148.556459\n",
      "iteration 500 / 1500: loss 148.140884\n",
      "iteration 600 / 1500: loss 148.444778\n",
      "iteration 700 / 1500: loss 148.040849\n",
      "iteration 800 / 1500: loss 147.801433\n",
      "iteration 900 / 1500: loss 147.542124\n",
      "iteration 1000 / 1500: loss 147.759838\n",
      "iteration 1100 / 1500: loss 147.430902\n",
      "iteration 1200 / 1500: loss 147.515278\n",
      "iteration 1300 / 1500: loss 146.866140\n",
      "iteration 1400 / 1500: loss 147.397564\n",
      "1e-09 10000.0\n",
      "iteration 0 / 1500: loss 314.176522\n",
      "iteration 100 / 1500: loss 314.598012\n",
      "iteration 200 / 1500: loss 313.267910\n",
      "iteration 300 / 1500: loss 312.871929\n",
      "iteration 400 / 1500: loss 311.958827\n",
      "iteration 500 / 1500: loss 311.565579\n",
      "iteration 600 / 1500: loss 310.711541\n",
      "iteration 700 / 1500: loss 309.997994\n",
      "iteration 800 / 1500: loss 309.509789\n",
      "iteration 900 / 1500: loss 309.097619\n",
      "iteration 1000 / 1500: loss 307.983521\n",
      "iteration 1100 / 1500: loss 307.076110\n",
      "iteration 1200 / 1500: loss 307.034192\n",
      "iteration 1300 / 1500: loss 306.341394\n",
      "iteration 1400 / 1500: loss 305.692025\n",
      "1e-09 21544.346900318822\n",
      "iteration 0 / 1500: loss 653.279929\n",
      "iteration 100 / 1500: loss 650.753543\n",
      "iteration 200 / 1500: loss 647.487845\n",
      "iteration 300 / 1500: loss 644.782099\n",
      "iteration 400 / 1500: loss 642.165967\n",
      "iteration 500 / 1500: loss 639.051523\n",
      "iteration 600 / 1500: loss 636.781470\n",
      "iteration 700 / 1500: loss 633.733958\n",
      "iteration 800 / 1500: loss 630.738277\n",
      "iteration 900 / 1500: loss 628.453032\n",
      "iteration 1000 / 1500: loss 625.401960\n",
      "iteration 1100 / 1500: loss 622.942202\n",
      "iteration 1200 / 1500: loss 620.238024\n",
      "iteration 1300 / 1500: loss 617.690659\n",
      "iteration 1400 / 1500: loss 614.971485\n",
      "1e-09 46415.888336127726\n",
      "iteration 0 / 1500: loss 1431.077224\n",
      "iteration 100 / 1500: loss 1417.842423\n",
      "iteration 200 / 1500: loss 1404.693187\n",
      "iteration 300 / 1500: loss 1391.544364\n",
      "iteration 400 / 1500: loss 1378.760262\n",
      "iteration 500 / 1500: loss 1366.214215\n",
      "iteration 600 / 1500: loss 1353.571626\n",
      "iteration 700 / 1500: loss 1340.944480\n",
      "iteration 800 / 1500: loss 1328.424744\n",
      "iteration 900 / 1500: loss 1316.190479\n",
      "iteration 1000 / 1500: loss 1303.560341\n",
      "iteration 1100 / 1500: loss 1291.487430\n",
      "iteration 1200 / 1500: loss 1279.659767\n",
      "iteration 1300 / 1500: loss 1267.647306\n",
      "iteration 1400 / 1500: loss 1255.886548\n",
      "1e-09 100000.0\n",
      "iteration 0 / 1500: loss 3099.929877\n",
      "iteration 100 / 1500: loss 3038.214566\n",
      "iteration 200 / 1500: loss 2977.774319\n",
      "iteration 300 / 1500: loss 2919.141331\n",
      "iteration 400 / 1500: loss 2861.133735\n",
      "iteration 500 / 1500: loss 2804.338223\n",
      "iteration 600 / 1500: loss 2748.553746\n",
      "iteration 700 / 1500: loss 2694.205662\n",
      "iteration 800 / 1500: loss 2640.685404\n",
      "iteration 900 / 1500: loss 2588.261998\n",
      "iteration 1000 / 1500: loss 2537.117626\n",
      "iteration 1100 / 1500: loss 2486.132322\n",
      "iteration 1200 / 1500: loss 2437.394098\n",
      "iteration 1300 / 1500: loss 2389.400875\n",
      "iteration 1400 / 1500: loss 2342.087491\n",
      "1e-09 215443.46900318822\n",
      "iteration 0 / 1500: loss 6696.003400\n",
      "iteration 100 / 1500: loss 6413.250345\n",
      "iteration 200 / 1500: loss 6143.081015\n",
      "iteration 300 / 1500: loss 5883.346789\n",
      "iteration 400 / 1500: loss 5635.246211\n",
      "iteration 500 / 1500: loss 5397.673096\n",
      "iteration 600 / 1500: loss 5169.789592\n",
      "iteration 700 / 1500: loss 4951.733715\n",
      "iteration 800 / 1500: loss 4742.973197\n",
      "iteration 900 / 1500: loss 4542.824008\n",
      "iteration 1000 / 1500: loss 4351.556695\n",
      "iteration 1100 / 1500: loss 4167.860857\n",
      "iteration 1200 / 1500: loss 3991.865907\n",
      "iteration 1300 / 1500: loss 3823.447380\n",
      "iteration 1400 / 1500: loss 3662.112526\n",
      "1e-09 464158.8833612772\n",
      "iteration 0 / 1500: loss 14382.132245\n",
      "iteration 100 / 1500: loss 13106.604732\n",
      "iteration 200 / 1500: loss 11944.278851\n",
      "iteration 300 / 1500: loss 10885.345795\n",
      "iteration 400 / 1500: loss 9919.926352\n",
      "iteration 500 / 1500: loss 9040.183418\n",
      "iteration 600 / 1500: loss 8238.682983\n",
      "iteration 700 / 1500: loss 7508.137114\n",
      "iteration 800 / 1500: loss 6842.288163\n",
      "iteration 900 / 1500: loss 6235.389102\n",
      "iteration 1000 / 1500: loss 5682.709608\n",
      "iteration 1100 / 1500: loss 5178.723415\n",
      "iteration 1200 / 1500: loss 4719.551662\n",
      "iteration 1300 / 1500: loss 4301.295219\n",
      "iteration 1400 / 1500: loss 3919.948033\n",
      "1e-09 1000000.0\n",
      "iteration 0 / 1500: loss 30790.099177\n",
      "iteration 100 / 1500: loss 25205.637236\n",
      "iteration 200 / 1500: loss 20634.385470\n",
      "iteration 300 / 1500: loss 16892.636509\n",
      "iteration 400 / 1500: loss 13828.932856\n",
      "iteration 500 / 1500: loss 11321.162208\n",
      "iteration 600 / 1500: loss 9267.793985\n",
      "iteration 700 / 1500: loss 7587.453590\n",
      "iteration 800 / 1500: loss 6211.295743\n",
      "iteration 900 / 1500: loss 5084.946611\n",
      "iteration 1000 / 1500: loss 4163.114243\n",
      "iteration 1100 / 1500: loss 3408.325975\n",
      "iteration 1200 / 1500: loss 2790.621953\n",
      "iteration 1300 / 1500: loss 2284.694624\n",
      "iteration 1400 / 1500: loss 1870.738579\n",
      "3.1622776601683795e-09 100.0\n",
      "iteration 0 / 1500: loss 9.224930\n",
      "iteration 100 / 1500: loss 9.556495\n",
      "iteration 200 / 1500: loss 9.116945\n",
      "iteration 300 / 1500: loss 8.966560\n",
      "iteration 400 / 1500: loss 9.054575\n",
      "iteration 500 / 1500: loss 8.679791\n",
      "iteration 600 / 1500: loss 8.243468\n",
      "iteration 700 / 1500: loss 8.264214\n",
      "iteration 800 / 1500: loss 8.241166\n",
      "iteration 900 / 1500: loss 7.852129\n",
      "iteration 1000 / 1500: loss 8.359334\n",
      "iteration 1100 / 1500: loss 8.393647\n",
      "iteration 1200 / 1500: loss 7.925304\n",
      "iteration 1300 / 1500: loss 7.707027\n",
      "iteration 1400 / 1500: loss 7.634132\n",
      "3.1622776601683795e-09 215.44346900318845\n",
      "iteration 0 / 1500: loss 12.539276\n",
      "iteration 100 / 1500: loss 11.977580\n",
      "iteration 200 / 1500: loss 12.309696\n",
      "iteration 300 / 1500: loss 12.254784\n",
      "iteration 400 / 1500: loss 11.971184\n",
      "iteration 500 / 1500: loss 11.640450\n",
      "iteration 600 / 1500: loss 11.699237\n",
      "iteration 700 / 1500: loss 11.715406\n",
      "iteration 800 / 1500: loss 11.471477\n",
      "iteration 900 / 1500: loss 11.772003\n",
      "iteration 1000 / 1500: loss 11.616230\n",
      "iteration 1100 / 1500: loss 11.556531\n",
      "iteration 1200 / 1500: loss 11.028230\n",
      "iteration 1300 / 1500: loss 11.403520\n",
      "iteration 1400 / 1500: loss 10.990469\n",
      "3.1622776601683795e-09 464.15888336127773\n",
      "iteration 0 / 1500: loss 19.304744\n",
      "iteration 100 / 1500: loss 19.364688\n",
      "iteration 200 / 1500: loss 19.047399\n",
      "iteration 300 / 1500: loss 18.873795\n",
      "iteration 400 / 1500: loss 19.061112\n",
      "iteration 500 / 1500: loss 18.971438\n",
      "iteration 600 / 1500: loss 18.648176\n",
      "iteration 700 / 1500: loss 19.072991\n",
      "iteration 800 / 1500: loss 18.549039\n",
      "iteration 900 / 1500: loss 18.700117\n",
      "iteration 1000 / 1500: loss 18.478646\n",
      "iteration 1100 / 1500: loss 18.519077\n",
      "iteration 1200 / 1500: loss 18.415412\n",
      "iteration 1300 / 1500: loss 18.207664\n",
      "iteration 1400 / 1500: loss 18.536893\n",
      "3.1622776601683795e-09 1000.0\n",
      "iteration 0 / 1500: loss 35.676586\n",
      "iteration 100 / 1500: loss 35.752894\n",
      "iteration 200 / 1500: loss 35.723966\n",
      "iteration 300 / 1500: loss 35.567397\n",
      "iteration 400 / 1500: loss 35.584480\n",
      "iteration 500 / 1500: loss 35.615800\n",
      "iteration 600 / 1500: loss 35.008009\n",
      "iteration 700 / 1500: loss 35.100890\n",
      "iteration 800 / 1500: loss 35.549398\n",
      "iteration 900 / 1500: loss 35.198171\n",
      "iteration 1000 / 1500: loss 34.751325\n",
      "iteration 1100 / 1500: loss 35.190182\n",
      "iteration 1200 / 1500: loss 35.079357\n",
      "iteration 1300 / 1500: loss 35.070525\n",
      "iteration 1400 / 1500: loss 34.716054\n",
      "3.1622776601683795e-09 2154.4346900318824\n",
      "iteration 0 / 1500: loss 72.488163\n",
      "iteration 100 / 1500: loss 72.207421\n",
      "iteration 200 / 1500: loss 72.008385\n",
      "iteration 300 / 1500: loss 71.806790\n",
      "iteration 400 / 1500: loss 71.677409\n",
      "iteration 500 / 1500: loss 71.781859\n",
      "iteration 600 / 1500: loss 71.170915\n",
      "iteration 700 / 1500: loss 70.945669\n",
      "iteration 800 / 1500: loss 70.813788\n",
      "iteration 900 / 1500: loss 70.847925\n",
      "iteration 1000 / 1500: loss 70.289367\n",
      "iteration 1100 / 1500: loss 70.524200\n",
      "iteration 1200 / 1500: loss 69.873010\n",
      "iteration 1300 / 1500: loss 70.292375\n",
      "iteration 1400 / 1500: loss 69.553800\n",
      "3.1622776601683795e-09 4641.588833612777\n",
      "iteration 0 / 1500: loss 149.097184\n",
      "iteration 100 / 1500: loss 148.708276\n",
      "iteration 200 / 1500: loss 148.502610\n",
      "iteration 300 / 1500: loss 147.999771\n",
      "iteration 400 / 1500: loss 147.208804\n",
      "iteration 500 / 1500: loss 146.799894\n",
      "iteration 600 / 1500: loss 146.178288\n",
      "iteration 700 / 1500: loss 145.495683\n",
      "iteration 800 / 1500: loss 145.237226\n",
      "iteration 900 / 1500: loss 144.510890\n",
      "iteration 1000 / 1500: loss 144.211575\n",
      "iteration 1100 / 1500: loss 143.427356\n",
      "iteration 1200 / 1500: loss 143.486930\n",
      "iteration 1300 / 1500: loss 142.922849\n",
      "iteration 1400 / 1500: loss 142.534986\n",
      "3.1622776601683795e-09 10000.0\n",
      "iteration 0 / 1500: loss 316.408959\n",
      "iteration 100 / 1500: loss 314.330597\n",
      "iteration 200 / 1500: loss 312.576907\n",
      "iteration 300 / 1500: loss 310.541983\n",
      "iteration 400 / 1500: loss 308.385908\n",
      "iteration 500 / 1500: loss 306.073662\n",
      "iteration 600 / 1500: loss 304.514866\n",
      "iteration 700 / 1500: loss 302.443212\n",
      "iteration 800 / 1500: loss 300.568890\n",
      "iteration 900 / 1500: loss 298.300432\n",
      "iteration 1000 / 1500: loss 296.639419\n",
      "iteration 1100 / 1500: loss 294.684422\n",
      "iteration 1200 / 1500: loss 292.602946\n",
      "iteration 1300 / 1500: loss 290.919465\n",
      "iteration 1400 / 1500: loss 288.478871\n",
      "3.1622776601683795e-09 21544.346900318822\n",
      "iteration 0 / 1500: loss 669.513512\n",
      "iteration 100 / 1500: loss 660.853006\n",
      "iteration 200 / 1500: loss 651.056964\n",
      "iteration 300 / 1500: loss 642.445338\n",
      "iteration 400 / 1500: loss 633.806226\n",
      "iteration 500 / 1500: loss 624.453298\n",
      "iteration 600 / 1500: loss 616.457125\n",
      "iteration 700 / 1500: loss 608.271201\n",
      "iteration 800 / 1500: loss 599.686870\n",
      "iteration 900 / 1500: loss 591.549583\n",
      "iteration 1000 / 1500: loss 583.289532\n",
      "iteration 1100 / 1500: loss 575.575258\n",
      "iteration 1200 / 1500: loss 567.521479\n",
      "iteration 1300 / 1500: loss 559.469841\n",
      "iteration 1400 / 1500: loss 551.985882\n",
      "3.1622776601683795e-09 46415.888336127726\n",
      "iteration 0 / 1500: loss 1458.233796\n",
      "iteration 100 / 1500: loss 1415.618247\n",
      "iteration 200 / 1500: loss 1374.881207\n",
      "iteration 300 / 1500: loss 1334.831715\n",
      "iteration 400 / 1500: loss 1295.920061\n",
      "iteration 500 / 1500: loss 1258.123143\n",
      "iteration 600 / 1500: loss 1221.396770\n",
      "iteration 700 / 1500: loss 1186.544045\n",
      "iteration 800 / 1500: loss 1151.990681\n",
      "iteration 900 / 1500: loss 1118.374779\n",
      "iteration 1000 / 1500: loss 1085.975778\n",
      "iteration 1100 / 1500: loss 1054.555976\n",
      "iteration 1200 / 1500: loss 1023.977054\n",
      "iteration 1300 / 1500: loss 994.445963\n",
      "iteration 1400 / 1500: loss 965.289927\n",
      "3.1622776601683795e-09 100000.0\n",
      "iteration 0 / 1500: loss 3060.797679\n",
      "iteration 100 / 1500: loss 2873.190702\n",
      "iteration 200 / 1500: loss 2696.973864\n",
      "iteration 300 / 1500: loss 2530.882531\n",
      "iteration 400 / 1500: loss 2376.104442\n",
      "iteration 500 / 1500: loss 2229.756449\n",
      "iteration 600 / 1500: loss 2093.456558\n",
      "iteration 700 / 1500: loss 1965.012701\n",
      "iteration 800 / 1500: loss 1844.594995\n",
      "iteration 900 / 1500: loss 1731.245778\n",
      "iteration 1000 / 1500: loss 1625.276796\n",
      "iteration 1100 / 1500: loss 1525.221926\n",
      "iteration 1200 / 1500: loss 1431.783803\n",
      "iteration 1300 / 1500: loss 1344.118730\n",
      "iteration 1400 / 1500: loss 1261.745078\n",
      "3.1622776601683795e-09 215443.46900318822\n",
      "iteration 0 / 1500: loss 6605.174345\n",
      "iteration 100 / 1500: loss 5763.436959\n",
      "iteration 200 / 1500: loss 5029.151661\n",
      "iteration 300 / 1500: loss 4388.026217\n",
      "iteration 400 / 1500: loss 3828.251477\n",
      "iteration 500 / 1500: loss 3340.374556\n",
      "iteration 600 / 1500: loss 2914.833665\n",
      "iteration 700 / 1500: loss 2543.046441\n",
      "iteration 800 / 1500: loss 2218.846208\n",
      "iteration 900 / 1500: loss 1936.406492\n",
      "iteration 1000 / 1500: loss 1689.317716\n",
      "iteration 1100 / 1500: loss 1474.291478\n",
      "iteration 1200 / 1500: loss 1286.270941\n",
      "iteration 1300 / 1500: loss 1122.677381\n",
      "iteration 1400 / 1500: loss 979.672640\n",
      "3.1622776601683795e-09 464158.8833612772\n",
      "iteration 0 / 1500: loss 14264.241273\n",
      "iteration 100 / 1500: loss 10633.481273\n",
      "iteration 200 / 1500: loss 7925.985978\n",
      "iteration 300 / 1500: loss 5908.189250\n",
      "iteration 400 / 1500: loss 4404.108203\n",
      "iteration 500 / 1500: loss 3283.071708\n",
      "iteration 600 / 1500: loss 2447.668489\n",
      "iteration 700 / 1500: loss 1824.767783\n",
      "iteration 800 / 1500: loss 1360.559103\n",
      "iteration 900 / 1500: loss 1014.607711\n",
      "iteration 1000 / 1500: loss 756.793125\n",
      "iteration 1100 / 1500: loss 564.573747\n",
      "iteration 1200 / 1500: loss 421.414894\n",
      "iteration 1300 / 1500: loss 314.687304\n",
      "iteration 1400 / 1500: loss 235.127595\n",
      "3.1622776601683795e-09 1000000.0\n",
      "iteration 0 / 1500: loss 30865.890234\n",
      "iteration 100 / 1500: loss 16381.825169\n",
      "iteration 200 / 1500: loss 8694.787906\n",
      "iteration 300 / 1500: loss 4615.247093\n",
      "iteration 400 / 1500: loss 2450.256893\n",
      "iteration 500 / 1500: loss 1301.429432\n",
      "iteration 600 / 1500: loss 691.630921\n",
      "iteration 700 / 1500: loss 368.149377\n",
      "iteration 800 / 1500: loss 196.437368\n",
      "iteration 900 / 1500: loss 105.330414\n",
      "iteration 1000 / 1500: loss 56.956556\n",
      "iteration 1100 / 1500: loss 31.328925\n",
      "iteration 1200 / 1500: loss 17.693743\n",
      "iteration 1300 / 1500: loss 10.479416\n",
      "iteration 1400 / 1500: loss 6.642889\n",
      "1e-08 100.0\n",
      "iteration 0 / 1500: loss 8.158936\n",
      "iteration 100 / 1500: loss 8.390982\n",
      "iteration 200 / 1500: loss 7.666176\n",
      "iteration 300 / 1500: loss 7.634244\n",
      "iteration 400 / 1500: loss 7.387356\n",
      "iteration 500 / 1500: loss 7.731254\n",
      "iteration 600 / 1500: loss 7.979505\n",
      "iteration 700 / 1500: loss 7.017196\n",
      "iteration 800 / 1500: loss 7.394043\n",
      "iteration 900 / 1500: loss 7.250107\n",
      "iteration 1000 / 1500: loss 7.125616\n",
      "iteration 1100 / 1500: loss 6.904360\n",
      "iteration 1200 / 1500: loss 7.071334\n",
      "iteration 1300 / 1500: loss 6.955742\n",
      "iteration 1400 / 1500: loss 6.932027\n",
      "1e-08 215.44346900318845\n",
      "iteration 0 / 1500: loss 11.537999\n",
      "iteration 100 / 1500: loss 11.677263\n",
      "iteration 200 / 1500: loss 11.389751\n",
      "iteration 300 / 1500: loss 11.098444\n",
      "iteration 400 / 1500: loss 10.748381\n",
      "iteration 500 / 1500: loss 10.508318\n",
      "iteration 600 / 1500: loss 10.576673\n",
      "iteration 700 / 1500: loss 10.328952\n",
      "iteration 800 / 1500: loss 10.204549\n",
      "iteration 900 / 1500: loss 10.628746\n",
      "iteration 1000 / 1500: loss 10.544571\n",
      "iteration 1100 / 1500: loss 10.358754\n",
      "iteration 1200 / 1500: loss 10.346559\n",
      "iteration 1300 / 1500: loss 10.207355\n",
      "iteration 1400 / 1500: loss 10.221486\n",
      "1e-08 464.15888336127773\n",
      "iteration 0 / 1500: loss 21.200206\n",
      "iteration 100 / 1500: loss 19.832410\n",
      "iteration 200 / 1500: loss 19.977193\n",
      "iteration 300 / 1500: loss 19.336048\n",
      "iteration 400 / 1500: loss 19.096198\n",
      "iteration 500 / 1500: loss 18.767807\n",
      "iteration 600 / 1500: loss 18.424250\n",
      "iteration 700 / 1500: loss 18.327731\n",
      "iteration 800 / 1500: loss 18.065164\n",
      "iteration 900 / 1500: loss 18.288306\n",
      "iteration 1000 / 1500: loss 17.921572\n",
      "iteration 1100 / 1500: loss 17.825886\n",
      "iteration 1200 / 1500: loss 17.880516\n",
      "iteration 1300 / 1500: loss 17.569680\n",
      "iteration 1400 / 1500: loss 17.615080\n",
      "1e-08 1000.0\n",
      "iteration 0 / 1500: loss 36.296069\n",
      "iteration 100 / 1500: loss 35.561169\n",
      "iteration 200 / 1500: loss 35.510255\n",
      "iteration 300 / 1500: loss 35.182256\n",
      "iteration 400 / 1500: loss 34.731752\n",
      "iteration 500 / 1500: loss 34.676395\n",
      "iteration 600 / 1500: loss 34.440846\n",
      "iteration 700 / 1500: loss 34.102243\n",
      "iteration 800 / 1500: loss 33.937823\n",
      "iteration 900 / 1500: loss 33.871679\n",
      "iteration 1000 / 1500: loss 33.780160\n",
      "iteration 1100 / 1500: loss 33.802690\n",
      "iteration 1200 / 1500: loss 33.382446\n",
      "iteration 1300 / 1500: loss 33.814892\n",
      "iteration 1400 / 1500: loss 33.343674\n",
      "1e-08 2154.4346900318824\n",
      "iteration 0 / 1500: loss 71.704309\n",
      "iteration 100 / 1500: loss 70.840753\n",
      "iteration 200 / 1500: loss 70.445620\n",
      "iteration 300 / 1500: loss 69.322684\n",
      "iteration 400 / 1500: loss 68.934411\n",
      "iteration 500 / 1500: loss 68.111998\n",
      "iteration 600 / 1500: loss 68.029013\n",
      "iteration 700 / 1500: loss 67.708541\n",
      "iteration 800 / 1500: loss 66.973366\n",
      "iteration 900 / 1500: loss 66.856434\n",
      "iteration 1000 / 1500: loss 66.765503\n",
      "iteration 1100 / 1500: loss 66.282656\n",
      "iteration 1200 / 1500: loss 66.125403\n",
      "iteration 1300 / 1500: loss 65.547808\n",
      "iteration 1400 / 1500: loss 65.206929\n",
      "1e-08 4641.588833612777\n",
      "iteration 0 / 1500: loss 147.184587\n",
      "iteration 100 / 1500: loss 145.766138\n",
      "iteration 200 / 1500: loss 144.362255\n",
      "iteration 300 / 1500: loss 142.784941\n",
      "iteration 400 / 1500: loss 141.535435\n",
      "iteration 500 / 1500: loss 139.983933\n",
      "iteration 600 / 1500: loss 138.573524\n",
      "iteration 700 / 1500: loss 137.081671\n",
      "iteration 800 / 1500: loss 135.740587\n",
      "iteration 900 / 1500: loss 134.345026\n",
      "iteration 1000 / 1500: loss 133.129217\n",
      "iteration 1100 / 1500: loss 132.083025\n",
      "iteration 1200 / 1500: loss 130.251652\n",
      "iteration 1300 / 1500: loss 129.031321\n",
      "iteration 1400 / 1500: loss 127.889607\n",
      "1e-08 10000.0\n",
      "iteration 0 / 1500: loss 315.695979\n",
      "iteration 100 / 1500: loss 309.275085\n",
      "iteration 200 / 1500: loss 303.226345\n",
      "iteration 300 / 1500: loss 296.940323\n",
      "iteration 400 / 1500: loss 290.374362\n",
      "iteration 500 / 1500: loss 284.568051\n",
      "iteration 600 / 1500: loss 278.894030\n",
      "iteration 700 / 1500: loss 272.826821\n",
      "iteration 800 / 1500: loss 267.690595\n",
      "iteration 900 / 1500: loss 261.974999\n",
      "iteration 1000 / 1500: loss 257.050565\n",
      "iteration 1100 / 1500: loss 251.825776\n",
      "iteration 1200 / 1500: loss 246.844286\n",
      "iteration 1300 / 1500: loss 242.206548\n",
      "iteration 1400 / 1500: loss 236.792279\n",
      "1e-08 21544.346900318822\n",
      "iteration 0 / 1500: loss 666.571340\n",
      "iteration 100 / 1500: loss 638.051273\n",
      "iteration 200 / 1500: loss 610.479042\n",
      "iteration 300 / 1500: loss 584.502348\n",
      "iteration 400 / 1500: loss 559.645409\n",
      "iteration 500 / 1500: loss 536.110113\n",
      "iteration 600 / 1500: loss 513.721138\n",
      "iteration 700 / 1500: loss 491.524925\n",
      "iteration 800 / 1500: loss 470.606202\n",
      "iteration 900 / 1500: loss 450.567077\n",
      "iteration 1000 / 1500: loss 431.752152\n",
      "iteration 1100 / 1500: loss 413.576884\n",
      "iteration 1200 / 1500: loss 395.654209\n",
      "iteration 1300 / 1500: loss 379.353426\n",
      "iteration 1400 / 1500: loss 363.077080\n",
      "1e-08 46415.888336127726\n",
      "iteration 0 / 1500: loss 1445.143796\n",
      "iteration 100 / 1500: loss 1317.334945\n",
      "iteration 200 / 1500: loss 1200.126166\n",
      "iteration 300 / 1500: loss 1093.583351\n",
      "iteration 400 / 1500: loss 996.873559\n",
      "iteration 500 / 1500: loss 907.962797\n",
      "iteration 600 / 1500: loss 827.331714\n",
      "iteration 700 / 1500: loss 753.870400\n",
      "iteration 800 / 1500: loss 687.312480\n",
      "iteration 900 / 1500: loss 626.279435\n",
      "iteration 1000 / 1500: loss 570.718708\n",
      "iteration 1100 / 1500: loss 520.225443\n",
      "iteration 1200 / 1500: loss 474.129236\n",
      "iteration 1300 / 1500: loss 432.095642\n",
      "iteration 1400 / 1500: loss 393.902732\n",
      "1e-08 100000.0\n",
      "iteration 0 / 1500: loss 3036.030561\n",
      "iteration 100 / 1500: loss 2485.647577\n",
      "iteration 200 / 1500: loss 2033.986819\n",
      "iteration 300 / 1500: loss 1664.739362\n",
      "iteration 400 / 1500: loss 1362.757636\n",
      "iteration 500 / 1500: loss 1115.640018\n",
      "iteration 600 / 1500: loss 913.401797\n",
      "iteration 700 / 1500: loss 747.885435\n",
      "iteration 800 / 1500: loss 612.342923\n",
      "iteration 900 / 1500: loss 501.571610\n",
      "iteration 1000 / 1500: loss 410.782307\n",
      "iteration 1100 / 1500: loss 336.625503\n",
      "iteration 1200 / 1500: loss 275.870110\n",
      "iteration 1300 / 1500: loss 226.140913\n",
      "iteration 1400 / 1500: loss 185.477711\n",
      "1e-08 215443.46900318822\n",
      "iteration 0 / 1500: loss 6482.031315\n",
      "iteration 100 / 1500: loss 4209.809654\n",
      "iteration 200 / 1500: loss 2734.627789\n",
      "iteration 300 / 1500: loss 1776.515389\n",
      "iteration 400 / 1500: loss 1154.355592\n",
      "iteration 500 / 1500: loss 750.371649\n",
      "iteration 600 / 1500: loss 488.142312\n",
      "iteration 700 / 1500: loss 317.787871\n",
      "iteration 800 / 1500: loss 207.121874\n",
      "iteration 900 / 1500: loss 135.374621\n",
      "iteration 1000 / 1500: loss 88.619489\n",
      "iteration 1100 / 1500: loss 58.358749\n",
      "iteration 1200 / 1500: loss 38.702407\n",
      "iteration 1300 / 1500: loss 25.901413\n",
      "iteration 1400 / 1500: loss 17.662392\n",
      "1e-08 464158.8833612772\n",
      "iteration 0 / 1500: loss 14232.235414\n",
      "iteration 100 / 1500: loss 5611.918600\n",
      "iteration 200 / 1500: loss 2213.737885\n",
      "iteration 300 / 1500: loss 873.976278\n",
      "iteration 400 / 1500: loss 345.879406\n",
      "iteration 500 / 1500: loss 137.718423\n",
      "iteration 600 / 1500: loss 55.665652\n",
      "iteration 700 / 1500: loss 23.333769\n",
      "iteration 800 / 1500: loss 10.605092\n",
      "iteration 900 / 1500: loss 5.552845\n",
      "iteration 1000 / 1500: loss 3.559643\n",
      "iteration 1100 / 1500: loss 2.795579\n",
      "iteration 1200 / 1500: loss 2.474541\n",
      "iteration 1300 / 1500: loss 2.353237\n",
      "iteration 1400 / 1500: loss 2.300990\n",
      "1e-08 1000000.0\n",
      "iteration 0 / 1500: loss 30777.012730\n",
      "iteration 100 / 1500: loss 4123.633377\n",
      "iteration 200 / 1500: loss 554.100684\n",
      "iteration 300 / 1500: loss 76.200534\n",
      "iteration 400 / 1500: loss 12.198631\n",
      "iteration 500 / 1500: loss 3.615609\n",
      "iteration 600 / 1500: loss 2.476771\n",
      "iteration 700 / 1500: loss 2.329824\n",
      "iteration 800 / 1500: loss 2.303318\n",
      "iteration 900 / 1500: loss 2.294342\n",
      "iteration 1000 / 1500: loss 2.300350\n",
      "iteration 1100 / 1500: loss 2.285321\n",
      "iteration 1200 / 1500: loss 2.279995\n",
      "iteration 1300 / 1500: loss 2.296527\n",
      "iteration 1400 / 1500: loss 2.284254\n",
      "3.162277660168379e-08 100.0\n",
      "iteration 0 / 1500: loss 8.351759\n",
      "iteration 100 / 1500: loss 7.723028\n",
      "iteration 200 / 1500: loss 7.544092\n",
      "iteration 300 / 1500: loss 6.979978\n",
      "iteration 400 / 1500: loss 6.943620\n",
      "iteration 500 / 1500: loss 6.767117\n",
      "iteration 600 / 1500: loss 6.707358\n",
      "iteration 700 / 1500: loss 7.054761\n",
      "iteration 800 / 1500: loss 6.592331\n",
      "iteration 900 / 1500: loss 6.648327\n",
      "iteration 1000 / 1500: loss 6.223235\n",
      "iteration 1100 / 1500: loss 6.300420\n",
      "iteration 1200 / 1500: loss 6.062894\n",
      "iteration 1300 / 1500: loss 6.143424\n",
      "iteration 1400 / 1500: loss 6.087761\n",
      "3.162277660168379e-08 215.44346900318845\n",
      "iteration 0 / 1500: loss 12.981877\n",
      "iteration 100 / 1500: loss 11.879192\n",
      "iteration 200 / 1500: loss 11.206791\n",
      "iteration 300 / 1500: loss 10.886654\n",
      "iteration 400 / 1500: loss 10.584617\n",
      "iteration 500 / 1500: loss 10.945152\n",
      "iteration 600 / 1500: loss 10.363692\n",
      "iteration 700 / 1500: loss 10.383218\n",
      "iteration 800 / 1500: loss 10.523128\n",
      "iteration 900 / 1500: loss 10.170896\n",
      "iteration 1000 / 1500: loss 10.059534\n",
      "iteration 1100 / 1500: loss 10.142360\n",
      "iteration 1200 / 1500: loss 9.712604\n",
      "iteration 1300 / 1500: loss 9.566191\n",
      "iteration 1400 / 1500: loss 9.783709\n",
      "3.162277660168379e-08 464.15888336127773\n",
      "iteration 0 / 1500: loss 20.690505\n",
      "iteration 100 / 1500: loss 19.525911\n",
      "iteration 200 / 1500: loss 18.744391\n",
      "iteration 300 / 1500: loss 18.520909\n",
      "iteration 400 / 1500: loss 18.212646\n",
      "iteration 500 / 1500: loss 17.962978\n",
      "iteration 600 / 1500: loss 17.767175\n",
      "iteration 700 / 1500: loss 17.202371\n",
      "iteration 800 / 1500: loss 17.258143\n",
      "iteration 900 / 1500: loss 17.511462\n",
      "iteration 1000 / 1500: loss 17.237045\n",
      "iteration 1100 / 1500: loss 17.084823\n",
      "iteration 1200 / 1500: loss 16.930132\n",
      "iteration 1300 / 1500: loss 16.908387\n",
      "iteration 1400 / 1500: loss 16.659062\n",
      "3.162277660168379e-08 1000.0\n",
      "iteration 0 / 1500: loss 36.778388\n",
      "iteration 100 / 1500: loss 34.991679\n",
      "iteration 200 / 1500: loss 34.939381\n",
      "iteration 300 / 1500: loss 34.156168\n",
      "iteration 400 / 1500: loss 33.322114\n",
      "iteration 500 / 1500: loss 33.224163\n",
      "iteration 600 / 1500: loss 33.071954\n",
      "iteration 700 / 1500: loss 32.888659\n",
      "iteration 800 / 1500: loss 32.538211\n",
      "iteration 900 / 1500: loss 31.802083\n",
      "iteration 1000 / 1500: loss 31.844468\n",
      "iteration 1100 / 1500: loss 31.774132\n",
      "iteration 1200 / 1500: loss 31.264283\n",
      "iteration 1300 / 1500: loss 31.229804\n",
      "iteration 1400 / 1500: loss 30.963576\n",
      "3.162277660168379e-08 2154.4346900318824\n",
      "iteration 0 / 1500: loss 72.307455\n",
      "iteration 100 / 1500: loss 69.952369\n",
      "iteration 200 / 1500: loss 68.706047\n",
      "iteration 300 / 1500: loss 67.436801\n",
      "iteration 400 / 1500: loss 66.024199\n",
      "iteration 500 / 1500: loss 65.279568\n",
      "iteration 600 / 1500: loss 64.239622\n",
      "iteration 700 / 1500: loss 63.243209\n",
      "iteration 800 / 1500: loss 62.426693\n",
      "iteration 900 / 1500: loss 61.678169\n",
      "iteration 1000 / 1500: loss 60.632631\n",
      "iteration 1100 / 1500: loss 59.551891\n",
      "iteration 1200 / 1500: loss 58.766050\n",
      "iteration 1300 / 1500: loss 57.808346\n",
      "iteration 1400 / 1500: loss 57.054976\n",
      "3.162277660168379e-08 4641.588833612777\n",
      "iteration 0 / 1500: loss 148.165481\n",
      "iteration 100 / 1500: loss 143.754919\n",
      "iteration 200 / 1500: loss 138.977823\n",
      "iteration 300 / 1500: loss 134.969345\n",
      "iteration 400 / 1500: loss 130.783585\n",
      "iteration 500 / 1500: loss 126.982185\n",
      "iteration 600 / 1500: loss 123.410740\n",
      "iteration 700 / 1500: loss 119.714464\n",
      "iteration 800 / 1500: loss 116.104058\n",
      "iteration 900 / 1500: loss 112.557097\n",
      "iteration 1000 / 1500: loss 109.574166\n",
      "iteration 1100 / 1500: loss 106.200057\n",
      "iteration 1200 / 1500: loss 103.217825\n",
      "iteration 1300 / 1500: loss 100.088799\n",
      "iteration 1400 / 1500: loss 97.355202\n",
      "3.162277660168379e-08 10000.0\n",
      "iteration 0 / 1500: loss 312.979394\n",
      "iteration 100 / 1500: loss 292.600587\n",
      "iteration 200 / 1500: loss 273.842429\n",
      "iteration 300 / 1500: loss 256.616655\n",
      "iteration 400 / 1500: loss 240.340475\n",
      "iteration 500 / 1500: loss 225.907296\n",
      "iteration 600 / 1500: loss 211.817612\n",
      "iteration 700 / 1500: loss 199.003587\n",
      "iteration 800 / 1500: loss 186.634197\n",
      "iteration 900 / 1500: loss 175.280431\n",
      "iteration 1000 / 1500: loss 164.397121\n",
      "iteration 1100 / 1500: loss 154.395164\n",
      "iteration 1200 / 1500: loss 145.106861\n",
      "iteration 1300 / 1500: loss 136.190597\n",
      "iteration 1400 / 1500: loss 127.898610\n",
      "3.162277660168379e-08 21544.346900318822\n",
      "iteration 0 / 1500: loss 663.432546\n",
      "iteration 100 / 1500: loss 578.312180\n",
      "iteration 200 / 1500: loss 504.229468\n",
      "iteration 300 / 1500: loss 439.894982\n",
      "iteration 400 / 1500: loss 383.633222\n",
      "iteration 500 / 1500: loss 334.958180\n",
      "iteration 600 / 1500: loss 292.291383\n",
      "iteration 700 / 1500: loss 255.095272\n",
      "iteration 800 / 1500: loss 222.564466\n",
      "iteration 900 / 1500: loss 194.432042\n",
      "iteration 1000 / 1500: loss 169.785614\n",
      "iteration 1100 / 1500: loss 148.542174\n",
      "iteration 1200 / 1500: loss 129.568548\n",
      "iteration 1300 / 1500: loss 113.416331\n",
      "iteration 1400 / 1500: loss 99.165133\n",
      "3.162277660168379e-08 46415.888336127726\n",
      "iteration 0 / 1500: loss 1452.015151\n",
      "iteration 100 / 1500: loss 1081.948675\n",
      "iteration 200 / 1500: loss 806.344964\n",
      "iteration 300 / 1500: loss 600.980299\n",
      "iteration 400 / 1500: loss 448.185374\n",
      "iteration 500 / 1500: loss 334.273064\n",
      "iteration 600 / 1500: loss 249.593692\n",
      "iteration 700 / 1500: loss 186.506479\n",
      "iteration 800 / 1500: loss 139.385437\n",
      "iteration 900 / 1500: loss 104.383150\n",
      "iteration 1000 / 1500: loss 78.190840\n",
      "iteration 1100 / 1500: loss 58.897804\n",
      "iteration 1200 / 1500: loss 44.387637\n",
      "iteration 1300 / 1500: loss 33.571268\n",
      "iteration 1400 / 1500: loss 25.662757\n",
      "3.162277660168379e-08 100000.0\n",
      "iteration 0 / 1500: loss 3053.421150\n",
      "iteration 100 / 1500: loss 1619.418289\n",
      "iteration 200 / 1500: loss 859.641650\n",
      "iteration 300 / 1500: loss 456.846354\n",
      "iteration 400 / 1500: loss 243.206503\n",
      "iteration 500 / 1500: loss 130.077573\n",
      "iteration 600 / 1500: loss 70.005848\n",
      "iteration 700 / 1500: loss 38.108049\n",
      "iteration 800 / 1500: loss 21.293246\n",
      "iteration 900 / 1500: loss 12.353798\n",
      "iteration 1000 / 1500: loss 7.580602\n",
      "iteration 1100 / 1500: loss 5.070807\n",
      "iteration 1200 / 1500: loss 3.776994\n",
      "iteration 1300 / 1500: loss 3.062436\n",
      "iteration 1400 / 1500: loss 2.685781\n",
      "3.162277660168379e-08 215443.46900318822\n",
      "iteration 0 / 1500: loss 6566.731482\n",
      "iteration 100 / 1500: loss 1673.005421\n",
      "iteration 200 / 1500: loss 427.360364\n",
      "iteration 300 / 1500: loss 110.449464\n",
      "iteration 400 / 1500: loss 29.788602\n",
      "iteration 500 / 1500: loss 9.262986\n",
      "iteration 600 / 1500: loss 4.045339\n",
      "iteration 700 / 1500: loss 2.739642\n",
      "iteration 800 / 1500: loss 2.361323\n",
      "iteration 900 / 1500: loss 2.301466\n",
      "iteration 1000 / 1500: loss 2.272779\n",
      "iteration 1100 / 1500: loss 2.282483\n",
      "iteration 1200 / 1500: loss 2.242623\n",
      "iteration 1300 / 1500: loss 2.303461\n",
      "iteration 1400 / 1500: loss 2.288290\n",
      "3.162277660168379e-08 464158.8833612772\n",
      "iteration 0 / 1500: loss 14152.286572\n",
      "iteration 100 / 1500: loss 736.497631\n",
      "iteration 200 / 1500: loss 40.362977\n",
      "iteration 300 / 1500: loss 4.267645\n",
      "iteration 400 / 1500: loss 2.371138\n",
      "iteration 500 / 1500: loss 2.300418\n",
      "iteration 600 / 1500: loss 2.277092\n",
      "iteration 700 / 1500: loss 2.298910\n",
      "iteration 800 / 1500: loss 2.290001\n",
      "iteration 900 / 1500: loss 2.310097\n",
      "iteration 1000 / 1500: loss 2.292277\n",
      "iteration 1100 / 1500: loss 2.287096\n",
      "iteration 1200 / 1500: loss 2.281309\n",
      "iteration 1300 / 1500: loss 2.285315\n",
      "iteration 1400 / 1500: loss 2.278195\n",
      "3.162277660168379e-08 1000000.0\n",
      "iteration 0 / 1500: loss 30674.219771\n",
      "iteration 100 / 1500: loss 51.814905\n",
      "iteration 200 / 1500: loss 2.381714\n",
      "iteration 300 / 1500: loss 2.298792\n",
      "iteration 400 / 1500: loss 2.290133\n",
      "iteration 500 / 1500: loss 2.300154\n",
      "iteration 600 / 1500: loss 2.294312\n",
      "iteration 700 / 1500: loss 2.295190\n",
      "iteration 800 / 1500: loss 2.305905\n",
      "iteration 900 / 1500: loss 2.285768\n",
      "iteration 1000 / 1500: loss 2.288663\n",
      "iteration 1100 / 1500: loss 2.298437\n",
      "iteration 1200 / 1500: loss 2.293183\n",
      "iteration 1300 / 1500: loss 2.296576\n",
      "iteration 1400 / 1500: loss 2.299715\n",
      "1e-07 100.0\n",
      "iteration 0 / 1500: loss 8.484015\n",
      "iteration 100 / 1500: loss 7.391580\n",
      "iteration 200 / 1500: loss 7.024695\n",
      "iteration 300 / 1500: loss 6.412509\n",
      "iteration 400 / 1500: loss 6.110975\n",
      "iteration 500 / 1500: loss 6.407156\n",
      "iteration 600 / 1500: loss 5.954875\n",
      "iteration 700 / 1500: loss 6.123334\n",
      "iteration 800 / 1500: loss 6.026888\n",
      "iteration 900 / 1500: loss 5.885750\n",
      "iteration 1000 / 1500: loss 5.442136\n",
      "iteration 1100 / 1500: loss 5.842030\n",
      "iteration 1200 / 1500: loss 5.762608\n",
      "iteration 1300 / 1500: loss 5.536766\n",
      "iteration 1400 / 1500: loss 5.375973\n",
      "1e-07 215.44346900318845\n",
      "iteration 0 / 1500: loss 12.672290\n",
      "iteration 100 / 1500: loss 10.405032\n",
      "iteration 200 / 1500: loss 10.168506\n",
      "iteration 300 / 1500: loss 9.722769\n",
      "iteration 400 / 1500: loss 9.533343\n",
      "iteration 500 / 1500: loss 9.403377\n",
      "iteration 600 / 1500: loss 9.518241\n",
      "iteration 700 / 1500: loss 9.449531\n",
      "iteration 800 / 1500: loss 9.080222\n",
      "iteration 900 / 1500: loss 8.857761\n",
      "iteration 1000 / 1500: loss 8.944018\n",
      "iteration 1100 / 1500: loss 9.066272\n",
      "iteration 1200 / 1500: loss 8.758507\n",
      "iteration 1300 / 1500: loss 9.006360\n",
      "iteration 1400 / 1500: loss 8.901750\n",
      "1e-07 464.15888336127773\n",
      "iteration 0 / 1500: loss 19.705141\n",
      "iteration 100 / 1500: loss 18.184720\n",
      "iteration 200 / 1500: loss 17.299995\n",
      "iteration 300 / 1500: loss 17.041205\n",
      "iteration 400 / 1500: loss 16.750350\n",
      "iteration 500 / 1500: loss 16.228238\n",
      "iteration 600 / 1500: loss 16.011639\n",
      "iteration 700 / 1500: loss 15.728946\n",
      "iteration 800 / 1500: loss 15.469801\n",
      "iteration 900 / 1500: loss 15.688186\n",
      "iteration 1000 / 1500: loss 15.383489\n",
      "iteration 1100 / 1500: loss 15.328279\n",
      "iteration 1200 / 1500: loss 15.147029\n",
      "iteration 1300 / 1500: loss 15.000712\n",
      "iteration 1400 / 1500: loss 14.862465\n",
      "1e-07 1000.0\n",
      "iteration 0 / 1500: loss 36.475328\n",
      "iteration 100 / 1500: loss 34.126248\n",
      "iteration 200 / 1500: loss 33.434326\n",
      "iteration 300 / 1500: loss 32.307259\n",
      "iteration 400 / 1500: loss 31.717897\n",
      "iteration 500 / 1500: loss 30.831647\n",
      "iteration 600 / 1500: loss 30.047568\n",
      "iteration 700 / 1500: loss 29.556726\n",
      "iteration 800 / 1500: loss 28.586956\n",
      "iteration 900 / 1500: loss 28.264435\n",
      "iteration 1000 / 1500: loss 27.582379\n",
      "iteration 1100 / 1500: loss 27.187927\n",
      "iteration 1200 / 1500: loss 26.609512\n",
      "iteration 1300 / 1500: loss 26.043213\n",
      "iteration 1400 / 1500: loss 25.677557\n",
      "1e-07 2154.4346900318824\n",
      "iteration 0 / 1500: loss 70.328435\n",
      "iteration 100 / 1500: loss 65.689372\n",
      "iteration 200 / 1500: loss 63.174095\n",
      "iteration 300 / 1500: loss 59.943452\n",
      "iteration 400 / 1500: loss 56.956582\n",
      "iteration 500 / 1500: loss 54.712060\n",
      "iteration 600 / 1500: loss 52.658235\n",
      "iteration 700 / 1500: loss 50.035288\n",
      "iteration 800 / 1500: loss 47.877678\n",
      "iteration 900 / 1500: loss 45.993057\n",
      "iteration 1000 / 1500: loss 44.097118\n",
      "iteration 1100 / 1500: loss 42.304621\n",
      "iteration 1200 / 1500: loss 40.531023\n",
      "iteration 1300 / 1500: loss 38.714122\n",
      "iteration 1400 / 1500: loss 37.116653\n",
      "1e-07 4641.588833612777\n",
      "iteration 0 / 1500: loss 145.962289\n",
      "iteration 100 / 1500: loss 130.894284\n",
      "iteration 200 / 1500: loss 118.985140\n",
      "iteration 300 / 1500: loss 108.318701\n",
      "iteration 400 / 1500: loss 98.716411\n",
      "iteration 500 / 1500: loss 90.028294\n",
      "iteration 600 / 1500: loss 81.974414\n",
      "iteration 700 / 1500: loss 74.449341\n",
      "iteration 800 / 1500: loss 68.220925\n",
      "iteration 900 / 1500: loss 62.245342\n",
      "iteration 1000 / 1500: loss 56.736053\n",
      "iteration 1100 / 1500: loss 51.903815\n",
      "iteration 1200 / 1500: loss 47.305450\n",
      "iteration 1300 / 1500: loss 43.288055\n",
      "iteration 1400 / 1500: loss 39.561694\n",
      "1e-07 10000.0\n",
      "iteration 0 / 1500: loss 313.928089\n",
      "iteration 100 / 1500: loss 255.198341\n",
      "iteration 200 / 1500: loss 208.422851\n",
      "iteration 300 / 1500: loss 170.507609\n",
      "iteration 400 / 1500: loss 139.724487\n",
      "iteration 500 / 1500: loss 114.501299\n",
      "iteration 600 / 1500: loss 93.755037\n",
      "iteration 700 / 1500: loss 77.221061\n",
      "iteration 800 / 1500: loss 63.403129\n",
      "iteration 900 / 1500: loss 52.303693\n",
      "iteration 1000 / 1500: loss 43.117883\n",
      "iteration 1100 / 1500: loss 35.532280\n",
      "iteration 1200 / 1500: loss 29.412473\n",
      "iteration 1300 / 1500: loss 24.431211\n",
      "iteration 1400 / 1500: loss 20.279802\n",
      "1e-07 21544.346900318822\n",
      "iteration 0 / 1500: loss 677.428400\n",
      "iteration 100 / 1500: loss 438.250785\n",
      "iteration 200 / 1500: loss 284.620577\n",
      "iteration 300 / 1500: loss 185.158037\n",
      "iteration 400 / 1500: loss 120.688458\n",
      "iteration 500 / 1500: loss 79.047454\n",
      "iteration 600 / 1500: loss 52.048972\n",
      "iteration 700 / 1500: loss 34.382044\n",
      "iteration 800 / 1500: loss 23.011129\n",
      "iteration 900 / 1500: loss 15.686695\n",
      "iteration 1000 / 1500: loss 10.882984\n",
      "iteration 1100 / 1500: loss 7.800941\n",
      "iteration 1200 / 1500: loss 5.784872\n",
      "iteration 1300 / 1500: loss 4.519751\n",
      "iteration 1400 / 1500: loss 3.609201\n",
      "1e-07 46415.888336127726\n",
      "iteration 0 / 1500: loss 1434.189642\n",
      "iteration 100 / 1500: loss 564.974892\n",
      "iteration 200 / 1500: loss 223.588430\n",
      "iteration 300 / 1500: loss 89.145075\n",
      "iteration 400 / 1500: loss 36.352714\n",
      "iteration 500 / 1500: loss 15.569411\n",
      "iteration 600 / 1500: loss 7.472325\n",
      "iteration 700 / 1500: loss 4.229076\n",
      "iteration 800 / 1500: loss 3.015692\n",
      "iteration 900 / 1500: loss 2.491511\n",
      "iteration 1000 / 1500: loss 2.274388\n",
      "iteration 1100 / 1500: loss 2.186035\n",
      "iteration 1200 / 1500: loss 2.212953\n",
      "iteration 1300 / 1500: loss 2.138330\n",
      "iteration 1400 / 1500: loss 2.122161\n",
      "1e-07 100000.0\n",
      "iteration 0 / 1500: loss 3047.989795\n",
      "iteration 100 / 1500: loss 408.920347\n",
      "iteration 200 / 1500: loss 56.597131\n",
      "iteration 300 / 1500: loss 9.436169\n",
      "iteration 400 / 1500: loss 3.241253\n",
      "iteration 500 / 1500: loss 2.309017\n",
      "iteration 600 / 1500: loss 2.248892\n",
      "iteration 700 / 1500: loss 2.177905\n",
      "iteration 800 / 1500: loss 2.262946\n",
      "iteration 900 / 1500: loss 2.249157\n",
      "iteration 1000 / 1500: loss 2.183575\n",
      "iteration 1100 / 1500: loss 2.213755\n",
      "iteration 1200 / 1500: loss 2.216383\n",
      "iteration 1300 / 1500: loss 2.260460\n",
      "iteration 1400 / 1500: loss 2.247445\n",
      "1e-07 215443.46900318822\n",
      "iteration 0 / 1500: loss 6640.773318\n",
      "iteration 100 / 1500: loss 87.058716\n",
      "iteration 200 / 1500: loss 3.346864\n",
      "iteration 300 / 1500: loss 2.276010\n",
      "iteration 400 / 1500: loss 2.232973\n",
      "iteration 500 / 1500: loss 2.278309\n",
      "iteration 600 / 1500: loss 2.254793\n",
      "iteration 700 / 1500: loss 2.252240\n",
      "iteration 800 / 1500: loss 2.237071\n",
      "iteration 900 / 1500: loss 2.299044\n",
      "iteration 1000 / 1500: loss 2.234476\n",
      "iteration 1100 / 1500: loss 2.251644\n",
      "iteration 1200 / 1500: loss 2.240102\n",
      "iteration 1300 / 1500: loss 2.251725\n",
      "iteration 1400 / 1500: loss 2.272189\n",
      "1e-07 464158.8833612772\n",
      "iteration 0 / 1500: loss 14253.062320\n",
      "iteration 100 / 1500: loss 3.358180\n",
      "iteration 200 / 1500: loss 2.268488\n",
      "iteration 300 / 1500: loss 2.271325\n",
      "iteration 400 / 1500: loss 2.276694\n",
      "iteration 500 / 1500: loss 2.284072\n",
      "iteration 600 / 1500: loss 2.281640\n",
      "iteration 700 / 1500: loss 2.278433\n",
      "iteration 800 / 1500: loss 2.302079\n",
      "iteration 900 / 1500: loss 2.290794\n",
      "iteration 1000 / 1500: loss 2.281884\n",
      "iteration 1100 / 1500: loss 2.260837\n",
      "iteration 1200 / 1500: loss 2.295720\n",
      "iteration 1300 / 1500: loss 2.304499\n",
      "iteration 1400 / 1500: loss 2.298217\n",
      "1e-07 1000000.0\n",
      "iteration 0 / 1500: loss 30290.084785\n",
      "iteration 100 / 1500: loss 2.302216\n",
      "iteration 200 / 1500: loss 2.304510\n",
      "iteration 300 / 1500: loss 2.310501\n",
      "iteration 400 / 1500: loss 2.294519\n",
      "iteration 500 / 1500: loss 2.310351\n",
      "iteration 600 / 1500: loss 2.289352\n",
      "iteration 700 / 1500: loss 2.294628\n",
      "iteration 800 / 1500: loss 2.309717\n",
      "iteration 900 / 1500: loss 2.295596\n",
      "iteration 1000 / 1500: loss 2.297633\n",
      "iteration 1100 / 1500: loss 2.323510\n",
      "iteration 1200 / 1500: loss 2.287162\n",
      "iteration 1300 / 1500: loss 2.312035\n",
      "iteration 1400 / 1500: loss 2.303076\n",
      "3.162277660168379e-07 100.0\n",
      "iteration 0 / 1500: loss 8.296735\n",
      "iteration 100 / 1500: loss 6.211189\n",
      "iteration 200 / 1500: loss 5.790939\n",
      "iteration 300 / 1500: loss 5.534897\n",
      "iteration 400 / 1500: loss 5.623980\n",
      "iteration 500 / 1500: loss 5.554314\n",
      "iteration 600 / 1500: loss 5.249085\n",
      "iteration 700 / 1500: loss 5.289297\n",
      "iteration 800 / 1500: loss 5.297901\n",
      "iteration 900 / 1500: loss 5.251939\n",
      "iteration 1000 / 1500: loss 5.182009\n",
      "iteration 1100 / 1500: loss 4.918452\n",
      "iteration 1200 / 1500: loss 5.120303\n",
      "iteration 1300 / 1500: loss 4.900971\n",
      "iteration 1400 / 1500: loss 4.982767\n",
      "3.162277660168379e-07 215.44346900318845\n",
      "iteration 0 / 1500: loss 12.884092\n",
      "iteration 100 / 1500: loss 9.590798\n",
      "iteration 200 / 1500: loss 9.240151\n",
      "iteration 300 / 1500: loss 8.948500\n",
      "iteration 400 / 1500: loss 8.683289\n",
      "iteration 500 / 1500: loss 8.759455\n",
      "iteration 600 / 1500: loss 8.605111\n",
      "iteration 700 / 1500: loss 8.197083\n",
      "iteration 800 / 1500: loss 8.055515\n",
      "iteration 900 / 1500: loss 8.009927\n",
      "iteration 1000 / 1500: loss 7.993736\n",
      "iteration 1100 / 1500: loss 7.758945\n",
      "iteration 1200 / 1500: loss 7.773929\n",
      "iteration 1300 / 1500: loss 7.661203\n",
      "iteration 1400 / 1500: loss 7.561032\n",
      "3.162277660168379e-07 464.15888336127773\n",
      "iteration 0 / 1500: loss 19.253100\n",
      "iteration 100 / 1500: loss 16.874587\n",
      "iteration 200 / 1500: loss 15.923850\n",
      "iteration 300 / 1500: loss 15.461587\n",
      "iteration 400 / 1500: loss 15.023114\n",
      "iteration 500 / 1500: loss 14.249224\n",
      "iteration 600 / 1500: loss 13.816387\n",
      "iteration 700 / 1500: loss 13.259911\n",
      "iteration 800 / 1500: loss 13.056663\n",
      "iteration 900 / 1500: loss 12.743437\n",
      "iteration 1000 / 1500: loss 12.350238\n",
      "iteration 1100 / 1500: loss 12.048153\n",
      "iteration 1200 / 1500: loss 11.681851\n",
      "iteration 1300 / 1500: loss 11.327333\n",
      "iteration 1400 / 1500: loss 11.145756\n",
      "3.162277660168379e-07 1000.0\n",
      "iteration 0 / 1500: loss 35.859156\n",
      "iteration 100 / 1500: loss 31.772944\n",
      "iteration 200 / 1500: loss 29.744616\n",
      "iteration 300 / 1500: loss 27.543921\n",
      "iteration 400 / 1500: loss 25.753748\n",
      "iteration 500 / 1500: loss 24.154201\n",
      "iteration 600 / 1500: loss 22.899412\n",
      "iteration 700 / 1500: loss 21.538593\n",
      "iteration 800 / 1500: loss 20.262405\n",
      "iteration 900 / 1500: loss 18.970132\n",
      "iteration 1000 / 1500: loss 17.844635\n",
      "iteration 1100 / 1500: loss 16.752230\n",
      "iteration 1200 / 1500: loss 15.825313\n",
      "iteration 1300 / 1500: loss 14.903940\n",
      "iteration 1400 / 1500: loss 14.093589\n",
      "3.162277660168379e-07 2154.4346900318824\n",
      "iteration 0 / 1500: loss 71.048325\n",
      "iteration 100 / 1500: loss 60.177363\n",
      "iteration 200 / 1500: loss 52.310550\n",
      "iteration 300 / 1500: loss 45.433181\n",
      "iteration 400 / 1500: loss 39.870268\n",
      "iteration 500 / 1500: loss 34.958422\n",
      "iteration 600 / 1500: loss 30.350900\n",
      "iteration 700 / 1500: loss 26.867109\n",
      "iteration 800 / 1500: loss 23.418688\n",
      "iteration 900 / 1500: loss 20.793069\n",
      "iteration 1000 / 1500: loss 18.251274\n",
      "iteration 1100 / 1500: loss 16.200542\n",
      "iteration 1200 / 1500: loss 14.295786\n",
      "iteration 1300 / 1500: loss 12.566959\n",
      "iteration 1400 / 1500: loss 11.385301\n",
      "3.162277660168379e-07 4641.588833612777\n",
      "iteration 0 / 1500: loss 148.219344\n",
      "iteration 100 / 1500: loss 108.710523\n",
      "iteration 200 / 1500: loss 81.028462\n",
      "iteration 300 / 1500: loss 60.568574\n",
      "iteration 400 / 1500: loss 45.498894\n",
      "iteration 500 / 1500: loss 34.274884\n",
      "iteration 600 / 1500: loss 25.948361\n",
      "iteration 700 / 1500: loss 19.754925\n",
      "iteration 800 / 1500: loss 15.293883\n",
      "iteration 900 / 1500: loss 11.786556\n",
      "iteration 1000 / 1500: loss 9.279011\n",
      "iteration 1100 / 1500: loss 7.439782\n",
      "iteration 1200 / 1500: loss 5.936521\n",
      "iteration 1300 / 1500: loss 4.871562\n",
      "iteration 1400 / 1500: loss 4.306576\n",
      "3.162277660168379e-07 10000.0\n",
      "iteration 0 / 1500: loss 314.287784\n",
      "iteration 100 / 1500: loss 165.669381\n",
      "iteration 200 / 1500: loss 88.126390\n",
      "iteration 300 / 1500: loss 47.645462\n",
      "iteration 400 / 1500: loss 26.084751\n",
      "iteration 500 / 1500: loss 14.641322\n",
      "iteration 600 / 1500: loss 8.888015\n",
      "iteration 700 / 1500: loss 5.641466\n",
      "iteration 800 / 1500: loss 3.951378\n",
      "iteration 900 / 1500: loss 3.028451\n",
      "iteration 1000 / 1500: loss 2.533633\n",
      "iteration 1100 / 1500: loss 2.253985\n",
      "iteration 1200 / 1500: loss 2.211551\n",
      "iteration 1300 / 1500: loss 2.143362\n",
      "iteration 1400 / 1500: loss 2.021118\n",
      "3.162277660168379e-07 21544.346900318822\n",
      "iteration 0 / 1500: loss 660.975227\n",
      "iteration 100 / 1500: loss 168.420727\n",
      "iteration 200 / 1500: loss 44.134847\n",
      "iteration 300 / 1500: loss 12.758585\n",
      "iteration 400 / 1500: loss 4.723807\n",
      "iteration 500 / 1500: loss 2.770172\n",
      "iteration 600 / 1500: loss 2.292604\n",
      "iteration 700 / 1500: loss 2.160760\n",
      "iteration 800 / 1500: loss 2.106126\n",
      "iteration 900 / 1500: loss 2.177449\n",
      "iteration 1000 / 1500: loss 2.150983\n",
      "iteration 1100 / 1500: loss 2.161656\n",
      "iteration 1200 / 1500: loss 2.071295\n",
      "iteration 1300 / 1500: loss 2.101987\n",
      "iteration 1400 / 1500: loss 2.094863\n",
      "3.162277660168379e-07 46415.888336127726\n",
      "iteration 0 / 1500: loss 1444.188804\n",
      "iteration 100 / 1500: loss 76.422152\n",
      "iteration 200 / 1500: loss 6.006168\n",
      "iteration 300 / 1500: loss 2.358977\n",
      "iteration 400 / 1500: loss 2.128328\n",
      "iteration 500 / 1500: loss 2.154528\n",
      "iteration 600 / 1500: loss 2.171562\n",
      "iteration 700 / 1500: loss 2.154467\n",
      "iteration 800 / 1500: loss 2.241327\n",
      "iteration 900 / 1500: loss 2.160012\n",
      "iteration 1000 / 1500: loss 2.157144\n",
      "iteration 1100 / 1500: loss 2.196170\n",
      "iteration 1200 / 1500: loss 2.219672\n",
      "iteration 1300 / 1500: loss 2.172110\n",
      "iteration 1400 / 1500: loss 2.188910\n",
      "3.162277660168379e-07 100000.0\n",
      "iteration 0 / 1500: loss 3141.945647\n",
      "iteration 100 / 1500: loss 7.236701\n",
      "iteration 200 / 1500: loss 2.244464\n",
      "iteration 300 / 1500: loss 2.227152\n",
      "iteration 400 / 1500: loss 2.184372\n",
      "iteration 500 / 1500: loss 2.178860\n",
      "iteration 600 / 1500: loss 2.222513\n",
      "iteration 700 / 1500: loss 2.212268\n",
      "iteration 800 / 1500: loss 2.219237\n",
      "iteration 900 / 1500: loss 2.258348\n",
      "iteration 1000 / 1500: loss 2.184351\n",
      "iteration 1100 / 1500: loss 2.218926\n",
      "iteration 1200 / 1500: loss 2.236668\n",
      "iteration 1300 / 1500: loss 2.232226\n",
      "iteration 1400 / 1500: loss 2.224652\n",
      "3.162277660168379e-07 215443.46900318822\n",
      "iteration 0 / 1500: loss 6687.960988\n",
      "iteration 100 / 1500: loss 2.243592\n",
      "iteration 200 / 1500: loss 2.265260\n",
      "iteration 300 / 1500: loss 2.262610\n",
      "iteration 400 / 1500: loss 2.276095\n",
      "iteration 500 / 1500: loss 2.240414\n",
      "iteration 600 / 1500: loss 2.290930\n",
      "iteration 700 / 1500: loss 2.306693\n",
      "iteration 800 / 1500: loss 2.271788\n",
      "iteration 900 / 1500: loss 2.252713\n",
      "iteration 1000 / 1500: loss 2.269314\n",
      "iteration 1100 / 1500: loss 2.247863\n",
      "iteration 1200 / 1500: loss 2.293501\n",
      "iteration 1300 / 1500: loss 2.277010\n",
      "iteration 1400 / 1500: loss 2.278423\n",
      "3.162277660168379e-07 464158.8833612772\n",
      "iteration 0 / 1500: loss 14002.810930\n",
      "iteration 100 / 1500: loss 2.299979\n",
      "iteration 200 / 1500: loss 2.293673\n",
      "iteration 300 / 1500: loss 2.316219\n",
      "iteration 400 / 1500: loss 2.296658\n",
      "iteration 500 / 1500: loss 2.307999\n",
      "iteration 600 / 1500: loss 2.280920\n",
      "iteration 700 / 1500: loss 2.288904\n",
      "iteration 800 / 1500: loss 2.302290\n",
      "iteration 900 / 1500: loss 2.285065\n",
      "iteration 1000 / 1500: loss 2.287246\n",
      "iteration 1100 / 1500: loss 2.300927\n",
      "iteration 1200 / 1500: loss 2.295656\n",
      "iteration 1300 / 1500: loss 2.297751\n",
      "iteration 1400 / 1500: loss 2.297808\n",
      "3.162277660168379e-07 1000000.0\n",
      "iteration 0 / 1500: loss 31027.336903\n",
      "iteration 100 / 1500: loss 2.323553\n",
      "iteration 200 / 1500: loss 2.322006\n",
      "iteration 300 / 1500: loss 2.307754\n",
      "iteration 400 / 1500: loss 2.307695\n",
      "iteration 500 / 1500: loss 2.314678\n",
      "iteration 600 / 1500: loss 2.279232\n",
      "iteration 700 / 1500: loss 2.321974\n",
      "iteration 800 / 1500: loss 2.312405\n",
      "iteration 900 / 1500: loss 2.315352\n",
      "iteration 1000 / 1500: loss 2.293644\n",
      "iteration 1100 / 1500: loss 2.314970\n",
      "iteration 1200 / 1500: loss 2.306512\n",
      "iteration 1300 / 1500: loss 2.313215\n",
      "iteration 1400 / 1500: loss 2.300158\n",
      "1e-06 100.0\n",
      "iteration 0 / 1500: loss 8.280672\n",
      "iteration 100 / 1500: loss 5.848253\n",
      "iteration 200 / 1500: loss 5.774665\n",
      "iteration 300 / 1500: loss 5.021860\n",
      "iteration 400 / 1500: loss 4.898405\n",
      "iteration 500 / 1500: loss 4.948990\n",
      "iteration 600 / 1500: loss 4.778852\n",
      "iteration 700 / 1500: loss 4.535739\n",
      "iteration 800 / 1500: loss 4.517287\n",
      "iteration 900 / 1500: loss 4.557059\n",
      "iteration 1000 / 1500: loss 4.451226\n",
      "iteration 1100 / 1500: loss 4.401380\n",
      "iteration 1200 / 1500: loss 4.148735\n",
      "iteration 1300 / 1500: loss 4.289163\n",
      "iteration 1400 / 1500: loss 3.988412\n",
      "1e-06 215.44346900318845\n",
      "iteration 0 / 1500: loss 12.463449\n",
      "iteration 100 / 1500: loss 8.983893\n",
      "iteration 200 / 1500: loss 8.439382\n",
      "iteration 300 / 1500: loss 8.038886\n",
      "iteration 400 / 1500: loss 7.642152\n",
      "iteration 500 / 1500: loss 7.261276\n",
      "iteration 600 / 1500: loss 7.116966\n",
      "iteration 700 / 1500: loss 6.519007\n",
      "iteration 800 / 1500: loss 6.445231\n",
      "iteration 900 / 1500: loss 6.172110\n",
      "iteration 1000 / 1500: loss 6.045964\n",
      "iteration 1100 / 1500: loss 5.717248\n",
      "iteration 1200 / 1500: loss 5.624540\n",
      "iteration 1300 / 1500: loss 5.477767\n",
      "iteration 1400 / 1500: loss 5.196551\n",
      "1e-06 464.15888336127773\n",
      "iteration 0 / 1500: loss 19.180782\n",
      "iteration 100 / 1500: loss 15.661740\n",
      "iteration 200 / 1500: loss 13.822867\n",
      "iteration 300 / 1500: loss 12.846027\n",
      "iteration 400 / 1500: loss 11.803401\n",
      "iteration 500 / 1500: loss 10.931264\n",
      "iteration 600 / 1500: loss 9.870048\n",
      "iteration 700 / 1500: loss 9.078994\n",
      "iteration 800 / 1500: loss 8.354203\n",
      "iteration 900 / 1500: loss 7.644305\n",
      "iteration 1000 / 1500: loss 7.147897\n",
      "iteration 1100 / 1500: loss 6.750431\n",
      "iteration 1200 / 1500: loss 6.248191\n",
      "iteration 1300 / 1500: loss 5.710569\n",
      "iteration 1400 / 1500: loss 5.514276\n",
      "1e-06 1000.0\n",
      "iteration 0 / 1500: loss 36.457663\n",
      "iteration 100 / 1500: loss 27.479902\n",
      "iteration 200 / 1500: loss 22.328472\n",
      "iteration 300 / 1500: loss 18.527236\n",
      "iteration 400 / 1500: loss 15.388871\n",
      "iteration 500 / 1500: loss 12.828608\n",
      "iteration 600 / 1500: loss 10.815941\n",
      "iteration 700 / 1500: loss 9.103771\n",
      "iteration 800 / 1500: loss 7.644280\n",
      "iteration 900 / 1500: loss 6.707266\n",
      "iteration 1000 / 1500: loss 5.823337\n",
      "iteration 1100 / 1500: loss 5.123739\n",
      "iteration 1200 / 1500: loss 4.349526\n",
      "iteration 1300 / 1500: loss 3.944157\n",
      "iteration 1400 / 1500: loss 3.579931\n",
      "1e-06 2154.4346900318824\n",
      "iteration 0 / 1500: loss 71.473096\n",
      "iteration 100 / 1500: loss 44.270821\n",
      "iteration 200 / 1500: loss 29.177049\n",
      "iteration 300 / 1500: loss 19.441402\n",
      "iteration 400 / 1500: loss 13.120562\n",
      "iteration 500 / 1500: loss 9.222107\n",
      "iteration 600 / 1500: loss 6.673058\n",
      "iteration 700 / 1500: loss 4.973786\n",
      "iteration 800 / 1500: loss 3.804957\n",
      "iteration 900 / 1500: loss 3.179537\n",
      "iteration 1000 / 1500: loss 2.695454\n",
      "iteration 1100 / 1500: loss 2.458054\n",
      "iteration 1200 / 1500: loss 2.258158\n",
      "iteration 1300 / 1500: loss 2.126672\n",
      "iteration 1400 / 1500: loss 2.007607\n",
      "1e-06 4641.588833612777\n",
      "iteration 0 / 1500: loss 149.262119\n",
      "iteration 100 / 1500: loss 57.848487\n",
      "iteration 200 / 1500: loss 23.623447\n",
      "iteration 300 / 1500: loss 10.469636\n",
      "iteration 400 / 1500: loss 5.370965\n",
      "iteration 500 / 1500: loss 3.163405\n",
      "iteration 600 / 1500: loss 2.548804\n",
      "iteration 700 / 1500: loss 2.200658\n",
      "iteration 800 / 1500: loss 2.036647\n",
      "iteration 900 / 1500: loss 2.193682\n",
      "iteration 1000 / 1500: loss 1.943793\n",
      "iteration 1100 / 1500: loss 1.924659\n",
      "iteration 1200 / 1500: loss 1.991839\n",
      "iteration 1300 / 1500: loss 1.963315\n",
      "iteration 1400 / 1500: loss 2.023760\n",
      "1e-06 10000.0\n",
      "iteration 0 / 1500: loss 314.939907\n",
      "iteration 100 / 1500: loss 42.842505\n",
      "iteration 200 / 1500: loss 7.412069\n",
      "iteration 300 / 1500: loss 2.767086\n",
      "iteration 400 / 1500: loss 2.149593\n",
      "iteration 500 / 1500: loss 1.959895\n",
      "iteration 600 / 1500: loss 2.079328\n",
      "iteration 700 / 1500: loss 2.049864\n",
      "iteration 800 / 1500: loss 2.065594\n",
      "iteration 900 / 1500: loss 2.045036\n",
      "iteration 1000 / 1500: loss 2.050924\n",
      "iteration 1100 / 1500: loss 1.987504\n",
      "iteration 1200 / 1500: loss 2.005696\n",
      "iteration 1300 / 1500: loss 2.038510\n",
      "iteration 1400 / 1500: loss 1.990420\n",
      "1e-06 21544.346900318822\n",
      "iteration 0 / 1500: loss 663.853452\n",
      "iteration 100 / 1500: loss 10.395682\n",
      "iteration 200 / 1500: loss 2.192875\n",
      "iteration 300 / 1500: loss 2.190268\n",
      "iteration 400 / 1500: loss 2.186592\n",
      "iteration 500 / 1500: loss 2.197440\n",
      "iteration 600 / 1500: loss 2.055992\n",
      "iteration 700 / 1500: loss 2.149918\n",
      "iteration 800 / 1500: loss 2.135620\n",
      "iteration 900 / 1500: loss 2.080952\n",
      "iteration 1000 / 1500: loss 2.142418\n",
      "iteration 1100 / 1500: loss 2.063520\n",
      "iteration 1200 / 1500: loss 2.135828\n",
      "iteration 1300 / 1500: loss 2.192315\n",
      "iteration 1400 / 1500: loss 2.102910\n",
      "1e-06 46415.888336127726\n",
      "iteration 0 / 1500: loss 1430.070864\n",
      "iteration 100 / 1500: loss 2.241010\n",
      "iteration 200 / 1500: loss 2.168140\n",
      "iteration 300 / 1500: loss 2.212075\n",
      "iteration 400 / 1500: loss 2.134768\n",
      "iteration 500 / 1500: loss 2.199173\n",
      "iteration 600 / 1500: loss 2.227921\n",
      "iteration 700 / 1500: loss 2.237892\n",
      "iteration 800 / 1500: loss 2.184841\n",
      "iteration 900 / 1500: loss 2.137290\n",
      "iteration 1000 / 1500: loss 2.198508\n",
      "iteration 1100 / 1500: loss 2.203482\n",
      "iteration 1200 / 1500: loss 2.172784\n",
      "iteration 1300 / 1500: loss 2.201968\n",
      "iteration 1400 / 1500: loss 2.129462\n",
      "1e-06 100000.0\n",
      "iteration 0 / 1500: loss 3101.041331\n",
      "iteration 100 / 1500: loss 2.248163\n",
      "iteration 200 / 1500: loss 2.229948\n",
      "iteration 300 / 1500: loss 2.255100\n",
      "iteration 400 / 1500: loss 2.303680\n",
      "iteration 500 / 1500: loss 2.210155\n",
      "iteration 600 / 1500: loss 2.258268\n",
      "iteration 700 / 1500: loss 2.288852\n",
      "iteration 800 / 1500: loss 2.267599\n",
      "iteration 900 / 1500: loss 2.239527\n",
      "iteration 1000 / 1500: loss 2.291424\n",
      "iteration 1100 / 1500: loss 2.243477\n",
      "iteration 1200 / 1500: loss 2.195994\n",
      "iteration 1300 / 1500: loss 2.242540\n",
      "iteration 1400 / 1500: loss 2.264864\n",
      "1e-06 215443.46900318822\n",
      "iteration 0 / 1500: loss 6637.752065\n",
      "iteration 100 / 1500: loss 2.323797\n",
      "iteration 200 / 1500: loss 2.265322\n",
      "iteration 300 / 1500: loss 2.281663\n",
      "iteration 400 / 1500: loss 2.297805\n",
      "iteration 500 / 1500: loss 2.293706\n",
      "iteration 600 / 1500: loss 2.308552\n",
      "iteration 700 / 1500: loss 2.296006\n",
      "iteration 800 / 1500: loss 2.276007\n",
      "iteration 900 / 1500: loss 2.292226\n",
      "iteration 1000 / 1500: loss 2.289706\n",
      "iteration 1100 / 1500: loss 2.276506\n",
      "iteration 1200 / 1500: loss 2.294281\n",
      "iteration 1300 / 1500: loss 2.313783\n",
      "iteration 1400 / 1500: loss 2.304727\n",
      "1e-06 464158.8833612772\n",
      "iteration 0 / 1500: loss 14073.423936\n",
      "iteration 100 / 1500: loss 2.344528\n",
      "iteration 200 / 1500: loss 2.323005\n",
      "iteration 300 / 1500: loss 2.333270\n",
      "iteration 400 / 1500: loss 2.318174\n",
      "iteration 500 / 1500: loss 2.350897\n",
      "iteration 600 / 1500: loss 2.312543\n",
      "iteration 700 / 1500: loss 2.304843\n",
      "iteration 800 / 1500: loss 2.297622\n",
      "iteration 900 / 1500: loss 2.317755\n",
      "iteration 1000 / 1500: loss 2.336351\n",
      "iteration 1100 / 1500: loss 2.313110\n",
      "iteration 1200 / 1500: loss 2.315566\n",
      "iteration 1300 / 1500: loss 2.302312\n",
      "iteration 1400 / 1500: loss 2.298157\n",
      "1e-06 1000000.0\n",
      "iteration 0 / 1500: loss 31191.152453\n",
      "iteration 100 / 1500: loss 2.362253\n",
      "iteration 200 / 1500: loss 2.350240\n",
      "iteration 300 / 1500: loss 2.335132\n",
      "iteration 400 / 1500: loss 2.343572\n",
      "iteration 500 / 1500: loss 2.431329\n",
      "iteration 600 / 1500: loss 2.364469\n",
      "iteration 700 / 1500: loss 2.384732\n",
      "iteration 800 / 1500: loss 2.375337\n",
      "iteration 900 / 1500: loss 2.357739\n",
      "iteration 1000 / 1500: loss 2.371300\n",
      "iteration 1100 / 1500: loss 2.383996\n",
      "iteration 1200 / 1500: loss 2.328737\n",
      "iteration 1300 / 1500: loss 2.331894\n",
      "iteration 1400 / 1500: loss 2.368094\n",
      "3.162277660168379e-06 100.0\n",
      "iteration 0 / 1500: loss 7.763619\n",
      "iteration 100 / 1500: loss 5.091077\n",
      "iteration 200 / 1500: loss 4.755806\n",
      "iteration 300 / 1500: loss 4.533773\n",
      "iteration 400 / 1500: loss 4.325371\n",
      "iteration 500 / 1500: loss 4.156775\n",
      "iteration 600 / 1500: loss 3.722081\n",
      "iteration 700 / 1500: loss 3.680069\n",
      "iteration 800 / 1500: loss 3.570762\n",
      "iteration 900 / 1500: loss 3.237507\n",
      "iteration 1000 / 1500: loss 3.387296\n",
      "iteration 1100 / 1500: loss 3.052034\n",
      "iteration 1200 / 1500: loss 3.035432\n",
      "iteration 1300 / 1500: loss 3.025231\n",
      "iteration 1400 / 1500: loss 2.968114\n",
      "3.162277660168379e-06 215.44346900318845\n",
      "iteration 0 / 1500: loss 13.234304\n",
      "iteration 100 / 1500: loss 7.878255\n",
      "iteration 200 / 1500: loss 7.216702\n",
      "iteration 300 / 1500: loss 6.306389\n",
      "iteration 400 / 1500: loss 5.421241\n",
      "iteration 500 / 1500: loss 5.040016\n",
      "iteration 600 / 1500: loss 4.533504\n",
      "iteration 700 / 1500: loss 4.156797\n",
      "iteration 800 / 1500: loss 3.781254\n",
      "iteration 900 / 1500: loss 3.743308\n",
      "iteration 1000 / 1500: loss 3.385704\n",
      "iteration 1100 / 1500: loss 3.156274\n",
      "iteration 1200 / 1500: loss 3.096540\n",
      "iteration 1300 / 1500: loss 2.726900\n",
      "iteration 1400 / 1500: loss 2.601660\n",
      "3.162277660168379e-06 464.15888336127773\n",
      "iteration 0 / 1500: loss 19.456849\n",
      "iteration 100 / 1500: loss 12.520042\n",
      "iteration 200 / 1500: loss 9.653471\n",
      "iteration 300 / 1500: loss 7.533038\n",
      "iteration 400 / 1500: loss 5.961865\n",
      "iteration 500 / 1500: loss 4.977679\n",
      "iteration 600 / 1500: loss 4.143535\n",
      "iteration 700 / 1500: loss 3.366012\n",
      "iteration 800 / 1500: loss 3.036335\n",
      "iteration 900 / 1500: loss 2.777008\n",
      "iteration 1000 / 1500: loss 2.378425\n",
      "iteration 1100 / 1500: loss 2.365880\n",
      "iteration 1200 / 1500: loss 2.223839\n",
      "iteration 1300 / 1500: loss 2.056810\n",
      "iteration 1400 / 1500: loss 2.169886\n",
      "3.162277660168379e-06 1000.0\n",
      "iteration 0 / 1500: loss 36.459971\n",
      "iteration 100 / 1500: loss 18.222457\n",
      "iteration 200 / 1500: loss 10.302413\n",
      "iteration 300 / 1500: loss 6.309772\n",
      "iteration 400 / 1500: loss 4.134726\n",
      "iteration 500 / 1500: loss 3.171417\n",
      "iteration 600 / 1500: loss 2.510210\n",
      "iteration 700 / 1500: loss 2.185024\n",
      "iteration 800 / 1500: loss 2.047048\n",
      "iteration 900 / 1500: loss 2.066876\n",
      "iteration 1000 / 1500: loss 1.909757\n",
      "iteration 1100 / 1500: loss 1.894970\n",
      "iteration 1200 / 1500: loss 1.927531\n",
      "iteration 1300 / 1500: loss 1.923412\n",
      "iteration 1400 / 1500: loss 1.960864\n",
      "3.162277660168379e-06 2154.4346900318824\n",
      "iteration 0 / 1500: loss 71.276600\n",
      "iteration 100 / 1500: loss 18.344565\n",
      "iteration 200 / 1500: loss 5.919394\n",
      "iteration 300 / 1500: loss 2.793091\n",
      "iteration 400 / 1500: loss 2.212105\n",
      "iteration 500 / 1500: loss 1.945221\n",
      "iteration 600 / 1500: loss 1.936980\n",
      "iteration 700 / 1500: loss 2.065539\n",
      "iteration 800 / 1500: loss 1.871172\n",
      "iteration 900 / 1500: loss 1.910523\n",
      "iteration 1000 / 1500: loss 1.827774\n",
      "iteration 1100 / 1500: loss 1.964007\n",
      "iteration 1200 / 1500: loss 1.895748\n",
      "iteration 1300 / 1500: loss 2.064889\n",
      "iteration 1400 / 1500: loss 1.963466\n",
      "3.162277660168379e-06 4641.588833612777\n",
      "iteration 0 / 1500: loss 148.827635\n",
      "iteration 100 / 1500: loss 9.355222\n",
      "iteration 200 / 1500: loss 2.356446\n",
      "iteration 300 / 1500: loss 2.037026\n",
      "iteration 400 / 1500: loss 1.877586\n",
      "iteration 500 / 1500: loss 2.138554\n",
      "iteration 600 / 1500: loss 2.022406\n",
      "iteration 700 / 1500: loss 1.962436\n",
      "iteration 800 / 1500: loss 1.967161\n",
      "iteration 900 / 1500: loss 1.899565\n",
      "iteration 1000 / 1500: loss 1.911359\n",
      "iteration 1100 / 1500: loss 1.970994\n",
      "iteration 1200 / 1500: loss 1.993852\n",
      "iteration 1300 / 1500: loss 1.948219\n",
      "iteration 1400 / 1500: loss 2.031975\n",
      "3.162277660168379e-06 10000.0\n",
      "iteration 0 / 1500: loss 315.002928\n",
      "iteration 100 / 1500: loss 2.505610\n",
      "iteration 200 / 1500: loss 2.203785\n",
      "iteration 300 / 1500: loss 2.138220\n",
      "iteration 400 / 1500: loss 2.126911\n",
      "iteration 500 / 1500: loss 2.160003\n",
      "iteration 600 / 1500: loss 2.106983\n",
      "iteration 700 / 1500: loss 2.073257\n",
      "iteration 800 / 1500: loss 2.177490\n",
      "iteration 900 / 1500: loss 2.119154\n",
      "iteration 1000 / 1500: loss 2.161602\n",
      "iteration 1100 / 1500: loss 2.069706\n",
      "iteration 1200 / 1500: loss 2.120305\n",
      "iteration 1300 / 1500: loss 2.080259\n",
      "iteration 1400 / 1500: loss 2.139874\n",
      "3.162277660168379e-06 21544.346900318822\n",
      "iteration 0 / 1500: loss 667.577602\n",
      "iteration 100 / 1500: loss 2.137044\n",
      "iteration 200 / 1500: loss 2.189555\n",
      "iteration 300 / 1500: loss 2.193114\n",
      "iteration 400 / 1500: loss 2.168986\n",
      "iteration 500 / 1500: loss 2.157682\n",
      "iteration 600 / 1500: loss 2.226545\n",
      "iteration 700 / 1500: loss 2.106282\n",
      "iteration 800 / 1500: loss 2.221865\n",
      "iteration 900 / 1500: loss 2.212248\n",
      "iteration 1000 / 1500: loss 2.177877\n",
      "iteration 1100 / 1500: loss 2.106198\n",
      "iteration 1200 / 1500: loss 2.208978\n",
      "iteration 1300 / 1500: loss 2.226760\n",
      "iteration 1400 / 1500: loss 2.190235\n",
      "3.162277660168379e-06 46415.888336127726\n",
      "iteration 0 / 1500: loss 1443.975000\n",
      "iteration 100 / 1500: loss 2.228148\n",
      "iteration 200 / 1500: loss 2.175608\n",
      "iteration 300 / 1500: loss 2.347067\n",
      "iteration 400 / 1500: loss 2.288012\n",
      "iteration 500 / 1500: loss 2.302179\n",
      "iteration 600 / 1500: loss 2.252860\n",
      "iteration 700 / 1500: loss 2.274039\n",
      "iteration 800 / 1500: loss 2.231011\n",
      "iteration 900 / 1500: loss 2.272742\n",
      "iteration 1000 / 1500: loss 2.260803\n",
      "iteration 1100 / 1500: loss 2.292393\n",
      "iteration 1200 / 1500: loss 2.322088\n",
      "iteration 1300 / 1500: loss 2.230107\n",
      "iteration 1400 / 1500: loss 2.255039\n",
      "3.162277660168379e-06 100000.0\n",
      "iteration 0 / 1500: loss 3119.597812\n",
      "iteration 100 / 1500: loss 2.274533\n",
      "iteration 200 / 1500: loss 2.388283\n",
      "iteration 300 / 1500: loss 2.323838\n",
      "iteration 400 / 1500: loss 2.368723\n",
      "iteration 500 / 1500: loss 2.354898\n",
      "iteration 600 / 1500: loss 2.380395\n",
      "iteration 700 / 1500: loss 2.366663\n",
      "iteration 800 / 1500: loss 2.287876\n",
      "iteration 900 / 1500: loss 2.357918\n",
      "iteration 1000 / 1500: loss 2.333865\n",
      "iteration 1100 / 1500: loss 2.365705\n",
      "iteration 1200 / 1500: loss 2.346891\n",
      "iteration 1300 / 1500: loss 2.336031\n",
      "iteration 1400 / 1500: loss 2.330393\n",
      "3.162277660168379e-06 215443.46900318822\n",
      "iteration 0 / 1500: loss 6621.607506\n",
      "iteration 100 / 1500: loss 2.433451\n",
      "iteration 200 / 1500: loss 2.526730\n",
      "iteration 300 / 1500: loss 3.121048\n",
      "iteration 400 / 1500: loss 2.361821\n",
      "iteration 500 / 1500: loss 2.505126\n",
      "iteration 600 / 1500: loss 2.463201\n",
      "iteration 700 / 1500: loss 2.538504\n",
      "iteration 800 / 1500: loss 2.528636\n",
      "iteration 900 / 1500: loss 2.737660\n",
      "iteration 1000 / 1500: loss 2.766968\n",
      "iteration 1100 / 1500: loss 2.715609\n",
      "iteration 1200 / 1500: loss 2.594203\n",
      "iteration 1300 / 1500: loss 2.863069\n",
      "iteration 1400 / 1500: loss 2.782006\n",
      "3.162277660168379e-06 464158.8833612772\n",
      "iteration 0 / 1500: loss 14176.656494\n",
      "iteration 100 / 1500: loss 26.275760\n",
      "iteration 200 / 1500: loss 27.207592\n",
      "iteration 300 / 1500: loss 28.200257\n",
      "iteration 400 / 1500: loss 23.154263\n",
      "iteration 500 / 1500: loss 30.054036\n",
      "iteration 600 / 1500: loss 24.782632\n",
      "iteration 700 / 1500: loss 26.121463\n",
      "iteration 800 / 1500: loss 23.618795\n",
      "iteration 900 / 1500: loss 22.415610\n",
      "iteration 1000 / 1500: loss 23.105239\n",
      "iteration 1100 / 1500: loss 24.109775\n",
      "iteration 1200 / 1500: loss 21.837039\n",
      "iteration 1300 / 1500: loss 27.605474\n",
      "iteration 1400 / 1500: loss 21.779969\n",
      "3.162277660168379e-06 1000000.0\n",
      "iteration 0 / 1500: loss 30802.221642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot/yadisk/ds/courses/dlinnlp/tasks/backpropagation_workshop/task1/cs231n/classifiers/softmax.py:86: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -np.log(softmax_val[idx_train, y]).sum()\n",
      "/home/prot/yadisk/ds/courses/dlinnlp/tasks/backpropagation_workshop/task1/cs231n/classifiers/softmax.py:85: RuntimeWarning: invalid value encountered in true_divide\n",
      "  softmax_val = classes / np.sum(classes, axis=1)[:, None]\n",
      "/home/prot/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:83: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "1e-05 100.0\n",
      "iteration 0 / 1500: loss 8.527133\n",
      "iteration 100 / 1500: loss 5.540097\n",
      "iteration 200 / 1500: loss 6.424820\n",
      "iteration 300 / 1500: loss 4.660209\n",
      "iteration 400 / 1500: loss 4.407483\n",
      "iteration 500 / 1500: loss 4.196936\n",
      "iteration 600 / 1500: loss 3.259758\n",
      "iteration 700 / 1500: loss 4.463929\n",
      "iteration 800 / 1500: loss 3.060771\n",
      "iteration 900 / 1500: loss 3.536234\n",
      "iteration 1000 / 1500: loss 3.100996\n",
      "iteration 1100 / 1500: loss 2.611578\n",
      "iteration 1200 / 1500: loss 3.807340\n",
      "iteration 1300 / 1500: loss 2.624035\n",
      "iteration 1400 / 1500: loss 2.757161\n",
      "1e-05 215.44346900318845\n",
      "iteration 0 / 1500: loss 12.326113\n",
      "iteration 100 / 1500: loss 7.649984\n",
      "iteration 200 / 1500: loss 4.891258\n",
      "iteration 300 / 1500: loss 4.209882\n",
      "iteration 400 / 1500: loss 3.918861\n",
      "iteration 500 / 1500: loss 3.404923\n",
      "iteration 600 / 1500: loss 3.455402\n",
      "iteration 700 / 1500: loss 2.517028\n",
      "iteration 800 / 1500: loss 3.187699\n",
      "iteration 900 / 1500: loss 2.777093\n",
      "iteration 1000 / 1500: loss 3.028817\n",
      "iteration 1100 / 1500: loss 4.333067\n",
      "iteration 1200 / 1500: loss 3.736471\n",
      "iteration 1300 / 1500: loss 3.718836\n",
      "iteration 1400 / 1500: loss 3.775886\n",
      "1e-05 464.15888336127773\n",
      "iteration 0 / 1500: loss 19.243634\n",
      "iteration 100 / 1500: loss 9.080633\n",
      "iteration 200 / 1500: loss 5.643944\n",
      "iteration 300 / 1500: loss 4.023336\n",
      "iteration 400 / 1500: loss 3.445325\n",
      "iteration 500 / 1500: loss 2.827026\n",
      "iteration 600 / 1500: loss 4.492323\n",
      "iteration 700 / 1500: loss 3.199702\n",
      "iteration 800 / 1500: loss 3.785979\n",
      "iteration 900 / 1500: loss 4.265714\n",
      "iteration 1000 / 1500: loss 2.779906\n",
      "iteration 1100 / 1500: loss 3.975393\n",
      "iteration 1200 / 1500: loss 2.867484\n",
      "iteration 1300 / 1500: loss 3.656046\n",
      "iteration 1400 / 1500: loss 2.656733\n",
      "1e-05 1000.0\n",
      "iteration 0 / 1500: loss 36.761864\n",
      "iteration 100 / 1500: loss 7.002385\n",
      "iteration 200 / 1500: loss 3.967163\n",
      "iteration 300 / 1500: loss 3.537745\n",
      "iteration 400 / 1500: loss 3.988942\n",
      "iteration 500 / 1500: loss 4.191531\n",
      "iteration 600 / 1500: loss 4.947260\n",
      "iteration 700 / 1500: loss 5.871485\n",
      "iteration 800 / 1500: loss 4.410788\n",
      "iteration 900 / 1500: loss 3.311378\n",
      "iteration 1000 / 1500: loss 3.734640\n",
      "iteration 1100 / 1500: loss 4.684074\n",
      "iteration 1200 / 1500: loss 4.677833\n",
      "iteration 1300 / 1500: loss 2.771683\n",
      "iteration 1400 / 1500: loss 2.825523\n",
      "1e-05 2154.4346900318824\n",
      "iteration 0 / 1500: loss 71.165826\n",
      "iteration 100 / 1500: loss 5.814255\n",
      "iteration 200 / 1500: loss 2.515976\n",
      "iteration 300 / 1500: loss 6.301093\n",
      "iteration 400 / 1500: loss 3.373844\n",
      "iteration 500 / 1500: loss 2.896389\n",
      "iteration 600 / 1500: loss 2.986768\n",
      "iteration 700 / 1500: loss 2.770573\n",
      "iteration 800 / 1500: loss 3.272896\n",
      "iteration 900 / 1500: loss 5.607263\n",
      "iteration 1000 / 1500: loss 5.824386\n",
      "iteration 1100 / 1500: loss 3.690033\n",
      "iteration 1200 / 1500: loss 2.898333\n",
      "iteration 1300 / 1500: loss 3.176785\n",
      "iteration 1400 / 1500: loss 3.863814\n",
      "1e-05 4641.588833612777\n",
      "iteration 0 / 1500: loss 148.111258\n",
      "iteration 100 / 1500: loss 3.421029\n",
      "iteration 200 / 1500: loss 4.075687\n",
      "iteration 300 / 1500: loss 4.480985\n",
      "iteration 400 / 1500: loss 3.612550\n",
      "iteration 500 / 1500: loss 3.247659\n",
      "iteration 600 / 1500: loss 4.196886\n",
      "iteration 700 / 1500: loss 4.698489\n",
      "iteration 800 / 1500: loss 4.737696\n",
      "iteration 900 / 1500: loss 3.960771\n",
      "iteration 1000 / 1500: loss 4.199080\n",
      "iteration 1100 / 1500: loss 5.205478\n",
      "iteration 1200 / 1500: loss 4.741656\n",
      "iteration 1300 / 1500: loss 4.532105\n",
      "iteration 1400 / 1500: loss 4.414548\n",
      "1e-05 10000.0\n",
      "iteration 0 / 1500: loss 311.452595\n",
      "iteration 100 / 1500: loss 3.588621\n",
      "iteration 200 / 1500: loss 4.664960\n",
      "iteration 300 / 1500: loss 3.829129\n",
      "iteration 400 / 1500: loss 5.500339\n",
      "iteration 500 / 1500: loss 3.196406\n",
      "iteration 600 / 1500: loss 7.151156\n",
      "iteration 700 / 1500: loss 4.346336\n",
      "iteration 800 / 1500: loss 5.254202\n",
      "iteration 900 / 1500: loss 5.853466\n",
      "iteration 1000 / 1500: loss 5.686100\n",
      "iteration 1100 / 1500: loss 3.906359\n",
      "iteration 1200 / 1500: loss 3.578586\n",
      "iteration 1300 / 1500: loss 4.710519\n",
      "iteration 1400 / 1500: loss 4.231605\n",
      "1e-05 21544.346900318822\n",
      "iteration 0 / 1500: loss 669.691468\n",
      "iteration 100 / 1500: loss 5.846348\n",
      "iteration 200 / 1500: loss 6.008738\n",
      "iteration 300 / 1500: loss 6.692745\n",
      "iteration 400 / 1500: loss 5.947375\n",
      "iteration 500 / 1500: loss 8.137915\n",
      "iteration 600 / 1500: loss 6.502839\n",
      "iteration 700 / 1500: loss 6.183248\n",
      "iteration 800 / 1500: loss 7.957125\n",
      "iteration 900 / 1500: loss 5.693979\n",
      "iteration 1000 / 1500: loss 4.927652\n",
      "iteration 1100 / 1500: loss 4.827860\n",
      "iteration 1200 / 1500: loss 4.792084\n",
      "iteration 1300 / 1500: loss 5.416145\n",
      "iteration 1400 / 1500: loss 4.743558\n",
      "1e-05 46415.888336127726\n",
      "iteration 0 / 1500: loss 1428.155968\n",
      "iteration 100 / 1500: loss 7.751387\n",
      "iteration 200 / 1500: loss 6.384406\n",
      "iteration 300 / 1500: loss 5.904653\n",
      "iteration 400 / 1500: loss 11.983712\n",
      "iteration 500 / 1500: loss 4.533076\n",
      "iteration 600 / 1500: loss 13.295023\n",
      "iteration 700 / 1500: loss 10.201209\n",
      "iteration 800 / 1500: loss 7.660180\n",
      "iteration 900 / 1500: loss 10.153621\n",
      "iteration 1000 / 1500: loss 9.019764\n",
      "iteration 1100 / 1500: loss 7.339954\n",
      "iteration 1200 / 1500: loss 10.421805\n",
      "iteration 1300 / 1500: loss 5.593079\n",
      "iteration 1400 / 1500: loss 8.475823\n",
      "1e-05 100000.0\n",
      "iteration 0 / 1500: loss 3083.415465\n",
      "iteration 100 / 1500: loss 19.932445\n",
      "iteration 200 / 1500: loss 20.226725\n",
      "iteration 300 / 1500: loss 26.583400\n",
      "iteration 400 / 1500: loss 23.746591\n",
      "iteration 500 / 1500: loss 25.932940\n",
      "iteration 600 / 1500: loss 23.893792\n",
      "iteration 700 / 1500: loss 25.363697\n",
      "iteration 800 / 1500: loss 23.147812\n",
      "iteration 900 / 1500: loss 21.451481\n",
      "iteration 1000 / 1500: loss 22.677079\n",
      "iteration 1100 / 1500: loss 22.696695\n",
      "iteration 1200 / 1500: loss 23.227550\n",
      "iteration 1300 / 1500: loss 20.528043\n",
      "iteration 1400 / 1500: loss 21.580652\n",
      "1e-05 215443.46900318822\n",
      "iteration 0 / 1500: loss 6648.333191\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "1e-05 464158.8833612772\n",
      "iteration 0 / 1500: loss 14208.207060\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "1e-05 1000000.0\n",
      "iteration 0 / 1500: loss 30647.668785\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "lr 1.000000e-09 reg 1.000000e+02 train accuracy: 0.126837 val accuracy: 0.114000\n",
      "lr 1.000000e-09 reg 2.154435e+02 train accuracy: 0.119653 val accuracy: 0.123000\n",
      "lr 1.000000e-09 reg 4.641589e+02 train accuracy: 0.101184 val accuracy: 0.098000\n",
      "lr 1.000000e-09 reg 1.000000e+03 train accuracy: 0.117898 val accuracy: 0.116000\n",
      "lr 1.000000e-09 reg 2.154435e+03 train accuracy: 0.114714 val accuracy: 0.104000\n",
      "lr 1.000000e-09 reg 4.641589e+03 train accuracy: 0.101571 val accuracy: 0.098000\n",
      "lr 1.000000e-09 reg 1.000000e+04 train accuracy: 0.089306 val accuracy: 0.081000\n",
      "lr 1.000000e-09 reg 2.154435e+04 train accuracy: 0.099429 val accuracy: 0.099000\n",
      "lr 1.000000e-09 reg 4.641589e+04 train accuracy: 0.082918 val accuracy: 0.093000\n",
      "lr 1.000000e-09 reg 1.000000e+05 train accuracy: 0.090041 val accuracy: 0.087000\n",
      "lr 1.000000e-09 reg 2.154435e+05 train accuracy: 0.132306 val accuracy: 0.146000\n",
      "lr 1.000000e-09 reg 4.641589e+05 train accuracy: 0.125837 val accuracy: 0.127000\n",
      "lr 1.000000e-09 reg 1.000000e+06 train accuracy: 0.132694 val accuracy: 0.145000\n",
      "lr 3.162278e-09 reg 1.000000e+02 train accuracy: 0.105020 val accuracy: 0.118000\n",
      "lr 3.162278e-09 reg 2.154435e+02 train accuracy: 0.108184 val accuracy: 0.116000\n",
      "lr 3.162278e-09 reg 4.641589e+02 train accuracy: 0.120327 val accuracy: 0.120000\n",
      "lr 3.162278e-09 reg 1.000000e+03 train accuracy: 0.152265 val accuracy: 0.155000\n",
      "lr 3.162278e-09 reg 2.154435e+03 train accuracy: 0.099837 val accuracy: 0.082000\n",
      "lr 3.162278e-09 reg 4.641589e+03 train accuracy: 0.119592 val accuracy: 0.121000\n",
      "lr 3.162278e-09 reg 1.000000e+04 train accuracy: 0.130163 val accuracy: 0.126000\n",
      "lr 3.162278e-09 reg 2.154435e+04 train accuracy: 0.116061 val accuracy: 0.129000\n",
      "lr 3.162278e-09 reg 4.641589e+04 train accuracy: 0.120245 val accuracy: 0.128000\n",
      "lr 3.162278e-09 reg 1.000000e+05 train accuracy: 0.145816 val accuracy: 0.124000\n",
      "lr 3.162278e-09 reg 2.154435e+05 train accuracy: 0.147694 val accuracy: 0.163000\n",
      "lr 3.162278e-09 reg 4.641589e+05 train accuracy: 0.158082 val accuracy: 0.177000\n",
      "lr 3.162278e-09 reg 1.000000e+06 train accuracy: 0.262429 val accuracy: 0.276000\n",
      "lr 1.000000e-08 reg 1.000000e+02 train accuracy: 0.152449 val accuracy: 0.149000\n",
      "lr 1.000000e-08 reg 2.154435e+02 train accuracy: 0.154898 val accuracy: 0.178000\n",
      "lr 1.000000e-08 reg 4.641589e+02 train accuracy: 0.145959 val accuracy: 0.139000\n",
      "lr 1.000000e-08 reg 1.000000e+03 train accuracy: 0.162776 val accuracy: 0.164000\n",
      "lr 1.000000e-08 reg 2.154435e+03 train accuracy: 0.174347 val accuracy: 0.177000\n",
      "lr 1.000000e-08 reg 4.641589e+03 train accuracy: 0.153082 val accuracy: 0.144000\n",
      "lr 1.000000e-08 reg 1.000000e+04 train accuracy: 0.152980 val accuracy: 0.134000\n",
      "lr 1.000000e-08 reg 2.154435e+04 train accuracy: 0.156959 val accuracy: 0.161000\n",
      "lr 1.000000e-08 reg 4.641589e+04 train accuracy: 0.201592 val accuracy: 0.197000\n",
      "lr 1.000000e-08 reg 1.000000e+05 train accuracy: 0.200408 val accuracy: 0.205000\n",
      "lr 1.000000e-08 reg 2.154435e+05 train accuracy: 0.267612 val accuracy: 0.280000\n",
      "lr 1.000000e-08 reg 4.641589e+05 train accuracy: 0.272204 val accuracy: 0.283000\n",
      "lr 1.000000e-08 reg 1.000000e+06 train accuracy: 0.256143 val accuracy: 0.264000\n",
      "lr 3.162278e-08 reg 1.000000e+02 train accuracy: 0.196755 val accuracy: 0.204000\n",
      "lr 3.162278e-08 reg 2.154435e+02 train accuracy: 0.204020 val accuracy: 0.219000\n",
      "lr 3.162278e-08 reg 4.641589e+02 train accuracy: 0.189429 val accuracy: 0.203000\n",
      "lr 3.162278e-08 reg 1.000000e+03 train accuracy: 0.201857 val accuracy: 0.210000\n",
      "lr 3.162278e-08 reg 2.154435e+03 train accuracy: 0.200735 val accuracy: 0.211000\n",
      "lr 3.162278e-08 reg 4.641589e+03 train accuracy: 0.197306 val accuracy: 0.214000\n",
      "lr 3.162278e-08 reg 1.000000e+04 train accuracy: 0.220571 val accuracy: 0.225000\n",
      "lr 3.162278e-08 reg 2.154435e+04 train accuracy: 0.244020 val accuracy: 0.269000\n",
      "lr 3.162278e-08 reg 4.641589e+04 train accuracy: 0.303122 val accuracy: 0.319000\n",
      "lr 3.162278e-08 reg 1.000000e+05 train accuracy: 0.306367 val accuracy: 0.323000\n",
      "lr 3.162278e-08 reg 2.154435e+05 train accuracy: 0.286429 val accuracy: 0.301000\n",
      "lr 3.162278e-08 reg 4.641589e+05 train accuracy: 0.262714 val accuracy: 0.278000\n",
      "lr 3.162278e-08 reg 1.000000e+06 train accuracy: 0.250673 val accuracy: 0.265000\n",
      "lr 1.000000e-07 reg 1.000000e+02 train accuracy: 0.247653 val accuracy: 0.238000\n",
      "lr 1.000000e-07 reg 2.154435e+02 train accuracy: 0.252796 val accuracy: 0.274000\n",
      "lr 1.000000e-07 reg 4.641589e+02 train accuracy: 0.253041 val accuracy: 0.254000\n",
      "lr 1.000000e-07 reg 1.000000e+03 train accuracy: 0.252265 val accuracy: 0.259000\n",
      "lr 1.000000e-07 reg 2.154435e+03 train accuracy: 0.265061 val accuracy: 0.264000\n",
      "lr 1.000000e-07 reg 4.641589e+03 train accuracy: 0.297041 val accuracy: 0.301000\n",
      "lr 1.000000e-07 reg 1.000000e+04 train accuracy: 0.327449 val accuracy: 0.330000\n",
      "lr 1.000000e-07 reg 2.154435e+04 train accuracy: 0.351388 val accuracy: 0.373000\n",
      "lr 1.000000e-07 reg 4.641589e+04 train accuracy: 0.329061 val accuracy: 0.356000\n",
      "lr 1.000000e-07 reg 1.000000e+05 train accuracy: 0.310918 val accuracy: 0.321000\n",
      "lr 1.000000e-07 reg 2.154435e+05 train accuracy: 0.279204 val accuracy: 0.295000\n",
      "lr 1.000000e-07 reg 4.641589e+05 train accuracy: 0.260143 val accuracy: 0.278000\n",
      "lr 1.000000e-07 reg 1.000000e+06 train accuracy: 0.251367 val accuracy: 0.264000\n",
      "lr 3.162278e-07 reg 1.000000e+02 train accuracy: 0.306612 val accuracy: 0.328000\n",
      "lr 3.162278e-07 reg 2.154435e+02 train accuracy: 0.302327 val accuracy: 0.311000\n",
      "lr 3.162278e-07 reg 4.641589e+02 train accuracy: 0.312490 val accuracy: 0.312000\n",
      "lr 3.162278e-07 reg 1.000000e+03 train accuracy: 0.326755 val accuracy: 0.336000\n",
      "lr 3.162278e-07 reg 2.154435e+03 train accuracy: 0.358653 val accuracy: 0.362000\n",
      "lr 3.162278e-07 reg 4.641589e+03 train accuracy: 0.381449 val accuracy: 0.389000\n",
      "lr 3.162278e-07 reg 1.000000e+04 train accuracy: 0.373898 val accuracy: 0.388000\n",
      "lr 3.162278e-07 reg 2.154435e+04 train accuracy: 0.353163 val accuracy: 0.371000\n",
      "lr 3.162278e-07 reg 4.641589e+04 train accuracy: 0.329000 val accuracy: 0.339000\n",
      "lr 3.162278e-07 reg 1.000000e+05 train accuracy: 0.302286 val accuracy: 0.324000\n",
      "lr 3.162278e-07 reg 2.154435e+05 train accuracy: 0.285531 val accuracy: 0.292000\n",
      "lr 3.162278e-07 reg 4.641589e+05 train accuracy: 0.254673 val accuracy: 0.263000\n",
      "lr 3.162278e-07 reg 1.000000e+06 train accuracy: 0.256531 val accuracy: 0.251000\n",
      "lr 1.000000e-06 reg 1.000000e+02 train accuracy: 0.357245 val accuracy: 0.339000\n",
      "lr 1.000000e-06 reg 2.154435e+02 train accuracy: 0.367265 val accuracy: 0.362000\n",
      "lr 1.000000e-06 reg 4.641589e+02 train accuracy: 0.385510 val accuracy: 0.367000\n",
      "lr 1.000000e-06 reg 1.000000e+03 train accuracy: 0.403327 val accuracy: 0.408000\n",
      "lr 1.000000e-06 reg 2.154435e+03 train accuracy: 0.402082 val accuracy: 0.406000\n",
      "lr 1.000000e-06 reg 4.641589e+03 train accuracy: 0.385592 val accuracy: 0.396000\n",
      "lr 1.000000e-06 reg 1.000000e+04 train accuracy: 0.364878 val accuracy: 0.378000\n",
      "lr 1.000000e-06 reg 2.154435e+04 train accuracy: 0.336286 val accuracy: 0.340000\n",
      "lr 1.000000e-06 reg 4.641589e+04 train accuracy: 0.310041 val accuracy: 0.326000\n",
      "lr 1.000000e-06 reg 1.000000e+05 train accuracy: 0.297551 val accuracy: 0.315000\n",
      "lr 1.000000e-06 reg 2.154435e+05 train accuracy: 0.273204 val accuracy: 0.284000\n",
      "lr 1.000000e-06 reg 4.641589e+05 train accuracy: 0.251245 val accuracy: 0.259000\n",
      "lr 1.000000e-06 reg 1.000000e+06 train accuracy: 0.235837 val accuracy: 0.252000\n",
      "lr 3.162278e-06 reg 1.000000e+02 train accuracy: 0.394633 val accuracy: 0.383000\n",
      "lr 3.162278e-06 reg 2.154435e+02 train accuracy: 0.406673 val accuracy: 0.396000\n",
      "lr 3.162278e-06 reg 4.641589e+02 train accuracy: 0.404755 val accuracy: 0.385000\n",
      "lr 3.162278e-06 reg 1.000000e+03 train accuracy: 0.390061 val accuracy: 0.378000\n",
      "lr 3.162278e-06 reg 2.154435e+03 train accuracy: 0.381204 val accuracy: 0.378000\n",
      "lr 3.162278e-06 reg 4.641589e+03 train accuracy: 0.373939 val accuracy: 0.370000\n",
      "lr 3.162278e-06 reg 1.000000e+04 train accuracy: 0.336449 val accuracy: 0.340000\n",
      "lr 3.162278e-06 reg 2.154435e+04 train accuracy: 0.319204 val accuracy: 0.325000\n",
      "lr 3.162278e-06 reg 4.641589e+04 train accuracy: 0.296429 val accuracy: 0.308000\n",
      "lr 3.162278e-06 reg 1.000000e+05 train accuracy: 0.263816 val accuracy: 0.276000\n",
      "lr 3.162278e-06 reg 2.154435e+05 train accuracy: 0.210714 val accuracy: 0.200000\n",
      "lr 3.162278e-06 reg 4.641589e+05 train accuracy: 0.131122 val accuracy: 0.159000\n",
      "lr 3.162278e-06 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-05 reg 1.000000e+02 train accuracy: 0.270878 val accuracy: 0.263000\n",
      "lr 1.000000e-05 reg 2.154435e+02 train accuracy: 0.232429 val accuracy: 0.241000\n",
      "lr 1.000000e-05 reg 4.641589e+02 train accuracy: 0.295061 val accuracy: 0.304000\n",
      "lr 1.000000e-05 reg 1.000000e+03 train accuracy: 0.273735 val accuracy: 0.265000\n",
      "lr 1.000000e-05 reg 2.154435e+03 train accuracy: 0.254265 val accuracy: 0.266000\n",
      "lr 1.000000e-05 reg 4.641589e+03 train accuracy: 0.269816 val accuracy: 0.304000\n",
      "lr 1.000000e-05 reg 1.000000e+04 train accuracy: 0.194327 val accuracy: 0.211000\n",
      "lr 1.000000e-05 reg 2.154435e+04 train accuracy: 0.187531 val accuracy: 0.202000\n",
      "lr 1.000000e-05 reg 4.641589e+04 train accuracy: 0.135429 val accuracy: 0.141000\n",
      "lr 1.000000e-05 reg 1.000000e+05 train accuracy: 0.103959 val accuracy: 0.088000\n",
      "lr 1.000000e-05 reg 2.154435e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-05 reg 4.641589e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-05 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "best validation accuracy achieved during cross-validation: 0.408000\n"
     ]
    }
   ],
   "source": [
    "import itertools as it\n",
    "\n",
    "learning_rates = np.logspace(-9, -5, num=9)\n",
    "regularization_strengths = np.logspace(2, 6, num=13)\n",
    "combs = it.product(learning_rates, regularization_strengths)\n",
    "\n",
    "# learning_rates = [1e-7, 5e-7]\n",
    "# regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "# results is dictionary mapping tuples of the form\n",
    "# (learning_rate, regularization_strength) to tuples of the form\n",
    "# (training_accuracy, validation_accuracy). The accuracy is simply the fraction\n",
    "# of data points that are correctly classified.\n",
    "results = {}\n",
    "best_val = -1       # The highest validation accuracy that we have seen so far.\n",
    "best_softmax = None # The Softmax object that achieved the highest validation rate.\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Write code that chooses the best hyperparameters by tuning on the validation #\n",
    "# set. For each combination of hyperparameters, train a Softmax clf on the     #\n",
    "# training set, compute its accuracy on the training and validation sets, and  #\n",
    "# store these numbers in the results dictionary. In addition, store the best   #\n",
    "# validation accuracy in best_val and the Softmax object that achieves this    #\n",
    "# accuracy in best_softmax.                                                    #\n",
    "#                                                                              #\n",
    "# Hint: You should use a small value for num_iters as you develop your         #\n",
    "# validation code so that the Softmax don't take much time to train; once you  #\n",
    "# are confident that your validation code works, you should rerun the          #\n",
    "# validation code with a larger value for num_iters.                           #\n",
    "################################################################################\n",
    "for lr, reg in combs:\n",
    "    print(lr, reg)\n",
    "    softmax = Softmax()\n",
    "    loss_hist = softmax.train(X_train, y_train, learning_rate=lr, reg=reg,\n",
    "                              num_iters=1500, verbose=True)\n",
    "    y_train_pred = softmax.predict(X_train)\n",
    "    y_val_pred = softmax.predict(X_val)\n",
    "    \n",
    "    train_acc = np.mean(y_train == y_train_pred)\n",
    "    val_acc = (np.mean(y_val == y_val_pred))\n",
    "\n",
    "    results[(lr, reg)] = (train_acc, val_acc)\n",
    "    if best_val < val_acc:\n",
    "        best_val = val_acc\n",
    "        best_softmax = softmax\n",
    "        best_lr = lr\n",
    "        best_reg = reg\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T14:43:28.307538Z",
     "start_time": "2019-03-18T14:43:28.285883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.377000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T14:43:28.588577Z",
     "start_time": "2019-03-18T14:43:28.309433Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max score: 10 points"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
